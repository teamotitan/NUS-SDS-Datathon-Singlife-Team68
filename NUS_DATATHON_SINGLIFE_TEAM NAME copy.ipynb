{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall scikit-learn --yes\n",
    "# !pip uninstall imblearn --yes\n",
    "# !pip install scikit-learn==1.2.2\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing all na values to 0 since there is initially 2 distinct values in this column, na values and 1.\n",
    "df[\"f_purchase_lh\"]=df[\"f_purchase_lh\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop these columns because these columns either have too many missing values or all of the values are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=pd.Index([\"flg_affconnect_lapse_ever\", \"hlthclaim_cnt_success\",\"giclaim_cnt_success\",\"recency_cancel\", \"recency_lapse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we convert these two columns that has values to 1 and those without values to be 0, those with numbers will be considered recent those without will be cosnidered not recent or never visited (caution run once only if not code will be executed twice and the 0 will now be changed to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We decided to drop the following columns with `ape_`, `sumins_`, `prempaid_*` because the customer would not know these information. \n",
    "#Hence, it will not affect their decision-making of whether they would buy the insurance (target variable)\n",
    "spike_cols = [col for col in df.columns if 'ape_' in col[:4]]\n",
    "df = df.drop(columns = pd.Index(spike_cols))\n",
    "\n",
    "sumins_cols = [col for col in df.columns if 'sumins_' in col[:len(\"sumins_\")]]\n",
    "df = df.drop(columns = pd.Index(sumins_cols))\n",
    "\n",
    "prempaid_cols = [col for col in df.columns if 'prempaid_' in col[:len(\"prempaid_\")]]\n",
    "df = df.drop(columns = pd.Index(prempaid_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following columns has nothing to do with the target variable, hence it will be deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop these agents specific parameters as it has no influence on customer purchasing decision\n",
    "\n",
    "for names in [\"clmcon_visit_days\", \"recency_clmcon\", \"recency_clm_regis\", \"flg_hlthclaim_\", \"flg_gi_claim_\" , \"f_ever_bought_\", \"n_months_last_bought\" , \"lapse_ape_\", \"n_months_since_lapse_\", \"cltsex_fix\"]:\n",
    "    spike_cols = [col for col in df.columns if names in col[:len(names)]]\n",
    "    df = df.drop(columns = pd.Index(spike_cols))\n",
    "\n",
    "df = df.drop(columns = pd.Index([\"clttype\", \"stat_flag\", \"min_occ_date\", \"recency_giclaim_success\", \"giclaim_cnt_unsuccess\", \"recency_giclaim_unsuccess\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting nan values into 0 while converting those postiive values into 1\n",
    "# 1 to indicate recency and 0 to indicate non-recent or never claim\n",
    "df[\"recency_giclaim\"] = df[\"recency_giclaim\"].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "df[\"recency_hlthclaim\"] = df[\"recency_hlthclaim\"].apply(lambda x: 1 if pd.notna(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the two columns that are in the code below, we converted them to float since they are in Object datatype initially and filled the missing values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"giclaim_amt\"] = df[\"giclaim_amt\"].astype(\"float64\")\n",
    "gi_claim_median = df[\"giclaim_amt\"].median()\n",
    "df[\"giclaim_amt\"] = df[\"giclaim_amt\"].fillna(gi_claim_median)\n",
    "\n",
    "df[\"hlthclaim_amt\"] = df[\"hlthclaim_amt\"].astype(\"float64\")\n",
    "hlthclaim_median = df[\"hlthclaim_amt\"].median()\n",
    "df[\"hlthclaim_amt\"] = df[\"hlthclaim_amt\"].fillna(hlthclaim_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following column refers to the Total number of in-force and canceled policies.\n",
    "# the unique values are [nan  1.  3.  2.  4.  6.]\n",
    "# nan would mean 0 cancelled policies\n",
    "df[\"tot_cancel_pols\"]=df[\"tot_cancel_pols\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_cols = df.select_dtypes(include=[\"string\", \"object\"]).columns\n",
    "df_numeric = df.drop(columns=non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the proportion of NA values in each columns\n",
    "df.isna().sum()/(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the distinct/unique values in each numerical columns of the dataset\n",
    "id = 1\n",
    "for col in df_numeric.columns:\n",
    "    print(id, col, \":  \", df[col].unique())\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"is_dependent_in_at_least_1_policy\", \"f_ever_declined_la\", \"flg_affconnect_show_interest_ever\", \"flg_affconnect_ready_to_buy_ever\", \"affcon_visit_days\" are columns that have either [nan 1] or [0 nan], or in a similar form. Hence, nan values will be converted to either 0 or 1 depending on the counterpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"flg_affconnect_ready_to_buy_ever\"] = df[\"flg_affconnect_ready_to_buy_ever\"].fillna(0)\n",
    "df[\"flg_affconnect_show_interest_ever\"] = df[\"flg_affconnect_show_interest_ever\"].fillna(0)\n",
    "df[\"f_ever_declined_la\"] = df[\"f_ever_declined_la\"].fillna(0)\n",
    "df[\"is_dependent_in_at_least_1_policy\"] = df[\"is_dependent_in_at_least_1_policy\"].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= df.drop([\"clntnum\", \"race_desc\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority of the clients are Singaporean as shown below. Hence, we are going to focus on Singaporean clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ctrycode_desc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ctrycode_desc\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"ctrycode_desc\"] == \"Singapore\"]\n",
    "df = df.drop(\"ctrycode_desc\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 1\n",
    "for col in df.columns:\n",
    "    print(id, col, \":  \", df[col].unique())\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing column feature: DOB -> Age\n",
    "Filling missing values with the median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting DOB to age\n",
    "age_list = list()\n",
    "for x in df[\"cltdob_fix\"]:\n",
    "    if x.lower() != \"none\":\n",
    "        year = int(x[:4])\n",
    "        age = 2024 - year\n",
    "        age_list.append(age)\n",
    "\n",
    "    \n",
    "df[\"cltdob_fix\"] = pd.Series(age_list)\n",
    "median_value = df[\"cltdob_fix\"].median()\n",
    "df[\"cltdob_fix\"] = df[\"cltdob_fix\"].replace({None: np.nan})\n",
    "df[\"cltdob_fix\"] = df[\"cltdob_fix\"].fillna(median_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using hh_20 column and pop_20 columns as the X_train data for KNN imputation of hh_size_est since there are links between these 3 columns. This is because we checked that there are the same number of missing values for the three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n"
     ]
    }
   ],
   "source": [
    "print(sum(df[\"hh_20\"].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(df[\"pop_20\"].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"hh_20\"].value_counts().sum())\n",
    "print(df.shape[0] - df[\"hh_20\"].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_20\"] = df[\"hh_20\"].fillna(-1)\n",
    "df[\"pop_20\"] = df[\"pop_20\"].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_20\"] = df[\"hh_20\"].astype(int)\n",
    "df[\"pop_20\"] = df[\"pop_20\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "#calculating median value for hh_20 column\n",
    "hh_20_lst = list()\n",
    "for i in df[\"hh_20\"]:\n",
    "    if i != -1:\n",
    "        hh_20_lst.append(i)\n",
    "hh_20_median = statistics.median(hh_20_lst)\n",
    "print(hh_20_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating median value for pop_20 column\n",
    "pop_20_lst = list()\n",
    "for i in df[\"pop_20\"]:\n",
    "    if i != -1:\n",
    "        pop_20_lst.append(i)\n",
    "pop_20_median = statistics.median(pop_20_lst)\n",
    "print(pop_20_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_20\"] = df[\"hh_20\"].replace(-1, hh_20_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[\"hh_20\"] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pop_20\"] = df[\"pop_20\"].replace(-1, hh_20_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[\"pop_20\"] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(df[\"hh_size_est\"].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_size_est\"] = df[\"hh_size_est\"].replace(\">4\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_size_est\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=15)\n",
    "imputed_data = imputer.fit_transform(df[[\"pop_20\",\"hh_20\",\"hh_size_est\"]]).round()\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_size_est\"] = pd.DataFrame(imputed_data[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since there are still NA values after performing imputation using KNN, we decided to use DecisionTreeClassifier to impute the remaining missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc_x_train = df.dropna(subset = [\"hh_size_est\"])[[\"hh_20\", \"pop_20\"]]\n",
    "dtc_y_train = df.dropna(subset=[\"hh_size_est\"])[\"hh_size_est\"]\n",
    "\n",
    "dtc_x_test = df[df[\"hh_size_est\"].isna()][[\"hh_20\", \"pop_20\"]]\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(dtc_x_train, dtc_y_train)\n",
    "\n",
    "dtc_y_predicted = clf.predict(dtc_x_test)\n",
    "print(dtc_y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(dtc_y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"hh_size_est\"].isna(), \"hh_size_est\"] = dtc_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(df[\"hh_size_est\"].isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, all the missing values of the hh_size_est column are fully imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "hh_size_est_lst_numpy_array = np.array(df[\"hh_size_est\"]).reshape(-1,1)\n",
    "label_encoding = OneHotEncoder()\n",
    "encoded = label_encoding.fit(hh_size_est_lst_numpy_array)\n",
    "print(encoded.transform(hh_size_est_lst_numpy_array).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding_hh_size_est = encoded.transform(hh_size_est_lst_numpy_array).toarray()\n",
    "type(one_hot_encoding_hh_size_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding_hh_size_est_T = one_hot_encoding_hh_size_est.T\n",
    "print(one_hot_encoding_hh_size_est_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_id = 0\n",
    "for i in [\"0\", \"1\", \"2\", \"3\", \"4\", \">4\"]:\n",
    "    name = \"hh_size_est_\" + i\n",
    "    df[name] = one_hot_encoding_hh_size_est_T[some_id]\n",
    "    some_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.transform(np.array([0,1,2,3,4,5]).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding_hh_size_est_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on annual_income_est column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_annual_income_est = {'C.60K-100K':3, 'D.30K-60K':4, 'A.ABOVE200K':1, 'B.100K-200K':2, 'E.BELOW30K':5}\n",
    "annual_income_est_lst = list()\n",
    "for i in df[\"annual_income_est\"]:\n",
    "    if dic_annual_income_est.get(i):\n",
    "        annual_income_est_lst.append(dic_annual_income_est.get(i))\n",
    "    else:\n",
    "        annual_income_est_lst.append(i)\n",
    "print(annual_income_est_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annual_income_est_label_encode\"] = annual_income_est_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annual_income_est_label_encode\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### annual_income_est_label_encode also have the same number of missing data as the 3 other columns (before these 3 columns are imputed): hh_20, pp_20 and hh_size_est. Hence, this is a good basis for the assumption that there is correlation between these 4 columns. hh_20, pp_20 and hh_size_est will be used as X_train for imputation using DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc_x_train_aie = df.dropna(subset = [\"annual_income_est\"])[[\"hh_20\", \"pop_20\",\"hh_size_est\"]]\n",
    "dtc_y_train_aie = df.dropna(subset=[\"annual_income_est\"])[\"annual_income_est\"]\n",
    "\n",
    "dtc_x_test_aie = df[df[\"annual_income_est\"].isna()][[\"hh_20\", \"pop_20\", \"hh_size_est\"]]\n",
    "\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf2.fit(dtc_x_train_aie, dtc_y_train_aie)\n",
    "\n",
    "dtc_y_predicted_aie = clf2.predict(dtc_x_test_aie)\n",
    "print(dtc_y_predicted_aie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"annual_income_est\"].isna(), \"annual_income_est\"] = dtc_y_predicted_aie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one hot encoding for annual_income_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_income_est_np_array = np.array(df[\"annual_income_est\"]).reshape(-1,1)\n",
    "label_encoding2 = OneHotEncoder()\n",
    "encoded2 = label_encoding2.fit(annual_income_est_np_array)\n",
    "print(encoded2.transform(annual_income_est_np_array).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded2.transform(np.array(['C.60K-100K','D.30K-60K','A.ABOVE200K','B.100K-200K','E.BELOW30K']).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annual_income_est\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_income_est_T = encoded2.transform(annual_income_est_np_array).toarray().T\n",
    "annual_income_est_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 0\n",
    "for i in ['A.ABOVE200K','B.100K-200K','C.60K-100K','D.30K-60K','E.BELOW30K']:\n",
    "    name = \"annual_income_est_\" + i\n",
    "    df[name] = annual_income_est_T[some_index]\n",
    "    some_index += 1\n",
    "df = df.drop(\"annual_income_est\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values of the numeric columns with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"f_purchase_lh\"])\n",
    "y = df[\"f_purchase_lh\"]\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print('Before:', Counter(y_train))\n",
    "X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "print('After:', Counter(y_train))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    " \n",
    "model = RandomForestClassifier() \n",
    "model.fit(X_train, y_train) \n",
    " \n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_train.columns) \n",
    "top_features = feature_importances.nlargest(20).index \n",
    "X_train_selected = X_train[top_features] \n",
    "print(top_features)\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# # instantiate the model (using the default parameters)\n",
    "# logreg3 = LogisticRegression(random_state=42)\n",
    "\n",
    "# # fit the model with data\n",
    "# logreg3.fit(X_train_selected, y_train)\n",
    "\n",
    "# y_pred = logreg3.predict(X_test[top_features])\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    " \n",
    "dt_clf = DecisionTreeClassifier() \n",
    "dt_clf.fit(X_train, y_train) \n",
    " \n",
    "y_test_pred = dt_clf.predict(X_test) \n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(df: pd.DataFrame) -> list:\n",
    "\n",
    "\n",
    "    df = df.drop(columns=pd.Index([\"flg_affconnect_lapse_ever\", \"hlthclaim_cnt_success\",\"giclaim_cnt_success\",\"recency_cancel\", \"recency_lapse\"]))\n",
    "    spike_cols = [col for col in df.columns if 'ape_' in col[:4]]\n",
    "    df = df.drop(columns = pd.Index(spike_cols))\n",
    "\n",
    "    sumins_cols = [col for col in df.columns if 'sumins_' in col[:len(\"sumins_\")]]\n",
    "    df = df.drop(columns = pd.Index(sumins_cols))\n",
    "\n",
    "    prempaid_cols = [col for col in df.columns if 'prempaid_' in col[:len(\"prempaid_\")]]\n",
    "    df = df.drop(columns = pd.Index(prempaid_cols))\n",
    "\n",
    "    for names in [\"clmcon_visit_days\", \"recency_clmcon\", \"recency_clm_regis\", \"flg_hlthclaim_\", \"flg_gi_claim_\" , \"f_ever_bought_\", \"n_months_last_bought\" , \"lapse_ape_\", \"n_months_since_lapse_\", \"cltsex_fix\"]:\n",
    "        spike_cols = [col for col in df.columns if names in col[:len(names)]]\n",
    "        df = df.drop(columns = pd.Index(spike_cols))\n",
    "\n",
    "    df = df.drop(columns = pd.Index([\"clttype\", \"stat_flag\", \"min_occ_date\", \"recency_giclaim_success\", \"giclaim_cnt_unsuccess\", \"recency_giclaim_unsuccess\"]), axis = 1)\n",
    "    df[\"recency_giclaim\"] = df[\"recency_giclaim\"].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "    df[\"recency_hlthclaim\"] = df[\"recency_hlthclaim\"].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "    df[\"giclaim_amt\"] = df[\"giclaim_amt\"].astype(\"float64\")\n",
    "    gi_claim_median = df[\"giclaim_amt\"].median()\n",
    "    df[\"giclaim_amt\"] = df[\"giclaim_amt\"].fillna(gi_claim_median)\n",
    "    df[\"hlthclaim_amt\"] = df[\"hlthclaim_amt\"].astype(\"float64\")\n",
    "    hlthclaim_median = df[\"hlthclaim_amt\"].median()\n",
    "    df[\"hlthclaim_amt\"] = df[\"hlthclaim_amt\"].fillna(hlthclaim_median)\n",
    "    df[\"tot_cancel_pols\"]=df[\"tot_cancel_pols\"].fillna(0)\n",
    "    non_numeric_cols = df.select_dtypes(include=[\"string\", \"object\"]).columns\n",
    "    df_numeric = df.drop(columns=non_numeric_cols)\n",
    "    id = 1\n",
    "    \n",
    "    df[\"flg_affconnect_ready_to_buy_ever\"] = df[\"flg_affconnect_ready_to_buy_ever\"].fillna(0)\n",
    "    df[\"flg_affconnect_show_interest_ever\"] = df[\"flg_affconnect_show_interest_ever\"].fillna(0)\n",
    "    df[\"f_ever_declined_la\"] = df[\"f_ever_declined_la\"].fillna(0)\n",
    "    df[\"is_dependent_in_at_least_1_policy\"] = df[\"is_dependent_in_at_least_1_policy\"].fillna(1)\n",
    "\n",
    "    df= df.drop([\"clntnum\", \"race_desc\"], axis=1)\n",
    "    ## Majority of the clients are Singaporean as shown below. Hence, we are going to focus on Singaporean clients\n",
    "    df[\"ctrycode_desc\"]\n",
    "    df[\"ctrycode_desc\"].value_counts()\n",
    "    df = df[df[\"ctrycode_desc\"] == \"Singapore\"]\n",
    "    df = df.drop(\"ctrycode_desc\", axis = 1)\n",
    "    id = 1\n",
    "    \n",
    "    #### Editing column feature: DOB -> Age\n",
    "  \n",
    "    # converting DOB to age\n",
    "    age_list = list()\n",
    "    for x in df[\"cltdob_fix\"]:\n",
    "        if x.lower() != \"none\":\n",
    "            year = int(x[:4])\n",
    "            age = 2024 - year\n",
    "            age_list.append(age)\n",
    "\n",
    "        \n",
    "    df[\"cltdob_fix\"] = pd.Series(age_list)\n",
    "    median_value = df[\"cltdob_fix\"].median()\n",
    "    df[\"cltdob_fix\"] = df[\"cltdob_fix\"].replace({None: np.nan})\n",
    "    df[\"cltdob_fix\"] = df[\"cltdob_fix\"].fillna(median_value)\n",
    "    ### Using hh_20 column and pop_20 columns as the X_train data for KNN imputation of hh_size_est since there are links between these 3 columns. This is because we checked that there are the same number of missing values for the three columns\n",
    "    \n",
    "    df[\"hh_20\"] = df[\"hh_20\"].fillna(-1)\n",
    "    df[\"pop_20\"] = df[\"pop_20\"].fillna(-1)\n",
    "    df[\"hh_20\"] = df[\"hh_20\"].astype(int)\n",
    "    df[\"pop_20\"] = df[\"pop_20\"].astype(int)\n",
    "    hh_20_lst = list()\n",
    "    for i in df[\"hh_20\"]:\n",
    "        if i != -1:\n",
    "            hh_20_lst.append(i)\n",
    "    hh_20_median = statistics.median(hh_20_lst)\n",
    "  \n",
    "    pop_20_lst = list()\n",
    "    for i in df[\"pop_20\"]:\n",
    "        if i != -1:\n",
    "            pop_20_lst.append(i)\n",
    "    pop_20_median = statistics.median(pop_20_lst)\n",
    "\n",
    "    df[\"hh_20\"] = df[\"hh_20\"].replace(-1, hh_20_median)\n",
    "    sum(df[\"hh_20\"] == -1)\n",
    "    df[\"pop_20\"] = df[\"pop_20\"].replace(-1, hh_20_median)\n",
    "    sum(df[\"pop_20\"] == -1)\n",
    "    \n",
    "    df[\"hh_size_est\"] = df[\"hh_size_est\"].replace(\">4\", \"5\")\n",
    "    df[\"hh_size_est\"].value_counts()\n",
    "    from sklearn.impute import KNNImputer\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=15)\n",
    "    imputed_data = imputer.fit_transform(df[[\"pop_20\",\"hh_20\",\"hh_size_est\"]]).round()\n",
    "    imputed_data\n",
    "    df[\"hh_size_est\"] = pd.DataFrame(imputed_data[:,2])\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    dtc_x_train = df.dropna(subset = [\"hh_size_est\"])[[\"hh_20\", \"pop_20\"]]\n",
    "    dtc_y_train = df.dropna(subset=[\"hh_size_est\"])[\"hh_size_est\"]\n",
    "\n",
    "    dtc_x_test = df[df[\"hh_size_est\"].isna()][[\"hh_20\", \"pop_20\"]]\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(dtc_x_train, dtc_y_train)\n",
    "\n",
    "    dtc_y_predicted = clf.predict(dtc_x_test)\n",
    "    \n",
    "    df.loc[df[\"hh_size_est\"].isna(), \"hh_size_est\"] = dtc_y_predicted\n",
    "\n",
    "    #### Now, all the missing values of the hh_size_est column are fully imputed\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    hh_size_est_lst_numpy_array = np.array(df[\"hh_size_est\"]).reshape(-1,1)\n",
    "    label_encoding = OneHotEncoder()\n",
    "    encoded = label_encoding.fit(hh_size_est_lst_numpy_array)\n",
    "    \n",
    "    one_hot_encoding_hh_size_est = encoded.transform(hh_size_est_lst_numpy_array).toarray()\n",
    "    type(one_hot_encoding_hh_size_est)\n",
    "    one_hot_encoding_hh_size_est_T = one_hot_encoding_hh_size_est.T\n",
    "    some_id = 0\n",
    "    for i in [\"0\", \"1\", \"2\", \"3\", \"4\", \">4\"]:\n",
    "        name = \"hh_size_est_\" + i\n",
    "        df[name] = one_hot_encoding_hh_size_est_T[some_id]\n",
    "        some_id += 1\n",
    "    encoded.transform(np.array([0,1,2,3,4,5]).reshape(-1,1)).toarray()\n",
    "    ## Working on annual_income_est column\n",
    "    dic_annual_income_est = {'C.60K-100K':3, 'D.30K-60K':4, 'A.ABOVE200K':1, 'B.100K-200K':2, 'E.BELOW30K':5}\n",
    "    annual_income_est_lst = list()\n",
    "    for i in df[\"annual_income_est\"]:\n",
    "        if dic_annual_income_est.get(i):\n",
    "            annual_income_est_lst.append(dic_annual_income_est.get(i))\n",
    "        else:\n",
    "            annual_income_est_lst.append(i)\n",
    "    \n",
    "    df[\"annual_income_est_label_encode\"] = annual_income_est_lst\n",
    "    df[\"annual_income_est_label_encode\"].isna().sum()\n",
    "    ##### annual_income_est_label_encode also have the same number of missing data as the 3 other columns (before these 3 columns are imputed): hh_20, pp_20 and hh_size_est. Hence, this is a good basis for the assumption that there is correlation between these 4 columns. hh_20, pp_20 and hh_size_est will be used as X_train for imputation using DecisionTreeClassifier \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    dtc_x_train_aie = df.dropna(subset = [\"annual_income_est\"])[[\"hh_20\", \"pop_20\",\"hh_size_est\"]]\n",
    "    dtc_y_train_aie = df.dropna(subset=[\"annual_income_est\"])[\"annual_income_est\"]\n",
    "\n",
    "    dtc_x_test_aie = df[df[\"annual_income_est\"].isna()][[\"hh_20\", \"pop_20\", \"hh_size_est\"]]\n",
    "\n",
    "    clf2 = DecisionTreeClassifier()\n",
    "    clf2.fit(dtc_x_train_aie, dtc_y_train_aie)\n",
    "\n",
    "    dtc_y_predicted_aie = clf2.predict(dtc_x_test_aie)\n",
    "    \n",
    "    df.loc[df[\"annual_income_est\"].isna(), \"annual_income_est\"] = dtc_y_predicted_aie\n",
    "    ##### one hot encoding for annual_income_est\n",
    "    annual_income_est_np_array = np.array(df[\"annual_income_est\"]).reshape(-1,1)\n",
    "    label_encoding2 = OneHotEncoder()\n",
    "    encoded2 = label_encoding2.fit(annual_income_est_np_array)\n",
    "    \n",
    "    encoded2.transform(np.array(['C.60K-100K','D.30K-60K','A.ABOVE200K','B.100K-200K','E.BELOW30K']).reshape(-1,1)).toarray()\n",
    "    df[\"annual_income_est\"].value_counts()\n",
    "    annual_income_est_T = encoded2.transform(annual_income_est_np_array).toarray().T\n",
    "    annual_income_est_T\n",
    "    some_index = 0\n",
    "    for i in ['A.ABOVE200K','B.100K-200K','C.60K-100K','D.30K-60K','E.BELOW30K']:\n",
    "        name = \"annual_income_est_\" + i\n",
    "        df[name] = annual_income_est_T[some_index]\n",
    "        some_index += 1\n",
    "    df = df.drop(\"annual_income_est\", axis = 1)\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    y_test_pred = dt_clf.predict(df) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    result = list(y_test_pred) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99     17079\n",
      "         1.0       0.80      0.86      0.83       710\n",
      "\n",
      "    accuracy                           0.99     17789\n",
      "   macro avg       0.90      0.93      0.91     17789\n",
      "weighted avg       0.99      0.99      0.99     17789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = testing_hidden_data(test_df)\n",
    "print(classification_report(df[\"f_purchase_lh\"], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
