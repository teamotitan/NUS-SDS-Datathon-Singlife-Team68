{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall scikit-learn --yes\n",
    "# !pip uninstall imblearn --yes\n",
    "# !pip install scikit-learn==1.2.2\n",
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clntnum</th>\n",
       "      <th>race_desc</th>\n",
       "      <th>ctrycode_desc</th>\n",
       "      <th>clttype</th>\n",
       "      <th>stat_flag</th>\n",
       "      <th>min_occ_date</th>\n",
       "      <th>cltdob_fix</th>\n",
       "      <th>cltsex_fix</th>\n",
       "      <th>flg_substandard</th>\n",
       "      <th>flg_is_borderline_standard</th>\n",
       "      <th>...</th>\n",
       "      <th>recency_giclaim</th>\n",
       "      <th>giclaim_cnt_success</th>\n",
       "      <th>recency_giclaim_success</th>\n",
       "      <th>giclaim_cnt_unsuccess</th>\n",
       "      <th>recency_giclaim_unsuccess</th>\n",
       "      <th>flg_gi_claim_29d435_ever</th>\n",
       "      <th>flg_gi_claim_058815_ever</th>\n",
       "      <th>flg_gi_claim_42e115_ever</th>\n",
       "      <th>flg_gi_claim_856320_ever</th>\n",
       "      <th>f_purchase_lh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19550</th>\n",
       "      <td>91b546e924</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>1974-05-09</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>896bae548c</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>1979-11-11</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13337</th>\n",
       "      <td>f364439ae6</td>\n",
       "      <td>Others</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>1976-01-28</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15074</th>\n",
       "      <td>70f319cfe1</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2021-10-18</td>\n",
       "      <td>1976-03-19</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19724</th>\n",
       "      <td>2647a81328</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>P</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>2018-07-20</td>\n",
       "      <td>1995-07-31</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          clntnum race_desc ctrycode_desc clttype stat_flag min_occ_date  \\\n",
       "19550  91b546e924   Chinese     Singapore       P    ACTIVE   2017-10-31   \n",
       "4600   896bae548c   Chinese     Singapore       P    ACTIVE   2007-05-23   \n",
       "13337  f364439ae6    Others     Singapore       P    ACTIVE   2019-08-31   \n",
       "15074  70f319cfe1   Chinese     Singapore       P    ACTIVE   2021-10-18   \n",
       "19724  2647a81328   Chinese     Singapore       P    ACTIVE   2018-07-20   \n",
       "\n",
       "       cltdob_fix cltsex_fix  flg_substandard  flg_is_borderline_standard  \\\n",
       "19550  1974-05-09     Female              0.0                         0.0   \n",
       "4600   1979-11-11       Male              0.0                         0.0   \n",
       "13337  1976-01-28       Male              0.0                         0.0   \n",
       "15074  1976-03-19     Female              0.0                         0.0   \n",
       "19724  1995-07-31     Female              0.0                         0.0   \n",
       "\n",
       "       ...  recency_giclaim  giclaim_cnt_success  recency_giclaim_success  \\\n",
       "19550  ...              NaN                 None                     None   \n",
       "4600   ...              NaN                 None                     None   \n",
       "13337  ...              NaN                 None                     None   \n",
       "15074  ...              NaN                 None                     None   \n",
       "19724  ...              NaN                 None                     None   \n",
       "\n",
       "       giclaim_cnt_unsuccess  recency_giclaim_unsuccess  \\\n",
       "19550                   None                       None   \n",
       "4600                    None                       None   \n",
       "13337                   None                       None   \n",
       "15074                   None                       None   \n",
       "19724                   None                       None   \n",
       "\n",
       "       flg_gi_claim_29d435_ever  flg_gi_claim_058815_ever  \\\n",
       "19550                      None                      None   \n",
       "4600                       None                      None   \n",
       "13337                      None                      None   \n",
       "15074                      None                      None   \n",
       "19724                      None                      None   \n",
       "\n",
       "       flg_gi_claim_42e115_ever  flg_gi_claim_856320_ever  f_purchase_lh  \n",
       "19550                      None                      None            NaN  \n",
       "4600                       None                      None            NaN  \n",
       "13337                      None                      None            NaN  \n",
       "15074                      None                      None            NaN  \n",
       "19724                      None                      None            NaN  \n",
       "\n",
       "[5 rows x 304 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing all na values to 0 since there is initially 2 distinct values in this column, na values and 1.\n",
    "df[\"f_purchase_lh\"]=df[\"f_purchase_lh\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop these columns because these columns either have too many missing values or all of the values are None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=pd.Index([\"flg_affconnect_lapse_ever\", \"hlthclaim_cnt_success\",\"giclaim_cnt_success\",\"recency_cancel\", \"recency_lapse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we convert these two columns that has values to 1 and those without values to be 0, those with numbers will be considered recent those without will be cosnidered not recent or never visited (caution run once only if not code will be executed twice and the 0 will now be changed to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We decided to drop the following columns with `ape_`, `sumins_`, `prempaid_*` because the customer would not know these information. \n",
    "#Hence, it will not affect their decision-making of whether they would buy the insurance (target variable)\n",
    "spike_cols = [col for col in df.columns if 'ape_' in col[:4]]\n",
    "df = df.drop(columns = pd.Index(spike_cols))\n",
    "\n",
    "sumins_cols = [col for col in df.columns if 'sumins_' in col[:len(\"sumins_\")]]\n",
    "df = df.drop(columns = pd.Index(sumins_cols))\n",
    "\n",
    "prempaid_cols = [col for col in df.columns if 'prempaid_' in col[:len(\"prempaid_\")]]\n",
    "df = df.drop(columns = pd.Index(prempaid_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following columns has nothing to do with the target variable, hence it will be deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop these agents specific parameters as it has no influence on customer purchasing decision\n",
    "\n",
    "for names in [\"clmcon_visit_days\", \"recency_clmcon\", \"recency_clm_regis\", \"flg_hlthclaim_\", \"flg_gi_claim_\" , \"f_ever_bought_\", \"n_months_last_bought\" , \"lapse_ape_\", \"n_months_since_lapse_\", \"cltsex_fix\"]:\n",
    "    spike_cols = [col for col in df.columns if names in col[:len(names)]]\n",
    "    df = df.drop(columns = pd.Index(spike_cols))\n",
    "\n",
    "df = df.drop(columns = pd.Index([\"clttype\", \"stat_flag\", \"min_occ_date\", \"recency_giclaim_success\", \"giclaim_cnt_unsuccess\", \"recency_giclaim_unsuccess\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting nan values into 0 while converting those postiive values into 1\n",
    "# 1 to indicate recency and 0 to indicate non-recent or never claim\n",
    "df[\"recency_giclaim\"] = df[\"recency_giclaim\"].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "df[\"recency_hlthclaim\"] = df[\"recency_hlthclaim\"].apply(lambda x: 1 if pd.notna(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the two columns that are in the code below, we converted them to float since they are in Object datatype initially and filled the missing values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"giclaim_amt\"] = df[\"giclaim_amt\"].astype(\"float64\")\n",
    "gi_claim_median = df[\"giclaim_amt\"].median()\n",
    "df[\"giclaim_amt\"] = df[\"giclaim_amt\"].fillna(gi_claim_median)\n",
    "\n",
    "df[\"hlthclaim_amt\"] = df[\"hlthclaim_amt\"].astype(\"float64\")\n",
    "hlthclaim_median = df[\"hlthclaim_amt\"].median()\n",
    "df[\"hlthclaim_amt\"] = df[\"hlthclaim_amt\"].fillna(hlthclaim_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following column refers to the Total number of in-force and canceled policies.\n",
    "# the unique values are [nan  1.  3.  2.  4.  6.]\n",
    "# nan would mean 0 cancelled policies\n",
    "df[\"tot_cancel_pols\"]=df[\"tot_cancel_pols\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_cols = df.select_dtypes(include=[\"string\", \"object\"]).columns\n",
    "df_numeric = df.drop(columns=non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17992, 58)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clntnum                              0.000000\n",
       "race_desc                            0.222099\n",
       "ctrycode_desc                        0.001112\n",
       "cltdob_fix                           0.000000\n",
       "flg_substandard                      0.056358\n",
       "flg_is_borderline_standard           0.056358\n",
       "flg_is_revised_term                  0.056358\n",
       "flg_is_rental_flat                   0.056358\n",
       "flg_has_health_claim                 0.056358\n",
       "flg_has_life_claim                   0.056358\n",
       "flg_gi_claim                         0.056358\n",
       "flg_is_proposal                      0.056358\n",
       "flg_with_preauthorisation            0.056358\n",
       "flg_is_returned_mail                 0.056358\n",
       "is_consent_to_mail                   0.056358\n",
       "is_consent_to_email                  0.056358\n",
       "is_consent_to_call                   0.056358\n",
       "is_consent_to_sms                    0.056358\n",
       "is_valid_dm                          0.056358\n",
       "is_valid_email                       0.056358\n",
       "is_housewife_retiree                 0.056358\n",
       "is_sg_pr                             0.056358\n",
       "is_class_1_2                         0.056358\n",
       "is_dependent_in_at_least_1_policy    0.056358\n",
       "f_ever_declined_la                   0.931470\n",
       "hh_20                                0.156125\n",
       "pop_20                               0.156125\n",
       "hh_size                              0.156125\n",
       "hh_size_est                          0.156125\n",
       "annual_income_est                    0.156125\n",
       "flg_latest_being_lapse               0.000000\n",
       "flg_latest_being_cancel              0.000000\n",
       "tot_inforce_pols                     0.000000\n",
       "tot_cancel_pols                      0.000000\n",
       "f_hold_839f8a                        0.000000\n",
       "f_hold_e22a6a                        0.000000\n",
       "f_hold_d0adeb                        0.000000\n",
       "f_hold_c4bda5                        0.000000\n",
       "f_hold_ltc                           0.000000\n",
       "f_hold_507c37                        0.000000\n",
       "f_hold_gi                            0.000000\n",
       "f_elx                                0.000000\n",
       "f_mindef_mha                         0.000000\n",
       "f_retail                             0.000000\n",
       "flg_affconnect_show_interest_ever    0.972488\n",
       "flg_affconnect_ready_to_buy_ever     0.954758\n",
       "affcon_visit_days                    0.954758\n",
       "n_months_since_visit_affcon          0.954758\n",
       "hlthclaim_amt                        0.000000\n",
       "recency_hlthclaim                    0.000000\n",
       "recency_hlthclaim_success            0.929024\n",
       "hlthclaim_cnt_unsuccess              0.966096\n",
       "recency_hlthclaim_unsuccess          0.966096\n",
       "recency_hlthclaim_839f8a             0.984160\n",
       "recency_hlthclaim_14cb37             0.923577\n",
       "giclaim_amt                          0.000000\n",
       "recency_giclaim                      0.000000\n",
       "f_purchase_lh                        0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the proportion of NA values in each columns\n",
    "df.isna().sum()/(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 flg_substandard :   [ 0. nan  1.]\n",
      "2 flg_is_borderline_standard :   [ 0. nan  1.]\n",
      "3 flg_is_revised_term :   [ 0. nan  1.]\n",
      "4 flg_is_rental_flat :   [ 0. nan  1.]\n",
      "5 flg_has_health_claim :   [ 0. nan  1.]\n",
      "6 flg_has_life_claim :   [ 0. nan  1.]\n",
      "7 flg_gi_claim :   [ 0. nan  1.]\n",
      "8 flg_is_proposal :   [ 0. nan  1.]\n",
      "9 flg_with_preauthorisation :   [ 0. nan  1.]\n",
      "10 flg_is_returned_mail :   [ 0. nan  1.]\n",
      "11 is_consent_to_mail :   [ 0.  1. nan]\n",
      "12 is_consent_to_email :   [ 0.  1. nan]\n",
      "13 is_consent_to_call :   [ 0.  1. nan]\n",
      "14 is_consent_to_sms :   [ 0.  1. nan]\n",
      "15 is_valid_dm :   [ 1.  0. nan]\n",
      "16 is_valid_email :   [ 1.  0. nan]\n",
      "17 is_housewife_retiree :   [ 0. nan  1.]\n",
      "18 is_sg_pr :   [ 1.  0. nan]\n",
      "19 is_class_1_2 :   [ 1.  0. nan]\n",
      "20 is_dependent_in_at_least_1_policy :   [ 0. nan]\n",
      "21 f_ever_declined_la :   [nan  1.]\n",
      "22 hh_size :   [1.40277778 3.1372549  2.88709677 ... 4.0862069  2.55       3.21176471]\n",
      "23 flg_latest_being_lapse :   [0 1]\n",
      "24 flg_latest_being_cancel :   [0 1]\n",
      "25 tot_inforce_pols :   [ 3  1  5  2  9  7  4  6 15 11 10 14  8 12 19 13 16 17 20 25 21 18 22 26\n",
      " 27 31 23 29 54]\n",
      "26 tot_cancel_pols :   [0. 1. 3. 2. 4. 6.]\n",
      "27 f_hold_839f8a :   [0 1]\n",
      "28 f_hold_e22a6a :   [1 0]\n",
      "29 f_hold_d0adeb :   [0]\n",
      "30 f_hold_c4bda5 :   [0 1]\n",
      "31 f_hold_ltc :   [1 0]\n",
      "32 f_hold_507c37 :   [0 1]\n",
      "33 f_hold_gi :   [0]\n",
      "34 f_elx :   [0 1]\n",
      "35 f_mindef_mha :   [0 1]\n",
      "36 f_retail :   [1 0]\n",
      "37 flg_affconnect_show_interest_ever :   [nan  1.]\n",
      "38 flg_affconnect_ready_to_buy_ever :   [nan  1.]\n",
      "39 affcon_visit_days :   [nan  2.  1.  9.  4.  3.  7. 15.  5.  6. 12.  8. 40. 14. 50. 11. 13. 19.\n",
      " 16. 10. 27.]\n",
      "40 n_months_since_visit_affcon :   [nan  5.  0.  2.  3.  1.  4.]\n",
      "41 hlthclaim_amt :   [8.4785000e+02 8.7350000e+02 0.0000000e+00 5.6523000e+02 1.3516500e+04\n",
      " 2.5941970e+04 5.5568900e+03 1.2636480e+04 6.6725900e+03 4.2880820e+04\n",
      " 1.5249000e+02 1.3531000e+02 1.5280910e+04 9.2486700e+03 4.5386000e+03\n",
      " 5.7942880e+04 1.4514510e+04 1.9485200e+03 2.0835300e+03 1.1147800e+03\n",
      " 4.7801000e+02 6.8677000e+02 1.0735400e+03 3.6496500e+04 2.8581820e+04\n",
      " 5.2331202e+05 1.3658000e+03 1.5766720e+04 3.6643800e+03 2.0319700e+04\n",
      " 5.9474300e+03 4.4940000e+02 2.2099240e+04 4.8909000e+02 8.3101700e+03\n",
      " 1.7568548e+05 9.2510900e+03 4.1757000e+02 9.4140500e+03 9.0520000e+02\n",
      " 1.0169660e+04 1.1649030e+04 1.7655000e+03 3.9758220e+04 2.3408900e+04\n",
      " 4.8410500e+03 2.2103900e+03 3.3737810e+04 3.5949000e+02 1.3383400e+03\n",
      " 5.3899000e+02 1.0363100e+03 1.1524880e+04 7.5949600e+03 1.5775100e+03\n",
      " 2.8185500e+03 1.5130000e+03 6.1570760e+04 2.0177000e+03 4.8748200e+03\n",
      " 6.4255800e+03 6.5847100e+03 3.8008800e+03 1.2981320e+04 2.6381840e+04\n",
      " 3.0743100e+03 8.4113700e+03 2.4999210e+04 6.1629000e+02 1.2253500e+04\n",
      " 4.3337800e+03 8.9840000e+01 4.4901000e+02 6.3943000e+02 1.7830150e+04\n",
      " 4.8081300e+03 6.2351600e+03 7.6098000e+02 4.9389000e+02 6.5916000e+02\n",
      " 1.0233000e+02 7.3699000e+03 4.9793000e+02 2.7533000e+02 1.4815000e+02\n",
      " 6.2543820e+04 5.8605000e+02 2.2661240e+04 3.6984000e+02 2.5707900e+04\n",
      " 1.1368243e+05 7.2298900e+03 1.2038000e+02 8.1307000e+02 4.6233100e+03\n",
      " 2.3758800e+03 2.3119390e+04 5.5332400e+03 1.1665800e+03 1.4272000e+02\n",
      " 3.8049300e+03 1.8602780e+04 3.9643000e+02 2.0895700e+04 1.3976400e+03\n",
      " 7.4502700e+03 2.0461700e+03 7.0647300e+04 1.6335500e+03 8.9028000e+02\n",
      " 5.6497000e+02 1.4123470e+04 6.5701300e+03 6.0235000e+02 5.1113200e+03\n",
      " 1.5762710e+04 3.2673000e+03 3.9896660e+04 5.7581400e+03 3.5231000e+02\n",
      " 1.7302965e+05 7.7224000e+02 1.0514100e+03 5.2454500e+03 2.2030000e+02\n",
      " 1.0614670e+04 9.1596300e+03 8.6299000e+02 9.1457000e+02 9.2162360e+04\n",
      " 1.7160250e+04 1.2631730e+04 1.2545000e+02 3.5423300e+03 1.4354400e+03\n",
      " 1.1299600e+03 1.6327500e+03 2.2526100e+03 6.9595000e+02 1.7511240e+04\n",
      " 7.0201700e+03 4.5902620e+04 6.1456400e+04 2.4797000e+02 8.2818000e+02\n",
      " 1.0060900e+03 3.9558000e+02 1.6596760e+04 2.1633680e+04 1.4672926e+05\n",
      " 1.2785900e+03 1.5317890e+04 8.0829000e+03 8.7279000e+02 2.4963000e+03\n",
      " 1.6690620e+05 8.1395650e+04 6.2485700e+03 1.8665500e+03 1.1235000e+03\n",
      " 1.5448830e+04 1.4115000e+02 4.7704100e+03 2.5570900e+03 4.3965000e+02\n",
      " 7.2060000e+03 6.2100000e+01 8.4308400e+03 6.8346200e+03 9.6499800e+03\n",
      " 3.9290700e+03 2.2995400e+03 2.3179000e+03 8.9345000e+02 3.0918850e+04\n",
      " 5.5770000e+02 2.4022430e+04 3.2040530e+04 4.8333800e+03 1.1809700e+03\n",
      " 7.6254300e+03 1.1453530e+04 2.1363500e+03 9.8500200e+03 4.0050000e+01\n",
      " 4.7128230e+04 4.9283000e+02 6.1816800e+03 4.2498800e+03 1.8343600e+03\n",
      " 2.7985150e+04 2.9988400e+03 2.3377300e+03 2.7022000e+02 6.8776900e+03\n",
      " 8.0069000e+02 3.8536000e+02 2.2680930e+04 9.7600430e+04 6.9811140e+04\n",
      " 3.1399160e+04 8.9612000e+02 2.5085500e+03 1.1571090e+04 7.1562800e+03\n",
      " 7.7767000e+02 4.4296900e+03 8.8519000e+02 1.3590000e+01 1.5086800e+03\n",
      " 1.5244410e+04 1.2142800e+03 7.2652800e+03 1.2093450e+04 9.0757210e+04\n",
      " 2.2069300e+03 2.1489060e+04 3.8774060e+04 2.0145650e+04 4.0240000e+03\n",
      " 1.1340000e+03 1.2667340e+04 1.1203900e+03 3.5251900e+03 3.1961800e+03\n",
      " 1.2174900e+03 1.4000000e+03 7.9319000e+02 6.1909100e+03 1.1827630e+04\n",
      " 3.9952200e+03 4.6968700e+03 7.4549400e+03 2.3000400e+03 9.5014600e+03\n",
      " 9.8000000e+01 1.4264590e+04 1.5410000e+01 1.9103000e+02 1.5642530e+04\n",
      " 2.3016250e+04 9.2973400e+03 2.0366700e+03 1.2133000e+02 1.0495900e+03\n",
      " 7.4849400e+03 3.4675900e+03 6.5187200e+04 3.2340500e+03 1.2766700e+04\n",
      " 1.6650240e+04 3.1647540e+04 1.4343880e+04 4.4493120e+04 4.0055500e+03\n",
      " 2.9692140e+04 7.6600000e+02 1.0979440e+04 5.2014960e+04 2.5415900e+03\n",
      " 3.9540900e+03 5.9143300e+03 1.5093000e+02 4.0271400e+03 3.8927640e+04\n",
      " 8.8769200e+03 2.4932300e+03 2.6720200e+03 2.9919000e+02 2.7059200e+03\n",
      " 4.6739000e+02 3.4787200e+03 3.2098360e+04 1.1079200e+03 3.8126000e+02\n",
      " 1.1377880e+04 1.6286660e+04 2.5166100e+03 1.4554100e+03 5.7320700e+03\n",
      " 8.8525400e+03 1.5000000e+02 1.8435400e+03 1.8764760e+04 8.4483000e+02\n",
      " 1.6437100e+03 1.7767390e+04 4.1709100e+03 5.1840910e+04 2.5049700e+03\n",
      " 5.9867000e+02 2.0047000e+02 7.0070800e+03 3.5426050e+04 2.4291270e+04\n",
      " 3.8193000e+03 3.5120200e+03 4.1625520e+04 1.9479500e+03 7.2830000e+01\n",
      " 2.8142650e+04 2.2920500e+03 2.6981300e+03 1.1727480e+04 3.2185400e+04\n",
      " 7.9390000e+02 7.7133000e+02 1.0940000e+02 2.1917938e+05 1.2680400e+03\n",
      " 2.0384470e+04 5.9103200e+03 4.4359100e+03 1.3817120e+04 2.8636000e+02\n",
      " 1.1296780e+04 4.2583000e+04 5.1630000e+02 2.0686560e+04 5.7491000e+02\n",
      " 4.0964700e+03 1.5634130e+04 1.6211000e+02 1.2258330e+04 1.6324500e+03\n",
      " 1.0808490e+04 9.6954080e+04 4.1885700e+03 2.6702300e+03 1.3575385e+05\n",
      " 1.6410700e+03 1.2934340e+04 1.2565110e+04 1.8786470e+04 5.1904840e+04\n",
      " 5.3572800e+03 7.7364000e+02 2.4500000e+02 2.7935960e+04 9.5959400e+03\n",
      " 2.3100100e+03 4.4332000e+02 1.8887100e+04 8.9176000e+02 8.7226140e+04\n",
      " 1.7199260e+04 8.5116000e+02 1.4277740e+04 1.6927000e+03 2.4017980e+04\n",
      " 3.9333600e+03 1.0353600e+03 1.0637427e+05 1.6218007e+05 1.5607020e+04\n",
      " 1.7513520e+04 3.3642000e+02 1.5382860e+04 6.0562000e+02 1.1652100e+04\n",
      " 3.9689000e+02 4.1202000e+02 4.6996270e+04 1.8777000e+02 4.5702210e+04\n",
      " 1.1531410e+04 3.8837200e+04 6.1005000e+02 1.4200440e+04 9.7263000e+02\n",
      " 9.4788600e+03 2.1506380e+04 7.3553000e+02 5.6418510e+04 6.4128060e+04\n",
      " 4.2544000e+02 2.6934590e+04 4.1790100e+03 5.2617000e+02 2.9060500e+03\n",
      " 3.8303800e+03 6.4212300e+03 9.2504100e+03 1.8632100e+03 9.8850100e+03\n",
      " 5.8669000e+02 8.5590000e+02 1.7860600e+03 1.6417060e+04 1.9617250e+04\n",
      " 9.4884800e+03 5.2214200e+03 1.4205540e+04 6.5530500e+03 3.3458900e+03\n",
      " 2.1406420e+04 2.4697410e+04 1.1450000e+03 7.6044000e+02 1.4742000e+04\n",
      " 2.2298610e+04 1.2414600e+03 2.9395100e+03 6.2919500e+03 1.1207410e+04\n",
      " 2.9980900e+04 5.1090900e+03 3.2654100e+03 3.3088700e+03 2.7451380e+04\n",
      " 7.0537000e+03 1.2194258e+05 9.2682290e+04 4.6205000e+02 3.9003000e+03\n",
      " 5.9971300e+03 2.6138900e+03 7.5201000e+02 2.1816760e+04 1.8567000e+02\n",
      " 1.2904120e+04 1.8841300e+03 7.1329300e+03 6.9020300e+03 9.6200500e+03\n",
      " 3.5785500e+03 4.9072480e+04 3.9760800e+03 3.6494400e+03 2.8970000e+02\n",
      " 4.9088130e+04 1.2608700e+03 2.6497840e+04 3.0443000e+02 4.3335000e+02\n",
      " 1.7100000e+01 1.1913510e+04 8.8783000e+02 2.0552900e+03 3.4020600e+03\n",
      " 3.5927200e+03 5.7744000e+03 3.8128000e+03 5.5796600e+03 3.4215000e+02\n",
      " 9.9270300e+03 7.1575700e+03 2.3893300e+03 2.3363000e+03 1.2051750e+04\n",
      " 7.9715000e+02 1.8769850e+04 1.5051900e+03 6.8964400e+03 1.1123910e+04\n",
      " 2.2951500e+03 7.4850210e+04 1.6100540e+04 1.0000000e-02 5.1774850e+04\n",
      " 1.2375000e+02 6.6568000e+02 1.1143091e+05 2.4722840e+04 4.1845700e+03\n",
      " 5.1701000e+02 1.5372000e+03 3.5387900e+03 3.6024680e+04 1.9915400e+03\n",
      " 7.1394100e+03 1.7371000e+02 4.6751800e+03 3.8215000e+02 5.6737730e+04\n",
      " 3.0264400e+03 2.9560100e+03 5.1870000e+01 2.9055400e+03 2.4818580e+04\n",
      " 1.9750000e+01 8.4040400e+03 5.5079000e+02 2.3795470e+04 7.1105000e+02\n",
      " 3.3204400e+03 2.4508940e+04 2.6986000e+02 3.2459400e+03 1.0166820e+04\n",
      " 6.9645900e+03 2.5769800e+03 4.1919900e+03 3.1940200e+03 1.4258100e+03\n",
      " 4.2493000e+02 1.4481400e+04 2.3512100e+03 1.7049740e+04 2.5951000e+02\n",
      " 6.4489020e+04 6.5051000e+03 1.0356340e+04 5.8472000e+02 6.3386240e+04\n",
      " 2.6994200e+03 2.1428510e+04 3.0010100e+03 6.1315750e+04 4.8730700e+03\n",
      " 2.9000000e+03 3.7798200e+03 6.3890000e+03 9.0801500e+03 7.2297660e+04\n",
      " 2.6140000e+02 2.0987160e+04 7.1280100e+03 1.0192490e+04 1.0557230e+04\n",
      " 2.5227200e+03 3.5109000e+03 1.6799010e+04 6.7287000e+02 8.1361400e+03\n",
      " 4.0697700e+03 2.2368080e+04 1.0090000e+03 1.5875500e+03 4.4928600e+03\n",
      " 1.0413080e+04 6.7415000e+02 1.4410400e+03 4.9651900e+03 3.9208600e+04\n",
      " 1.3775000e+03 1.5699300e+03 9.5059500e+03 4.8739900e+03 1.2226030e+04\n",
      " 2.5204600e+04 1.5169300e+03 2.1923770e+04 4.2541920e+04 2.7232730e+04\n",
      " 5.0730780e+04 1.0022950e+04 8.0781650e+04 8.2000000e+02 5.1519000e+02\n",
      " 2.4949200e+03 2.2138740e+04 1.2503090e+04 4.8125000e+03 4.3225030e+04\n",
      " 5.1366300e+03 1.8192800e+03 1.5200000e+02 8.3604500e+03 7.4550000e+03\n",
      " 1.9020600e+03 2.2064640e+04 2.0596400e+03 2.2580000e+02 1.3249650e+04\n",
      " 4.3995000e+02 4.9576000e+02 1.6338040e+04 1.2199200e+04 1.8497380e+04\n",
      " 9.2144400e+03 2.5389000e+03 8.7426960e+04 2.7214030e+04 9.8414700e+03\n",
      " 1.0619610e+04 4.2236972e+05 1.7079420e+04 8.4248000e+02 2.6874420e+04\n",
      " 4.1690900e+03 1.0639760e+04 4.9135690e+04 2.6305000e+03 8.1200000e+01\n",
      " 2.6347000e+02 1.2394500e+03 2.3809000e+02 5.8349550e+04 2.8785000e+03\n",
      " 1.3271920e+04 3.0090010e+04 2.4840600e+04 1.1008700e+03 2.8113300e+03\n",
      " 4.2719400e+03 2.7464400e+03 1.5090630e+04 3.0851480e+04 2.7511530e+04\n",
      " 2.4920000e+02 2.0500000e+03 1.1938370e+04 1.2945690e+04 4.0289000e+02\n",
      " 1.4761870e+04 6.1372000e+03 7.2549670e+04 6.7377900e+03 3.0045900e+03\n",
      " 2.3486620e+04 6.9610000e+02 5.6905790e+04 3.5922992e+05 1.6468610e+04\n",
      " 2.4392600e+04 2.7753600e+03 7.7593400e+03 6.9678800e+03 6.1526000e+02\n",
      " 7.4986000e+02 1.1233010e+04 3.6111500e+03 7.9384000e+02 2.4626000e+02\n",
      " 6.6544500e+03 1.5121110e+04 1.1997700e+03 5.7976000e+02 7.7287900e+03\n",
      " 9.0513400e+03 9.0935400e+03 5.3374900e+03 6.3679000e+02 6.5180000e+02\n",
      " 1.7492770e+04 5.7811000e+02 2.8330700e+03 1.1108040e+04 4.5100000e+02\n",
      " 3.0749200e+03 2.9315300e+03 3.6272000e+02 1.9341480e+04 7.5754800e+03\n",
      " 1.6743000e+02 4.1766000e+03 1.5568200e+03 1.7674100e+03 3.8626300e+03\n",
      " 6.0735000e+02 9.0500000e+02 3.0278000e+02 3.9445000e+02 5.7147000e+03\n",
      " 1.7450000e+02 4.4240960e+04 5.4215600e+03 4.9157850e+04 4.8819080e+04\n",
      " 3.7264500e+03 2.0593400e+03 3.1425000e+02 6.1843000e+02 6.1456900e+03\n",
      " 2.4656340e+04 4.5000000e+01 2.0275000e+03 1.3489950e+04 4.6924000e+02\n",
      " 2.5671000e+02 4.7596000e+02 5.3689500e+03 1.2096010e+04 1.5542360e+04\n",
      " 2.1107692e+05 6.3524700e+03 1.7275000e+03 5.7793900e+03 1.0229200e+03\n",
      " 3.2797000e+02 5.2744500e+03 9.7511000e+02 3.6024980e+04 1.8885400e+04\n",
      " 2.0815300e+03 1.9996810e+04 1.3368489e+05 2.2867090e+04 6.8114500e+03\n",
      " 4.8289100e+03 1.2278220e+04 8.2262000e+02 1.6443090e+04 2.0133300e+03\n",
      " 3.4010000e+02 1.5976300e+04 1.4599300e+03 3.8260600e+03 3.0000000e+02\n",
      " 1.5599500e+03 2.4214746e+05 1.6028100e+03 4.5622900e+03 2.3689300e+03\n",
      " 7.8638700e+03 1.6414250e+04 1.0358520e+04 1.4006800e+03 1.5120000e+02\n",
      " 1.8384670e+04 9.3931600e+04 3.7992200e+03 7.3916000e+02 2.3436600e+03\n",
      " 1.0738220e+04 1.4894600e+03 1.3482100e+04 2.3573000e+03 7.9219000e+03\n",
      " 3.6326470e+04 3.4222300e+03 1.0644040e+04 1.5355900e+03 4.8357390e+04\n",
      " 4.5168600e+03 3.5559120e+04 1.3379200e+03 3.2189800e+03 7.3081000e+03\n",
      " 2.3824000e+03 3.8699200e+03 2.5260660e+04 4.7796570e+04 1.4142800e+03\n",
      " 2.6794200e+03 3.7604000e+03 3.4100000e+03 4.9281100e+04 2.8346500e+03\n",
      " 4.2253610e+04 3.4847400e+03 2.5781250e+04 5.6981000e+02 9.1317800e+03\n",
      " 4.6560000e+02 9.5188670e+04 7.6769000e+02 7.1039000e+02 1.1158200e+03\n",
      " 8.5344900e+03 4.5238910e+04 2.5653000e+02 6.2732300e+03 2.9603830e+04\n",
      " 7.9872600e+03 1.8197390e+04 1.7860200e+03 1.5060726e+05 2.3534420e+04\n",
      " 7.5874000e+02 1.5024340e+04 8.2328000e+02 2.2807300e+03 6.7371700e+03\n",
      " 1.5683400e+03 4.8527800e+03 2.0693800e+03 1.5690100e+03 9.6427100e+03\n",
      " 1.1578470e+04 2.5921000e+02 2.4192900e+03 8.7118000e+02 5.4761830e+04\n",
      " 2.4310590e+04 4.2939400e+03 5.3933600e+03 1.7047500e+03 1.2102440e+04\n",
      " 3.5944400e+03 8.8500000e+01 3.3383000e+02 7.0529800e+03 1.3990170e+04\n",
      " 5.5050000e+02 3.2235000e+02 5.4071600e+03 1.2298000e+03 7.5849800e+03\n",
      " 1.3840600e+03 3.6366400e+03 1.4707520e+04 1.0324670e+04 2.1453000e+02\n",
      " 2.6274750e+04 3.1305260e+04 1.1574000e+03 1.5316683e+05 1.0188100e+03\n",
      " 4.8437000e+02 1.3589000e+03 1.7299100e+03 1.0256690e+04 2.7297000e+04\n",
      " 1.9786210e+04 2.3257000e+03 4.0604000e+03 7.8315200e+03 8.2333000e+02\n",
      " 9.3926200e+03 7.3661700e+03 2.4842770e+04 3.4090800e+03 9.1997900e+03\n",
      " 2.1720000e+01 1.7240400e+03 8.3430200e+03 4.1982400e+03 1.8441300e+03\n",
      " 7.9973700e+03 1.4936170e+04 6.1220220e+04 1.3340000e+02 3.7720000e+03\n",
      " 2.2500000e+01 2.9459500e+03 5.0932000e+02 2.0150800e+03 3.2855570e+04\n",
      " 3.5203000e+03 1.4401620e+04 4.0587740e+04 1.6260700e+03 8.1598000e+02\n",
      " 4.3318000e+03 3.5055400e+03 8.5742900e+03 4.4493500e+03 7.0804200e+03\n",
      " 1.0214500e+03 5.7070000e+02 3.4757300e+03 5.4103600e+03 1.1247990e+04\n",
      " 7.8000000e+02 1.9779167e+05 2.0447300e+03 8.2782000e+03 8.3474000e+02\n",
      " 2.6126760e+04 6.1020000e+02 8.5453730e+04 1.3294000e+04 3.5823900e+03\n",
      " 2.5498000e+02 2.2353670e+04 4.5327190e+04 1.2904940e+04 7.7349000e+03\n",
      " 1.5195190e+04 9.8664000e+02 1.1374690e+04 2.7240000e+02 2.4900000e+03\n",
      " 6.9420050e+04 1.5011000e+02 3.1858700e+03 6.1454900e+03 1.2453700e+04\n",
      " 2.4177800e+04 5.5415800e+03 3.0200300e+03 5.2149800e+03 7.5170700e+03\n",
      " 8.2529000e+02 2.1502650e+04 1.1699400e+03 2.6269800e+03 6.2688690e+04\n",
      " 8.6577400e+03 1.0382520e+04 1.3858870e+04 4.5772300e+03 2.8993300e+03\n",
      " 1.9211990e+04 8.3975000e+02 3.0262800e+03 1.6042300e+03 1.5219540e+04\n",
      " 4.5566900e+03 1.1447940e+04 6.5404000e+02 3.5920000e+02 3.5505400e+03\n",
      " 1.2554200e+03 1.5330059e+05 9.9300000e+01 1.9449780e+04 5.5106000e+02\n",
      " 2.7289630e+04 1.2335394e+05 1.0798220e+04 3.1018100e+03 8.3222700e+03\n",
      " 5.8072420e+04 5.2751000e+02 3.7438800e+03 2.8086700e+03 1.1325690e+04]\n",
      "42 recency_hlthclaim :   [0 1]\n",
      "43 recency_hlthclaim_success :   [ nan  11.  18.  71.  10.  88.  80.  83.  44.  15.   6.  66.  23.  32.\n",
      "  87.  12.  53.  50. 120.  67.   2.  48.  33.  17.   7.   9.  13.  21.\n",
      "   0.  29. 105.   1.  58.  41.  22.  64.  45.  62.  20.  75.  77.  89.\n",
      "   8.  99.  69.  42.   5.  34.  31.  85. 122. 121.  55.  52. 108.  74.\n",
      " 101.   3. 118.  60. 123.  26.  35.  36. 126.  56.  16.   4.  28.  93.\n",
      " 109.  90.  37.  84. 102.  24.  91.  86.  14.  92. 119.  47.  27. 111.\n",
      "  78.  49. 103.  46.  76.  79.  38.  51.  30.  65.  61.  57. 112.  68.\n",
      "  98.  25. 110.  73. 114.  59.  70.  40.  94.  63. 107.  19.  43.  39.\n",
      " 100.  81.  72.  82. 113. 106.  97. 124.  54. 104. 115. 125. 116. 117.]\n",
      "44 hlthclaim_cnt_unsuccess :   [nan  1.  2.  3.  4.  5.  6.  7.  8. 10. 12.  9. 19. 17. 23. 68. 13. 33.]\n",
      "45 recency_hlthclaim_unsuccess :   [ nan 111.  86. 112.  23.  88.  45.  80.  17.  53.  66. 120.  40.  11.\n",
      "  37.  79.  56.   1. 118.  13.  99. 126.  47.   8.   5.  31.  48.  43.\n",
      "  34.  12.  30.  35.  59.  24.  72. 123.   7.  28. 119.  33.  46.  91.\n",
      "  16.  93.   0.  90.  65.   4.  14.  52.  18.   3.  51. 104.  19.  71.\n",
      "  10. 102.  41.  49.  89.  32.  58.  22.  78. 107.  76. 101.  36.  38.\n",
      " 106.  68.   9.  61.  62.  57.  21.   2.   6.  25.  74.  55.  50.  85.\n",
      "  94.  64.  77.  26.  15.  83.  39.  63.  54.  42.  73. 117.  82. 103.\n",
      "  81.  69.  67.  44. 115. 108. 100. 110.  95.  60. 109.  27.  84.  98.\n",
      "  20.  92. 114.  29. 116.  87. 121. 105. 122. 125. 113.  75.]\n",
      "46 recency_hlthclaim_839f8a :   [ nan  11.  80.  22.  23.  44.  53.  25.  50.   2.  17.  42. 105.  75.\n",
      "  19.   1.  64.  62.  99.  14.   9.   5.  31. 101.  35. 108.  77.  18.\n",
      " 123.  20.  54.  63. 122.  46.   7.  34.  57.  89.  47. 124.   6.  32.\n",
      "  13.  24. 109.  59.  85.  74.  40.  60.  49.  93.  76.  26.   0.  68.\n",
      "  51.  90.  81.   3.  43.   4.  37.  12. 100.  79.  27.  41.  92.  29.\n",
      "  48.  83.  82.  91.  73.  84.  86.  16.  67.   8. 120.  95.  10.  36.\n",
      "  28.  15.  72. 110.  21. 113.  66. 112.  87.  55.  33. 118.  94.  45.\n",
      "  98. 115. 111. 114.  70.  69.  39.]\n",
      "47 recency_hlthclaim_14cb37 :   [ nan  11.  71. 111.  86. 112.  88.  80.  83.  59.  15.   6.  23.  17.\n",
      "  12.  53.  66.  50. 120.  67.   2.  48.  33.   7.   9.  13.  18.   0.\n",
      " 105.  37.   1.  58. 118.  22.  64.  45.  62.  20.  75.  77.  89.   8.\n",
      "  99.  69. 126.  47.   5.  34.  31.  85.  44. 122. 121.  55.  30. 108.\n",
      "  78.  24. 123. 101.   3.  60.  74.  26.  35.  70.  56.   4.  46.  91.\n",
      "  16.  42.  28.  72.  93. 109.  90.  65.  52.  36.  84.  51. 102.  40.\n",
      "  10.  14.  92.  41.  49.  19. 119.  32.  27. 103. 107.  21.  76.  79.\n",
      "  38. 106.  68.  57.  98.  25. 114.  29.  94.  63. 104.  39.  43.  54.\n",
      "  73.  87. 117. 100.  81. 110.  61. 113.  82. 124. 115.  97. 116. 125.]\n",
      "48 giclaim_amt :   [4.000000e+02 3.898840e+03 1.472500e+03 8.147800e+02 2.821000e+02\n",
      " 1.090936e+04 1.735200e+02 4.320600e+02 3.506897e+04 0.000000e+00\n",
      " 3.824600e+02 1.333278e+04 3.784300e+02 7.282200e+02 1.225940e+03\n",
      " 4.863290e+03 3.681000e+01 4.909100e+02 4.731000e+02 7.303740e+03\n",
      " 5.602000e+01 9.227000e+01 1.000000e+02 8.157300e+02 4.612170e+03\n",
      " 1.670430e+03 2.500000e+02 7.500000e+02 6.954300e+02 8.425000e+02\n",
      " 2.375000e+02 2.420000e+02 8.509303e+04 2.200000e+03 3.500000e+02\n",
      " 2.000000e+01 3.296700e+02 4.329100e+02 1.000000e+03 7.921900e+02\n",
      " 3.822490e+03 1.241200e+02 9.735000e+01 1.200000e+02 4.070000e+02\n",
      " 6.000000e+02 3.516900e+02 7.014510e+03 2.000000e+02 1.210000e+02\n",
      " 1.600000e+03 1.170000e+02 3.780272e+04 1.357100e+03 3.146928e+04\n",
      " 2.878500e+02 9.480450e+03 3.000000e+02 3.825760e+03 2.402190e+03\n",
      " 6.198980e+03 5.410800e+02 5.575000e+02 4.778000e+02 2.574250e+03\n",
      " 9.027500e+02 9.077500e+02 2.100360e+04 4.110900e+02 5.000000e+01\n",
      " 7.000000e+02 1.509980e+03 1.068650e+03 2.698000e+02 3.588870e+03\n",
      " 9.450000e+01 1.420200e+02 1.808500e+03 2.105000e+02 1.633481e+04\n",
      " 1.062000e+05 1.040950e+03 1.500000e+02 1.494696e+04 7.880000e+02\n",
      " 7.777100e+02 8.436800e+02 1.290000e+02 2.453500e+02 1.185700e+02\n",
      " 1.542950e+03 2.520000e+02 1.002750e+03 4.660000e+02 3.882800e+02\n",
      " 5.250000e+01 7.940900e+02 1.744990e+03 4.800000e+02 1.974500e+02\n",
      " 5.500000e+01 1.584535e+04 5.810000e+02 9.500000e+01 1.615200e+02\n",
      " 2.461000e+02 6.105000e+02 5.000000e+02 5.500000e+02 1.787715e+04\n",
      " 2.920000e+02 7.497000e+03 9.219500e+02 9.876000e+01 2.206480e+03\n",
      " 2.455700e+02 9.000000e+01 6.381000e+02 1.193000e+03 4.100000e+01\n",
      " 1.133750e+03 6.612730e+03 1.805000e+02 7.830000e+03 4.646490e+03\n",
      " 4.544150e+03 1.440900e+02 6.723000e+02 5.330200e+02 4.225000e+02\n",
      " 1.263500e+02 1.071012e+04 3.202600e+02 2.924290e+03 5.075000e+02\n",
      " 7.306700e+02 4.400701e+04 2.400000e+03 1.393050e+03 7.711000e+02\n",
      " 3.643200e+02 4.725804e+04 8.920000e+01 7.400000e+01 5.818090e+03\n",
      " 1.759940e+03 2.033300e+02 7.475000e+02 1.285500e+02 4.400000e+02\n",
      " 1.001900e+03 2.646930e+03 6.703587e+04 2.419990e+03 1.044720e+03\n",
      " 7.470000e+02 9.698000e+01 4.627500e+02 8.325000e+02 6.010300e+02\n",
      " 2.195000e+03 2.052600e+04 8.000020e+03 1.629363e+04 1.609300e+02\n",
      " 5.620000e+01 4.863400e+03 3.109440e+03 5.300000e+03 3.605300e+02\n",
      " 9.501260e+03 1.223200e+02 1.478500e+02 4.906500e+02 2.530000e+03\n",
      " 3.725000e+02 3.213260e+04 4.349320e+03 4.158020e+03 1.062750e+03\n",
      " 1.789250e+03 5.550000e+02 5.827500e+02 8.499200e+02 1.605000e+02\n",
      " 1.698190e+03 1.030000e+02 1.056792e+04 6.997500e+02 5.111700e+02\n",
      " 7.980000e+02 2.524768e+04 1.932162e+04 4.806900e+02 1.396000e+02\n",
      " 4.993500e+02 1.585720e+03 7.955400e+02 1.323367e+04 4.665530e+03\n",
      " 1.242100e+02 1.316200e+02 1.240130e+03 8.471000e+01 4.528700e+02\n",
      " 6.035800e+02 2.048700e+03 1.115630e+03 4.935700e+02 1.367884e+04\n",
      " 6.607000e+01 8.200000e+01 1.945900e+02 1.100000e+02 1.126000e+03\n",
      " 5.570000e+01 2.162850e+04 5.736840e+03 4.049600e+02 3.300000e+01\n",
      " 3.376000e+02 3.730950e+03 1.620030e+03 1.892500e+03 4.815000e+01\n",
      " 2.370000e+02 2.120000e+02 5.402000e+02 2.778159e+04 1.274340e+03\n",
      " 1.030520e+03 8.327500e+02 7.227500e+02 9.827000e+01 5.527500e+02\n",
      " 4.994640e+03 3.813500e+02 5.509220e+03 8.000000e+02 5.305050e+03\n",
      " 2.080550e+03 5.816500e+02 4.927500e+02 3.767600e+03 7.827400e+03\n",
      " 1.016117e+04 6.320000e+02 8.182950e+03 4.907294e+04 1.207500e+02\n",
      " 6.400000e+01 1.662500e+03 7.775000e+01 3.150000e+02 1.790000e+02\n",
      " 7.900000e+01 3.926540e+03 3.683000e+02 6.790000e+01 3.774200e+02\n",
      " 2.532000e+02 8.000000e+01 1.050000e+03 1.458000e+02 8.827500e+02\n",
      " 1.938530e+03 3.604900e+02 7.877500e+02 4.510000e+01 3.468110e+03\n",
      " 9.500000e+02 2.925000e+02 1.663500e+02 4.121550e+03 7.879000e+01\n",
      " 1.904890e+03 2.922730e+03 1.920000e+02 1.362750e+03 5.332500e+02\n",
      " 6.100000e+01 2.617900e+03 2.064120e+04 1.234486e+04 8.493750e+03\n",
      " 4.431332e+04 1.402400e+03 2.452260e+03 8.240000e+02 2.271900e+02\n",
      " 2.438210e+03 4.539740e+03 1.283300e+02 1.800000e+03 1.503400e+03\n",
      " 2.125970e+03 5.786000e+02 5.152000e+02 1.507500e+02 3.673630e+03\n",
      " 5.732750e+03 1.215407e+04 2.501795e+04 1.720947e+04 5.340000e+02\n",
      " 2.946100e+02 7.845300e+03 2.401200e+02 2.979000e+02 3.813850e+03\n",
      " 6.278480e+03 7.162900e+03 6.285400e+02 2.573200e+02 1.713000e+01\n",
      " 9.527500e+02 3.220952e+04 1.661690e+03 9.327500e+02 6.275000e+02\n",
      " 2.480000e+03 1.017750e+03 3.942380e+04]\n",
      "49 recency_giclaim :   [0 1]\n",
      "50 f_purchase_lh :   [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# find the distinct/unique values in each numerical columns of the dataset\n",
    "id = 1\n",
    "for col in df_numeric.columns:\n",
    "    print(id, col, \":  \", df[col].unique())\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"is_dependent_in_at_least_1_policy\", \"f_ever_declined_la\", \"flg_affconnect_show_interest_ever\", \"flg_affconnect_ready_to_buy_ever\", \"affcon_visit_days\" are columns that have either [nan 1] or [0 nan], or in a similar form. Hence, nan values will be converted to either 0 or 1 depending on the counterpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"flg_affconnect_ready_to_buy_ever\"] = df[\"flg_affconnect_ready_to_buy_ever\"].fillna(0)\n",
    "df[\"flg_affconnect_show_interest_ever\"] = df[\"flg_affconnect_show_interest_ever\"].fillna(0)\n",
    "df[\"f_ever_declined_la\"] = df[\"f_ever_declined_la\"].fillna(0)\n",
    "df[\"is_dependent_in_at_least_1_policy\"] = df[\"is_dependent_in_at_least_1_policy\"].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(columns=[\"f_purchase_lh\"])\n",
    "# y = df[\"f_purchase_lh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# print('Before:', Counter(y_train))\n",
    "# X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "# print('After:', Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_concat = pd.concat([X_train,y_train], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier \n",
    " \n",
    "# model = RandomForestClassifier() \n",
    "# model.fit(X_train, y_train) \n",
    " \n",
    "# feature_importances = pd.Series(model.feature_importances_, index=X_train.columns) \n",
    "# top_features = feature_importances.nlargest(7).index \n",
    "# X_train_selected = X_train[top_features] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # instantiate the model (using the default parameters)\n",
    "# logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "# # fit the model with data\n",
    "# logreg.fit(X_train_selected, y_train)\n",
    "\n",
    "# y_pred = logreg.predict(X_test[top_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "\n",
    "# cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay \n",
    " \n",
    " \n",
    "# cnf_matrix = confusion_matrix(y_test, y_pred) \n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix) \n",
    "# disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay \n",
    "# from sklearn.tree import DecisionTreeClassifier \n",
    " \n",
    "# dt_clf = DecisionTreeClassifier() \n",
    "# dt_clf.fit(X_train, y_train) \n",
    " \n",
    "# y_test_pred = dt_clf.predict(X_test) \n",
    "# print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= df.drop([\"clntnum\", \"race_desc\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority of the clients are Singaporean as shown below. Hence, we are going to focus on Singaporean clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19550    Singapore\n",
       "4600     Singapore\n",
       "13337    Singapore\n",
       "15074    Singapore\n",
       "19724    Singapore\n",
       "           ...    \n",
       "11284    Singapore\n",
       "11964    Singapore\n",
       "5390     Singapore\n",
       "860      Singapore\n",
       "15795    Singapore\n",
       "Name: ctrycode_desc, Length: 17992, dtype: object"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ctrycode_desc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Singapore               17789\n",
       "Malaysia                   85\n",
       "Not Applicable             20\n",
       "Indonesia                  18\n",
       "Australia                  10\n",
       "United Kingdom              9\n",
       "United States               5\n",
       "China                       4\n",
       "Thailand                    3\n",
       "Taiwan (R.O.C)              3\n",
       "Philippines                 3\n",
       "Unknown Country Code        3\n",
       "United Arab Emirates        3\n",
       "Hong Kong                   2\n",
       "Japan                       2\n",
       "Denmark                     2\n",
       "South Africa                2\n",
       "Spain                       1\n",
       "Brunei Darussalam           1\n",
       "Sweden                      1\n",
       "Ireland                     1\n",
       "New Zealand                 1\n",
       "Netherlands                 1\n",
       "Italy                       1\n",
       "Bosnia-Herzegovina          1\n",
       "Canada                      1\n",
       "Name: ctrycode_desc, dtype: int64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ctrycode_desc\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"ctrycode_desc\"] == \"Singapore\"]\n",
    "df = df.drop(\"ctrycode_desc\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 cltdob_fix :   ['1974-05-09' '1979-11-11' '1976-01-28' ... '1948-12-16' '1967-12-06'\n",
      " '1970-05-15']\n",
      "2 flg_substandard :   [ 0. nan  1.]\n",
      "3 flg_is_borderline_standard :   [ 0. nan  1.]\n",
      "4 flg_is_revised_term :   [ 0. nan  1.]\n",
      "5 flg_is_rental_flat :   [ 0. nan  1.]\n",
      "6 flg_has_health_claim :   [ 0. nan  1.]\n",
      "7 flg_has_life_claim :   [ 0. nan  1.]\n",
      "8 flg_gi_claim :   [ 0. nan  1.]\n",
      "9 flg_is_proposal :   [ 0. nan  1.]\n",
      "10 flg_with_preauthorisation :   [ 0. nan  1.]\n",
      "11 flg_is_returned_mail :   [ 0. nan  1.]\n",
      "12 is_consent_to_mail :   [ 0.  1. nan]\n",
      "13 is_consent_to_email :   [ 0.  1. nan]\n",
      "14 is_consent_to_call :   [ 0.  1. nan]\n",
      "15 is_consent_to_sms :   [ 0.  1. nan]\n",
      "16 is_valid_dm :   [ 1.  0. nan]\n",
      "17 is_valid_email :   [ 1.  0. nan]\n",
      "18 is_housewife_retiree :   [ 0. nan  1.]\n",
      "19 is_sg_pr :   [ 1.  0. nan]\n",
      "20 is_class_1_2 :   [ 1.  0. nan]\n",
      "21 is_dependent_in_at_least_1_policy :   [0. 1.]\n",
      "22 f_ever_declined_la :   [0. 1.]\n",
      "23 hh_20 :   ['144' '153' '62' '1' '114' '138' '131' '233' '109' '58' '107' '125' None\n",
      " '68' '84' '102' '60' '221' '72' '112' '100' '59' '159' '90' '88' '81'\n",
      " '116' '82' '214' '92' '98' '191' '32' '44' '120' '101' '135' '94' '48'\n",
      " '211' '29' '127' '67' '35' '76' '70' '86' '33' '37' '148' '145' '26'\n",
      " '186' '40' '288' '108' '163' '219' '61' '78' '111' '93' '20' '24' '45'\n",
      " '6' '50' '9' '162' '36' '130' '91' '119' '14' '69' '298' '73' '87' '147'\n",
      " '265' '77' '51' '42' '118' '34' '46' '55' '271' '83' '22' '89' '75' '126'\n",
      " '158' '85' '106' '146' '12' '63' '5' '19' '336' '80' '223' '123' '38'\n",
      " '47' '0' '79' '117' '110' '115' '53' '164' '95' '193' '113' '96' '124'\n",
      " '7' '74' '64' '129' '341' '133' '66' '143' '169' '228' '65' '18' '54'\n",
      " '226' '10' '105' '134' '142' '185' '156' '103' '190' '351' '225' '232'\n",
      " '15' '57' '206' '56' '27' '151' '141' '260' '248' '52' '41' '25' '121'\n",
      " '149' '137' '17' '201' '23' '440' '39' '104' '261' '212' '207' '173'\n",
      " '136' '200' '187' '31' '16' '4' '99' '195' '122' '43' '13' '250' '189'\n",
      " '176' '229' '154' '21' '172' '183' '155' '71' '8' '203' '28' '286' '157'\n",
      " '174' '170' '338' '30' '182' '175' '140' '49' '216' '287' '305' '192'\n",
      " '213' '150' '194' '322' '11' '160' '227' '252' '97' '234' '168' '165'\n",
      " '204' '139' '291' '374' '238' '277' '152' '276' '222' '132' '400' '177'\n",
      " '181' '128' '231' '266' '202' '290' '178' '196' '254' '215' '3' '161'\n",
      " '367' '246' '434' '205' '188' '171' '253' '264' '240' '259' '275' '197'\n",
      " '167' '263' '285' '2' '293' '346' '209' '236' '230' '300' '258' '247'\n",
      " '278' '348' '166' '243' '217' '340' '324' '198' '303' '218' '379' '262'\n",
      " '239' '251' '208' '333' '235' '180' '210' '199' '257' '316' '184' '329'\n",
      " '437' '244' '377' '280' '402' '220' '179' '335' '281' '237' '269' '337'\n",
      " '318' '330' '433' '311' '436' '314' '245' '302' '310' '256' '267' '327'\n",
      " '342' '376' '249' '319' '486' '296' '537' '309' '353' '304' '547' '224'\n",
      " '301' '331' '360' '439' '501' '241' '274' '334' '270' '484' '344']\n",
      "24 pop_20 :   ['202' '480' '179' '4' '478' '477' '418' '758' '261' '77' '328' '455' None\n",
      " '186' '242' '299' '338' '669' '151' '420' '394' '320' '221' '457' '139'\n",
      " '342' '289' '439' '245' '719' '265' '411' '395' '623' '78' '150' '357'\n",
      " '384' '583' '383' '158' '267' '268' '314' '96' '539' '120' '429' '174'\n",
      " '62' '263' '345' '235' '685' '164' '37' '227' '450' '237' '108' '410'\n",
      " '115' '483' '551' '645' '488' '109' '415' '185' '55' '101' '91' '87'\n",
      " '264' '17' '313' '200' '25' '508' '149' '106' '549' '335' '233' '48'\n",
      " '166' '363' '973' '274' '1' '397' '128' '861' '390' '828' '518' '291'\n",
      " '302' '176' '262' '3' '358' '92' '132' '52' '890' '466' '585' '387' '379'\n",
      " '306' '296' '375' '59' '183' '333' '368' '377' '224' '407' '14' '425'\n",
      " '571' '428' '258' '50' '511' '673' '212' '404' '447' '344' '413' '398'\n",
      " '232' '39' '239' '276' '334' '7' '95' '1595' '222' '42' '309' '654' '370'\n",
      " '506' '416' '249' '89' '528' '58' '0' '458' '287' '522' '230' '311' '452'\n",
      " '191' '378' '484' '208' '953' '351' '530' '401' '596' '277' '323' '353'\n",
      " '322' '405' '272' '325' '340' '443' '24' '253' '431' '393' '297' '288'\n",
      " '65' '930' '217' '34' '228' '402' '403' '545' '672' '116' '29' '214' '51'\n",
      " '371' '304' '192' '605' '312' '79' '682' '499' '256' '321' '482' '161'\n",
      " '538' '498' '28' '67' '389' '720' '406' '301' '956' '603' '372' '460'\n",
      " '376' '43' '456' '438' '474' '5' '255' '2' '248' '316' '26' '105' '18'\n",
      " '238' '492' '510' '146' '317' '424' '687' '319' '275' '1017' '193' '241'\n",
      " '93' '517' '197' '219' '130' '412' '143' '122' '74' '339' '266' '44'\n",
      " '231' '436' '254' '604' '117' '536' '364' '552' '38' '300' '142' '445'\n",
      " '64' '897' '71' '269' '531' '118' '432' '180' '83' '655' '392' '690'\n",
      " '187' '476' '380' '188' '194' '204' '556' '293' '553' '663' '526' '495'\n",
      " '563' '650' '559' '80' '170' '391' '82' '12' '470' '479' '147' '260'\n",
      " '198' '140' '467' '273' '367' '167' '350' '177' '305' '283' '236' '469'\n",
      " '54' '184' '620' '607' '75' '284' '315' '88' '290' '36' '271' '519' '119'\n",
      " '813' '141' '86' '35' '448' '280' '731' '646' '172' '81' '163' '246'\n",
      " '733' '440' '622' '330' '444' '72' '308' '356' '23' '560' '160' '196'\n",
      " '459' '148' '32' '542' '226' '66' '97' '615' '414' '493' '129' '527'\n",
      " '337' '13' '423' '348' '602' '485' '282' '532' '520' '500' '626' '307'\n",
      " '298' '464' '513' '69' '104' '812' '453' '359' '234' '270' '362' '501'\n",
      " '385' '181' '346' '331' '570' '550' '446' '601' '419' '16' '1384' '286'\n",
      " '332' '240' '30' '463' '582' '515' '171' '285' '68' '84' '329' '138'\n",
      " '133' '343' '60' '318' '178' '57' '251' '303' '668' '213' '173' '451'\n",
      " '409' '8' '21' '159' '887' '195' '417' '127' '218' '426' '454' '201'\n",
      " '489' '229' '1528' '504' '207' '216' '388' '421' '624' '205' '503' '818'\n",
      " '336' '625' '145' '100' '252' '20' '135' '110' '374' '247' '90' '126'\n",
      " '53' '509' '487' '365' '497' '533' '564' '361' '640' '154' '354' '734'\n",
      " '257' '382' '693' '349' '558' '175' '341' '468' '946' '491' '662' '73'\n",
      " '360' '1122' '355' '608' '724' '775' '136' '512' '937' '381' '373' '27'\n",
      " '825' '541' '103' '628' '400' '600' '33' '408' '220' '461' '649' '490'\n",
      " '278' '243' '386' '631' '695' '223' '19' '112' '634' '121' '591' '494'\n",
      " '189' '884' '442' '399' '152' '841' '811' '102' '225' '435' '153' '279'\n",
      " '475' '465' '182' '723' '785' '729' '324' '671' '598' '496' '9' '481'\n",
      " '568' '430' '472' '684' '46' '22' '609' '244' '486' '647' '748' '561'\n",
      " '449' '872' '754' '1068' '168' '63' '683' '630' '41' '652' '580' '61'\n",
      " '752' '157' '576' '664' '292' '427' '281' '618' '113' '56' '210' '49'\n",
      " '696' '543' '665' '40' '250' '616' '790' '369' '697' '738' '681' '1100'\n",
      " '131' '627' '554' '534' '514' '524' '889' '422' '1102' '638' '577' '471'\n",
      " '546' '76' '587' '751' '579' '886' '295' '6' '347' '635' '396' '505'\n",
      " '845' '473' '760' '765' '917' '310' '327' '853' '1026' '593' '782' '1119'\n",
      " '155' '674' '996' '894' '658' '679' '838' '743' '31' '703' '352' '326'\n",
      " '835' '502' '437' '99' '111' '1633' '156' '726' '1317' '206' '688' '98'\n",
      " '839' '215' '581' '639' '114' '699' '619' '433' '259' '750' '203' '144'\n",
      " '597' '441' '169' '10' '1613' '165' '1052' '544' '516' '875' '1061' '588'\n",
      " '717' '294' '366' '47' '125' '816' '869' '45' '523' '629' '651' '434'\n",
      " '162' '209' '123' '753' '1076' '793' '589' '928' '667' '1178' '94' '718'\n",
      " '572' '575' '680' '107' '1364' '562' '642' '659' '15' '199' '211' '847'\n",
      " '691' '792' '709' '948' '1048' '134' '190' '566' '757' '846' '557' '927'\n",
      " '797' '11' '507' '1024' '692' '1131' '910' '547' '700' '864' '721' '815'\n",
      " '641' '1217' '621' '636' '710' '606' '1020' '1449' '716' '565' '722'\n",
      " '569' '1375' '788' '1005' '1468' '899' '820' '807' '462' '525' '770'\n",
      " '599' '633' '821' '1033' '70' '702' '124' '1042' '850' '837' '548' '555'\n",
      " '707' '613' '85' '766' '689' '810' '637' '594' '972' '900' '868' '832'\n",
      " '773' '947' '670' '578' '573' '1029' '1010' '1146' '1083' '814' '764'\n",
      " '521' '777' '735' '1238' '1016' '661' '686' '137' '990' '1066' '984'\n",
      " '949' '840' '590' '1128' '794' '574' '617' '804' '644' '844' '842' '704'\n",
      " '961' '612' '891' '614' '1374' '882' '529' '728' '912' '843' '666' '857'\n",
      " '535' '1787' '784' '567' '694' '978' '1571' '678' '769' '749' '896' '943'\n",
      " '592' '711' '611' '781' '727' '1044' '537' '676' '1625' '942' '2412'\n",
      " '732' '904' '833' '783' '740' '1075' '787' '736' '1433' '540' '1199'\n",
      " '1869' '774' '610' '921' '970']\n",
      "25 hh_size :   [1.40277778 3.1372549  2.88709677 ... 4.0862069  2.55       3.21176471]\n",
      "26 hh_size_est :   ['1' '3' '4' '>4' '2' None '0']\n",
      "27 annual_income_est :   ['C.60K-100K' 'D.30K-60K' 'A.ABOVE200K' 'B.100K-200K' 'E.BELOW30K' None]\n",
      "28 flg_latest_being_lapse :   [0 1]\n",
      "29 flg_latest_being_cancel :   [0 1]\n",
      "30 tot_inforce_pols :   [ 3  1  5  2  9  7  4  6 15 11 10 14  8 12 19 13 16 17 20 25 21 18 22 26\n",
      " 27 31 23 29 54]\n",
      "31 tot_cancel_pols :   [0. 1. 3. 2. 4. 6.]\n",
      "32 f_hold_839f8a :   [0 1]\n",
      "33 f_hold_e22a6a :   [1 0]\n",
      "34 f_hold_d0adeb :   [0]\n",
      "35 f_hold_c4bda5 :   [0 1]\n",
      "36 f_hold_ltc :   [1 0]\n",
      "37 f_hold_507c37 :   [0 1]\n",
      "38 f_hold_gi :   [0]\n",
      "39 f_elx :   [0 1]\n",
      "40 f_mindef_mha :   [0 1]\n",
      "41 f_retail :   [1 0]\n",
      "42 flg_affconnect_show_interest_ever :   [0. 1.]\n",
      "43 flg_affconnect_ready_to_buy_ever :   [0. 1.]\n",
      "44 affcon_visit_days :   [nan  2.  1.  9.  4.  3.  7. 15.  5.  6. 12.  8. 40. 14. 50. 11. 13. 19.\n",
      " 16. 10. 27.]\n",
      "45 n_months_since_visit_affcon :   [nan  5.  0.  2.  3.  1.  4.]\n",
      "46 hlthclaim_amt :   [8.4785000e+02 8.7350000e+02 0.0000000e+00 5.6523000e+02 1.3516500e+04\n",
      " 2.5941970e+04 5.5568900e+03 1.2636480e+04 6.6725900e+03 4.2880820e+04\n",
      " 1.5249000e+02 1.3531000e+02 1.5280910e+04 9.2486700e+03 4.5386000e+03\n",
      " 5.7942880e+04 1.4514510e+04 1.9485200e+03 2.0835300e+03 1.1147800e+03\n",
      " 4.7801000e+02 6.8677000e+02 1.0735400e+03 3.6496500e+04 2.8581820e+04\n",
      " 5.2331202e+05 1.3658000e+03 1.5766720e+04 3.6643800e+03 2.0319700e+04\n",
      " 5.9474300e+03 4.4940000e+02 2.2099240e+04 4.8909000e+02 8.3101700e+03\n",
      " 1.7568548e+05 9.2510900e+03 4.1757000e+02 9.4140500e+03 9.0520000e+02\n",
      " 1.0169660e+04 1.1649030e+04 1.7655000e+03 3.9758220e+04 2.3408900e+04\n",
      " 4.8410500e+03 2.2103900e+03 3.3737810e+04 3.5949000e+02 1.3383400e+03\n",
      " 5.3899000e+02 1.0363100e+03 1.1524880e+04 7.5949600e+03 1.5775100e+03\n",
      " 2.8185500e+03 1.5130000e+03 6.1570760e+04 2.0177000e+03 4.8748200e+03\n",
      " 6.4255800e+03 6.5847100e+03 3.8008800e+03 1.2981320e+04 2.6381840e+04\n",
      " 3.0743100e+03 8.4113700e+03 2.4999210e+04 6.1629000e+02 1.2253500e+04\n",
      " 4.3337800e+03 8.9840000e+01 4.4901000e+02 6.3943000e+02 1.7830150e+04\n",
      " 4.8081300e+03 6.2351600e+03 7.6098000e+02 4.9389000e+02 6.5916000e+02\n",
      " 1.0233000e+02 7.3699000e+03 4.9793000e+02 2.7533000e+02 1.4815000e+02\n",
      " 6.2543820e+04 5.8605000e+02 2.2661240e+04 3.6984000e+02 2.5707900e+04\n",
      " 1.1368243e+05 7.2298900e+03 1.2038000e+02 8.1307000e+02 4.6233100e+03\n",
      " 2.3758800e+03 2.3119390e+04 5.5332400e+03 1.1665800e+03 1.4272000e+02\n",
      " 3.8049300e+03 1.8602780e+04 3.9643000e+02 2.0895700e+04 1.3976400e+03\n",
      " 7.4502700e+03 2.0461700e+03 7.0647300e+04 1.6335500e+03 8.9028000e+02\n",
      " 5.6497000e+02 1.4123470e+04 6.5701300e+03 6.0235000e+02 5.1113200e+03\n",
      " 1.5762710e+04 3.2673000e+03 3.9896660e+04 5.7581400e+03 3.5231000e+02\n",
      " 1.7302965e+05 7.7224000e+02 1.0514100e+03 5.2454500e+03 2.2030000e+02\n",
      " 1.0614670e+04 9.1596300e+03 8.6299000e+02 9.1457000e+02 9.2162360e+04\n",
      " 1.7160250e+04 1.2631730e+04 1.2545000e+02 3.5423300e+03 1.4354400e+03\n",
      " 1.1299600e+03 1.6327500e+03 2.2526100e+03 6.9595000e+02 1.7511240e+04\n",
      " 7.0201700e+03 4.5902620e+04 6.1456400e+04 2.4797000e+02 8.2818000e+02\n",
      " 1.0060900e+03 3.9558000e+02 1.6596760e+04 2.1633680e+04 1.4672926e+05\n",
      " 1.2785900e+03 1.5317890e+04 8.0829000e+03 8.7279000e+02 2.4963000e+03\n",
      " 1.6690620e+05 8.1395650e+04 6.2485700e+03 1.8665500e+03 1.1235000e+03\n",
      " 1.5448830e+04 1.4115000e+02 4.7704100e+03 2.5570900e+03 4.3965000e+02\n",
      " 7.2060000e+03 6.2100000e+01 8.4308400e+03 6.8346200e+03 9.6499800e+03\n",
      " 3.9290700e+03 2.2995400e+03 2.3179000e+03 8.9345000e+02 3.0918850e+04\n",
      " 5.5770000e+02 2.4022430e+04 3.2040530e+04 4.8333800e+03 1.1809700e+03\n",
      " 7.6254300e+03 1.1453530e+04 2.1363500e+03 9.8500200e+03 4.0050000e+01\n",
      " 4.7128230e+04 4.9283000e+02 6.1816800e+03 4.2498800e+03 1.8343600e+03\n",
      " 2.7985150e+04 2.9988400e+03 2.3377300e+03 2.7022000e+02 6.8776900e+03\n",
      " 8.0069000e+02 3.8536000e+02 2.2680930e+04 9.7600430e+04 6.9811140e+04\n",
      " 3.1399160e+04 8.9612000e+02 2.5085500e+03 1.1571090e+04 7.1562800e+03\n",
      " 7.7767000e+02 4.4296900e+03 8.8519000e+02 1.3590000e+01 1.5086800e+03\n",
      " 1.5244410e+04 1.2142800e+03 7.2652800e+03 1.2093450e+04 9.0757210e+04\n",
      " 2.2069300e+03 2.1489060e+04 3.8774060e+04 2.0145650e+04 4.0240000e+03\n",
      " 1.1340000e+03 1.2667340e+04 1.1203900e+03 3.5251900e+03 3.1961800e+03\n",
      " 1.2174900e+03 1.4000000e+03 7.9319000e+02 6.1909100e+03 1.1827630e+04\n",
      " 3.9952200e+03 4.6968700e+03 7.4549400e+03 9.5014600e+03 9.8000000e+01\n",
      " 1.4264590e+04 1.5410000e+01 1.9103000e+02 1.5642530e+04 2.3016250e+04\n",
      " 9.2973400e+03 2.0366700e+03 1.2133000e+02 1.0495900e+03 7.4849400e+03\n",
      " 3.4675900e+03 6.5187200e+04 3.2340500e+03 1.2766700e+04 1.6650240e+04\n",
      " 3.1647540e+04 1.4343880e+04 4.4493120e+04 4.0055500e+03 2.9692140e+04\n",
      " 7.6600000e+02 1.0979440e+04 5.2014960e+04 2.5415900e+03 3.9540900e+03\n",
      " 5.9143300e+03 1.5093000e+02 4.0271400e+03 3.8927640e+04 8.8769200e+03\n",
      " 2.4932300e+03 2.6720200e+03 2.9919000e+02 2.7059200e+03 4.6739000e+02\n",
      " 3.4787200e+03 3.2098360e+04 1.1079200e+03 3.8126000e+02 1.1377880e+04\n",
      " 1.6286660e+04 2.5166100e+03 1.4554100e+03 5.7320700e+03 8.8525400e+03\n",
      " 1.5000000e+02 1.8435400e+03 1.8764760e+04 8.4483000e+02 1.6437100e+03\n",
      " 1.7767390e+04 4.1709100e+03 5.1840910e+04 2.5049700e+03 5.9867000e+02\n",
      " 2.0047000e+02 7.0070800e+03 3.5426050e+04 2.4291270e+04 3.8193000e+03\n",
      " 3.5120200e+03 4.1625520e+04 1.9479500e+03 7.2830000e+01 2.8142650e+04\n",
      " 2.2920500e+03 2.6981300e+03 1.1727480e+04 3.2185400e+04 7.9390000e+02\n",
      " 7.7133000e+02 1.0940000e+02 2.1917938e+05 1.2680400e+03 2.0384470e+04\n",
      " 5.9103200e+03 4.4359100e+03 1.3817120e+04 2.8636000e+02 1.1296780e+04\n",
      " 4.2583000e+04 5.1630000e+02 2.0686560e+04 5.7491000e+02 4.0964700e+03\n",
      " 1.5634130e+04 1.6211000e+02 1.2258330e+04 1.6324500e+03 1.0808490e+04\n",
      " 9.6954080e+04 4.1885700e+03 2.6702300e+03 1.3575385e+05 1.6410700e+03\n",
      " 1.2934340e+04 1.2565110e+04 1.8786470e+04 5.1904840e+04 5.3572800e+03\n",
      " 7.7364000e+02 2.4500000e+02 2.7935960e+04 9.5959400e+03 2.3100100e+03\n",
      " 4.4332000e+02 1.8887100e+04 8.9176000e+02 8.7226140e+04 1.7199260e+04\n",
      " 8.5116000e+02 1.4277740e+04 1.6927000e+03 2.4017980e+04 3.9333600e+03\n",
      " 1.0353600e+03 1.0637427e+05 1.6218007e+05 1.5607020e+04 1.7513520e+04\n",
      " 3.3642000e+02 1.5382860e+04 6.0562000e+02 1.1652100e+04 3.9689000e+02\n",
      " 4.1202000e+02 4.6996270e+04 1.8777000e+02 4.5702210e+04 1.1531410e+04\n",
      " 3.8837200e+04 6.1005000e+02 1.4200440e+04 9.7263000e+02 9.4788600e+03\n",
      " 2.1506380e+04 7.3553000e+02 5.6418510e+04 6.4128060e+04 4.2544000e+02\n",
      " 2.6934590e+04 4.1790100e+03 5.2617000e+02 2.9060500e+03 3.8303800e+03\n",
      " 6.4212300e+03 9.2504100e+03 1.8632100e+03 9.8850100e+03 5.8669000e+02\n",
      " 8.5590000e+02 1.7860600e+03 1.6417060e+04 1.9617250e+04 9.4884800e+03\n",
      " 5.2214200e+03 1.4205540e+04 6.5530500e+03 3.3458900e+03 2.1406420e+04\n",
      " 2.4697410e+04 1.1450000e+03 7.6044000e+02 1.4742000e+04 2.2298610e+04\n",
      " 1.2414600e+03 2.9395100e+03 6.2919500e+03 1.1207410e+04 2.9980900e+04\n",
      " 5.1090900e+03 3.2654100e+03 3.3088700e+03 2.7451380e+04 7.0537000e+03\n",
      " 1.2194258e+05 9.2682290e+04 4.6205000e+02 3.9003000e+03 5.9971300e+03\n",
      " 2.6138900e+03 7.5201000e+02 2.1816760e+04 1.8567000e+02 1.2904120e+04\n",
      " 1.8841300e+03 7.1329300e+03 6.9020300e+03 9.6200500e+03 3.5785500e+03\n",
      " 4.9072480e+04 3.9760800e+03 3.6494400e+03 2.8970000e+02 4.9088130e+04\n",
      " 1.2608700e+03 2.6497840e+04 3.0443000e+02 4.3335000e+02 1.7100000e+01\n",
      " 1.1913510e+04 8.8783000e+02 2.0552900e+03 3.4020600e+03 3.5927200e+03\n",
      " 5.7744000e+03 3.8128000e+03 5.5796600e+03 3.4215000e+02 9.9270300e+03\n",
      " 7.1575700e+03 2.3893300e+03 2.3363000e+03 1.2051750e+04 7.9715000e+02\n",
      " 1.8769850e+04 1.5051900e+03 6.8964400e+03 1.1123910e+04 2.2951500e+03\n",
      " 7.4850210e+04 1.6100540e+04 1.0000000e-02 5.1774850e+04 1.2375000e+02\n",
      " 6.6568000e+02 1.1143091e+05 2.4722840e+04 4.1845700e+03 5.1701000e+02\n",
      " 1.5372000e+03 3.5387900e+03 3.6024680e+04 1.9915400e+03 7.1394100e+03\n",
      " 1.7371000e+02 4.6751800e+03 3.8215000e+02 5.6737730e+04 3.0264400e+03\n",
      " 2.9560100e+03 5.1870000e+01 2.9055400e+03 2.4818580e+04 1.9750000e+01\n",
      " 8.4040400e+03 5.5079000e+02 2.3795470e+04 7.1105000e+02 3.3204400e+03\n",
      " 2.4508940e+04 2.6986000e+02 3.2459400e+03 1.0166820e+04 6.9645900e+03\n",
      " 2.5769800e+03 4.1919900e+03 3.1940200e+03 1.4258100e+03 4.2493000e+02\n",
      " 1.4481400e+04 2.3512100e+03 1.7049740e+04 2.5951000e+02 6.4489020e+04\n",
      " 6.5051000e+03 1.0356340e+04 5.8472000e+02 6.3386240e+04 2.6994200e+03\n",
      " 2.1428510e+04 3.0010100e+03 6.1315750e+04 4.8730700e+03 2.9000000e+03\n",
      " 3.7798200e+03 6.3890000e+03 9.0801500e+03 7.2297660e+04 2.6140000e+02\n",
      " 2.0987160e+04 7.1280100e+03 1.0192490e+04 1.0557230e+04 2.5227200e+03\n",
      " 3.5109000e+03 1.6799010e+04 6.7287000e+02 8.1361400e+03 4.0697700e+03\n",
      " 2.2368080e+04 1.0090000e+03 1.5875500e+03 4.4928600e+03 1.0413080e+04\n",
      " 6.7415000e+02 1.4410400e+03 4.9651900e+03 3.9208600e+04 1.3775000e+03\n",
      " 1.5699300e+03 9.5059500e+03 4.8739900e+03 1.2226030e+04 2.5204600e+04\n",
      " 1.5169300e+03 2.1923770e+04 4.2541920e+04 2.7232730e+04 5.0730780e+04\n",
      " 1.0022950e+04 8.0781650e+04 8.2000000e+02 5.1519000e+02 2.4949200e+03\n",
      " 2.2138740e+04 1.2503090e+04 4.8125000e+03 4.3225030e+04 5.1366300e+03\n",
      " 1.8192800e+03 1.5200000e+02 8.3604500e+03 7.4550000e+03 1.9020600e+03\n",
      " 2.2064640e+04 2.0596400e+03 2.2580000e+02 1.3249650e+04 4.3995000e+02\n",
      " 4.9576000e+02 1.6338040e+04 1.2199200e+04 1.8497380e+04 9.2144400e+03\n",
      " 2.5389000e+03 8.7426960e+04 2.7214030e+04 9.8414700e+03 1.0619610e+04\n",
      " 4.2236972e+05 1.7079420e+04 8.4248000e+02 2.6874420e+04 4.1690900e+03\n",
      " 1.0639760e+04 4.9135690e+04 2.6305000e+03 8.1200000e+01 2.6347000e+02\n",
      " 1.2394500e+03 2.3809000e+02 5.8349550e+04 2.8785000e+03 1.3271920e+04\n",
      " 3.0090010e+04 2.4840600e+04 1.1008700e+03 2.8113300e+03 4.2719400e+03\n",
      " 2.7464400e+03 1.5090630e+04 3.0851480e+04 2.7511530e+04 2.4920000e+02\n",
      " 2.0500000e+03 1.1938370e+04 1.2945690e+04 4.0289000e+02 1.4761870e+04\n",
      " 6.1372000e+03 7.2549670e+04 6.7377900e+03 3.0045900e+03 2.3486620e+04\n",
      " 6.9610000e+02 5.6905790e+04 3.5922992e+05 1.6468610e+04 2.4392600e+04\n",
      " 2.7753600e+03 7.7593400e+03 6.9678800e+03 6.1526000e+02 7.4986000e+02\n",
      " 1.1233010e+04 3.6111500e+03 7.9384000e+02 2.4626000e+02 6.6544500e+03\n",
      " 1.5121110e+04 1.1997700e+03 5.7976000e+02 7.7287900e+03 9.0513400e+03\n",
      " 9.0935400e+03 5.3374900e+03 6.3679000e+02 6.5180000e+02 1.7492770e+04\n",
      " 5.7811000e+02 2.8330700e+03 1.1108040e+04 4.5100000e+02 3.0749200e+03\n",
      " 2.9315300e+03 3.6272000e+02 1.9341480e+04 7.5754800e+03 1.6743000e+02\n",
      " 4.1766000e+03 1.5568200e+03 1.7674100e+03 3.8626300e+03 6.0735000e+02\n",
      " 9.0500000e+02 3.0278000e+02 3.9445000e+02 5.7147000e+03 1.7450000e+02\n",
      " 4.4240960e+04 5.4215600e+03 4.9157850e+04 4.8819080e+04 3.7264500e+03\n",
      " 2.0593400e+03 3.1425000e+02 6.1843000e+02 6.1456900e+03 2.4656340e+04\n",
      " 4.5000000e+01 2.0275000e+03 1.3489950e+04 4.6924000e+02 2.5671000e+02\n",
      " 4.7596000e+02 5.3689500e+03 1.2096010e+04 1.5542360e+04 2.1107692e+05\n",
      " 6.3524700e+03 1.7275000e+03 5.7793900e+03 1.0229200e+03 3.2797000e+02\n",
      " 5.2744500e+03 9.7511000e+02 3.6024980e+04 1.8885400e+04 2.0815300e+03\n",
      " 1.9996810e+04 1.3368489e+05 2.2867090e+04 6.8114500e+03 4.8289100e+03\n",
      " 1.2278220e+04 8.2262000e+02 1.6443090e+04 2.0133300e+03 3.4010000e+02\n",
      " 1.5976300e+04 1.4599300e+03 3.8260600e+03 3.0000000e+02 1.5599500e+03\n",
      " 2.4214746e+05 1.6028100e+03 4.5622900e+03 2.3689300e+03 7.8638700e+03\n",
      " 1.6414250e+04 1.0358520e+04 1.4006800e+03 1.5120000e+02 1.8384670e+04\n",
      " 9.3931600e+04 3.7992200e+03 7.3916000e+02 2.3436600e+03 1.0738220e+04\n",
      " 1.4894600e+03 1.3482100e+04 2.3573000e+03 7.9219000e+03 3.6326470e+04\n",
      " 3.4222300e+03 1.0644040e+04 1.5355900e+03 4.8357390e+04 4.5168600e+03\n",
      " 3.5559120e+04 1.3379200e+03 3.2189800e+03 7.3081000e+03 2.3824000e+03\n",
      " 3.8699200e+03 2.5260660e+04 4.7796570e+04 1.4142800e+03 2.6794200e+03\n",
      " 3.7604000e+03 3.4100000e+03 4.9281100e+04 2.8346500e+03 4.2253610e+04\n",
      " 3.4847400e+03 2.5781250e+04 5.6981000e+02 9.1317800e+03 4.6560000e+02\n",
      " 9.5188670e+04 7.6769000e+02 7.1039000e+02 1.1158200e+03 8.5344900e+03\n",
      " 4.5238910e+04 2.5653000e+02 6.2732300e+03 2.9603830e+04 7.9872600e+03\n",
      " 1.8197390e+04 1.7860200e+03 1.5060726e+05 2.3534420e+04 7.5874000e+02\n",
      " 1.5024340e+04 8.2328000e+02 2.2807300e+03 6.7371700e+03 1.5683400e+03\n",
      " 4.8527800e+03 2.0693800e+03 1.5690100e+03 9.6427100e+03 1.1578470e+04\n",
      " 2.5921000e+02 2.4192900e+03 8.7118000e+02 5.4761830e+04 2.4310590e+04\n",
      " 4.2939400e+03 5.3933600e+03 1.7047500e+03 1.2102440e+04 3.5944400e+03\n",
      " 8.8500000e+01 3.3383000e+02 7.0529800e+03 1.3990170e+04 5.5050000e+02\n",
      " 3.2235000e+02 5.4071600e+03 1.2298000e+03 7.5849800e+03 1.3840600e+03\n",
      " 3.6366400e+03 1.4707520e+04 1.0324670e+04 2.1453000e+02 2.6274750e+04\n",
      " 3.1305260e+04 1.1574000e+03 1.5316683e+05 1.0188100e+03 4.8437000e+02\n",
      " 1.3589000e+03 1.7299100e+03 1.0256690e+04 2.7297000e+04 1.9786210e+04\n",
      " 2.3257000e+03 4.0604000e+03 7.8315200e+03 8.2333000e+02 9.3926200e+03\n",
      " 7.3661700e+03 2.4842770e+04 3.4090800e+03 9.1997900e+03 2.1720000e+01\n",
      " 1.7240400e+03 8.3430200e+03 4.1982400e+03 1.8441300e+03 7.9973700e+03\n",
      " 1.4936170e+04 6.1220220e+04 1.3340000e+02 3.7720000e+03 2.2500000e+01\n",
      " 2.9459500e+03 5.0932000e+02 2.0150800e+03 3.2855570e+04 3.5203000e+03\n",
      " 1.4401620e+04 4.0587740e+04 1.6260700e+03 8.1598000e+02 4.3318000e+03\n",
      " 3.5055400e+03 8.5742900e+03 4.4493500e+03 7.0804200e+03 1.0214500e+03\n",
      " 5.7070000e+02 3.4757300e+03 5.4103600e+03 1.1247990e+04 7.8000000e+02\n",
      " 1.9779167e+05 2.0447300e+03 8.2782000e+03 8.3474000e+02 2.6126760e+04\n",
      " 6.1020000e+02 8.5453730e+04 1.3294000e+04 3.5823900e+03 2.5498000e+02\n",
      " 2.2353670e+04 4.5327190e+04 1.2904940e+04 7.7349000e+03 1.5195190e+04\n",
      " 9.8664000e+02 1.1374690e+04 2.7240000e+02 2.4900000e+03 6.9420050e+04\n",
      " 1.5011000e+02 3.1858700e+03 6.1454900e+03 1.2453700e+04 2.4177800e+04\n",
      " 5.5415800e+03 3.0200300e+03 5.2149800e+03 7.5170700e+03 8.2529000e+02\n",
      " 2.1502650e+04 1.1699400e+03 2.6269800e+03 6.2688690e+04 8.6577400e+03\n",
      " 1.0382520e+04 1.3858870e+04 4.5772300e+03 2.8993300e+03 1.9211990e+04\n",
      " 8.3975000e+02 3.0262800e+03 1.6042300e+03 1.5219540e+04 4.5566900e+03\n",
      " 1.1447940e+04 6.5404000e+02 3.5920000e+02 3.5505400e+03 1.2554200e+03\n",
      " 1.5330059e+05 9.9300000e+01 1.9449780e+04 5.5106000e+02 2.7289630e+04\n",
      " 1.2335394e+05 1.0798220e+04 3.1018100e+03 8.3222700e+03 5.8072420e+04\n",
      " 5.2751000e+02 3.7438800e+03 2.8086700e+03 1.1325690e+04]\n",
      "47 recency_hlthclaim :   [0 1]\n",
      "48 recency_hlthclaim_success :   [ nan  11.  18.  71.  10.  88.  80.  83.  44.  15.   6.  66.  23.  32.\n",
      "  87.  12.  53.  50. 120.  67.   2.  48.  33.  17.   7.   9.  13.  21.\n",
      "   0.  29. 105.   1.  58.  41.  22.  64.  45.  62.  20.  75.  77.  89.\n",
      "   8.  99.  69.  42.   5.  34.  31.  85. 122. 121.  55.  52. 108.  74.\n",
      " 101.   3. 118.  60. 123.  26.  35.  36. 126.  56.  16.   4.  28.  93.\n",
      " 109.  90.  37.  84. 102.  24.  91.  86.  14.  92. 119.  47.  27. 111.\n",
      "  78.  49. 103.  46.  76.  79.  38.  51.  30.  65.  61.  57. 112.  68.\n",
      "  98.  25. 110.  73. 114.  59.  70.  40.  94.  63. 107.  19.  43.  39.\n",
      " 100.  81.  72.  82. 113. 106.  97. 124.  54. 104. 115. 125. 116. 117.]\n",
      "49 hlthclaim_cnt_unsuccess :   [nan  1.  2.  3.  4.  5.  6.  7.  8. 10. 12.  9. 19. 17. 23. 68. 13. 33.]\n",
      "50 recency_hlthclaim_unsuccess :   [ nan 111.  86. 112.  23.  88.  45.  80.  17.  53.  66. 120.  40.  11.\n",
      "  37.  79.  56.   1. 118.  13.  99. 126.  47.   8.   5.  31.  48.  43.\n",
      "  34.  12.  30.  35.  59.  24.  72. 123.   7.  28. 119.  33.  46.  91.\n",
      "  16.  93.   0.  90.  65.   4.  14.  52.  18.   3.  51. 104.  19.  71.\n",
      "  10. 102.  41.  49.  89.  32.  58.  22.  78. 107.  76. 101.  36.  38.\n",
      " 106.  68.   9.  61.  62.  57.  21.   2.   6.  25.  74.  55.  50.  85.\n",
      "  94.  64.  77.  26.  15.  83.  39.  63.  54.  42.  73. 117.  82. 103.\n",
      "  81.  69.  67.  44. 115. 108. 100. 110.  95.  60. 109.  27.  84.  98.\n",
      "  20.  92. 114.  29. 116.  87. 121. 105. 122. 125. 113.  75.]\n",
      "51 recency_hlthclaim_839f8a :   [ nan  11.  80.  22.  23.  44.  53.  25.  50.   2.  17.  42. 105.  75.\n",
      "  19.   1.  64.  62.  99.  14.   9.   5.  31. 101.  35. 108.  77.  18.\n",
      " 123.  20.  54.  63. 122.  46.   7.  34.  57.  89.  47. 124.   6.  32.\n",
      "  13.  24. 109.  59.  85.  74.  40.  60.  49.  93.  76.  26.   0.  68.\n",
      "  51.  90.  81.   3.  43.   4.  37.  12. 100.  79.  27.  41.  92.  29.\n",
      "  48.  83.  82.  91.  73.  84.  86.  16.  67.   8. 120.  95.  10.  36.\n",
      "  28.  15.  72. 110.  21. 113.  66. 112.  87.  55.  33. 118.  94.  45.\n",
      "  98. 115. 111. 114.  70.  69.  39.]\n",
      "52 recency_hlthclaim_14cb37 :   [ nan  11.  71. 111.  86. 112.  88.  80.  83.  59.  15.   6.  23.  17.\n",
      "  12.  53.  66.  50. 120.  67.   2.  48.  33.   7.   9.  13.  18.   0.\n",
      " 105.  37.   1.  58. 118.  22.  64.  45.  62.  20.  75.  77.  89.   8.\n",
      "  99.  69. 126.  47.   5.  34.  31.  85.  44. 122. 121.  55.  30. 108.\n",
      "  78.  24. 123. 101.   3.  60.  74.  26.  35.  70.  56.   4.  46.  91.\n",
      "  16.  42.  28.  72.  93. 109.  90.  65.  52.  36.  84.  51. 102.  40.\n",
      "  10.  14.  92.  41.  49.  19. 119.  32.  27. 103. 107.  21.  76.  79.\n",
      "  38. 106.  68.  57.  98.  25. 114.  29.  94.  63. 104.  39.  43.  54.\n",
      "  73.  87. 117. 100.  81. 110.  61. 113.  82. 124. 115.  97. 116. 125.]\n",
      "53 giclaim_amt :   [4.000000e+02 3.898840e+03 1.472500e+03 8.147800e+02 2.821000e+02\n",
      " 1.090936e+04 1.735200e+02 4.320600e+02 3.506897e+04 0.000000e+00\n",
      " 3.824600e+02 1.333278e+04 3.784300e+02 7.282200e+02 1.225940e+03\n",
      " 4.863290e+03 3.681000e+01 4.909100e+02 4.731000e+02 7.303740e+03\n",
      " 5.602000e+01 9.227000e+01 1.000000e+02 8.157300e+02 4.612170e+03\n",
      " 1.670430e+03 2.500000e+02 7.500000e+02 6.954300e+02 8.425000e+02\n",
      " 2.375000e+02 2.420000e+02 8.509303e+04 2.200000e+03 3.500000e+02\n",
      " 2.000000e+01 3.296700e+02 4.329100e+02 1.000000e+03 7.921900e+02\n",
      " 3.822490e+03 1.241200e+02 9.735000e+01 1.200000e+02 4.070000e+02\n",
      " 6.000000e+02 3.516900e+02 7.014510e+03 2.000000e+02 1.210000e+02\n",
      " 1.600000e+03 1.170000e+02 3.780272e+04 1.357100e+03 3.146928e+04\n",
      " 2.878500e+02 9.480450e+03 3.000000e+02 3.825760e+03 2.402190e+03\n",
      " 6.198980e+03 5.410800e+02 5.575000e+02 4.778000e+02 2.574250e+03\n",
      " 9.027500e+02 9.077500e+02 2.100360e+04 4.110900e+02 5.000000e+01\n",
      " 7.000000e+02 1.509980e+03 1.068650e+03 2.698000e+02 3.588870e+03\n",
      " 9.450000e+01 1.420200e+02 1.808500e+03 2.105000e+02 1.633481e+04\n",
      " 1.062000e+05 1.040950e+03 1.500000e+02 1.494696e+04 7.880000e+02\n",
      " 7.777100e+02 8.436800e+02 1.290000e+02 2.453500e+02 1.185700e+02\n",
      " 1.542950e+03 2.520000e+02 1.002750e+03 4.660000e+02 3.882800e+02\n",
      " 5.250000e+01 7.940900e+02 1.744990e+03 4.800000e+02 1.974500e+02\n",
      " 5.500000e+01 1.584535e+04 5.810000e+02 9.500000e+01 1.615200e+02\n",
      " 2.461000e+02 6.105000e+02 5.000000e+02 5.500000e+02 1.787715e+04\n",
      " 2.920000e+02 7.497000e+03 9.219500e+02 9.876000e+01 2.206480e+03\n",
      " 2.455700e+02 9.000000e+01 6.381000e+02 1.193000e+03 4.100000e+01\n",
      " 1.133750e+03 6.612730e+03 1.805000e+02 7.830000e+03 4.646490e+03\n",
      " 4.544150e+03 1.440900e+02 6.723000e+02 5.330200e+02 4.225000e+02\n",
      " 1.263500e+02 1.071012e+04 3.202600e+02 2.924290e+03 5.075000e+02\n",
      " 7.306700e+02 4.400701e+04 2.400000e+03 1.393050e+03 7.711000e+02\n",
      " 3.643200e+02 4.725804e+04 8.920000e+01 7.400000e+01 5.818090e+03\n",
      " 1.759940e+03 2.033300e+02 7.475000e+02 1.285500e+02 4.400000e+02\n",
      " 1.001900e+03 2.646930e+03 6.703587e+04 2.419990e+03 1.044720e+03\n",
      " 7.470000e+02 9.698000e+01 4.627500e+02 8.325000e+02 6.010300e+02\n",
      " 2.195000e+03 2.052600e+04 8.000020e+03 1.629363e+04 1.609300e+02\n",
      " 5.620000e+01 4.863400e+03 3.109440e+03 5.300000e+03 3.605300e+02\n",
      " 9.501260e+03 1.223200e+02 1.478500e+02 4.906500e+02 2.530000e+03\n",
      " 3.725000e+02 3.213260e+04 4.349320e+03 4.158020e+03 1.062750e+03\n",
      " 1.789250e+03 5.550000e+02 5.827500e+02 8.499200e+02 1.605000e+02\n",
      " 1.698190e+03 1.030000e+02 1.056792e+04 6.997500e+02 5.111700e+02\n",
      " 7.980000e+02 2.524768e+04 1.932162e+04 4.806900e+02 1.396000e+02\n",
      " 4.993500e+02 1.585720e+03 7.955400e+02 1.323367e+04 4.665530e+03\n",
      " 1.242100e+02 1.316200e+02 1.240130e+03 8.471000e+01 4.528700e+02\n",
      " 6.035800e+02 2.048700e+03 1.115630e+03 4.935700e+02 1.367884e+04\n",
      " 6.607000e+01 8.200000e+01 1.945900e+02 1.100000e+02 1.126000e+03\n",
      " 5.570000e+01 2.162850e+04 5.736840e+03 4.049600e+02 3.300000e+01\n",
      " 3.376000e+02 3.730950e+03 1.620030e+03 1.892500e+03 4.815000e+01\n",
      " 2.370000e+02 2.120000e+02 5.402000e+02 2.778159e+04 1.274340e+03\n",
      " 1.030520e+03 8.327500e+02 7.227500e+02 9.827000e+01 5.527500e+02\n",
      " 4.994640e+03 3.813500e+02 5.509220e+03 8.000000e+02 5.305050e+03\n",
      " 2.080550e+03 5.816500e+02 4.927500e+02 3.767600e+03 7.827400e+03\n",
      " 1.016117e+04 6.320000e+02 8.182950e+03 4.907294e+04 1.207500e+02\n",
      " 6.400000e+01 1.662500e+03 7.775000e+01 3.150000e+02 1.790000e+02\n",
      " 7.900000e+01 3.926540e+03 3.683000e+02 6.790000e+01 3.774200e+02\n",
      " 2.532000e+02 8.000000e+01 1.050000e+03 1.458000e+02 8.827500e+02\n",
      " 1.938530e+03 3.604900e+02 7.877500e+02 4.510000e+01 3.468110e+03\n",
      " 9.500000e+02 2.925000e+02 1.663500e+02 4.121550e+03 7.879000e+01\n",
      " 1.904890e+03 2.922730e+03 1.920000e+02 1.362750e+03 5.332500e+02\n",
      " 6.100000e+01 2.617900e+03 2.064120e+04 1.234486e+04 8.493750e+03\n",
      " 4.431332e+04 1.402400e+03 2.452260e+03 8.240000e+02 2.271900e+02\n",
      " 2.438210e+03 4.539740e+03 1.283300e+02 1.800000e+03 1.503400e+03\n",
      " 2.125970e+03 5.786000e+02 5.152000e+02 1.507500e+02 3.673630e+03\n",
      " 5.732750e+03 1.215407e+04 2.501795e+04 1.720947e+04 5.340000e+02\n",
      " 2.946100e+02 7.845300e+03 2.401200e+02 2.979000e+02 3.813850e+03\n",
      " 6.278480e+03 7.162900e+03 6.285400e+02 2.573200e+02 1.713000e+01\n",
      " 9.527500e+02 3.220952e+04 1.661690e+03 9.327500e+02 6.275000e+02\n",
      " 2.480000e+03 1.017750e+03 3.942380e+04]\n",
      "54 recency_giclaim :   [0 1]\n",
      "55 f_purchase_lh :   [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "id = 1\n",
    "for col in df.columns:\n",
    "    print(id, col, \":  \", df[col].unique())\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing column feature: DOB -> Age\n",
    "Filling missing values with the median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting DOB to age\n",
    "age_list = list()\n",
    "for x in df[\"cltdob_fix\"]:\n",
    "    if x.lower() != \"none\":\n",
    "        year = int(x[:4])\n",
    "        age = 2024 - year\n",
    "        age_list.append(age)\n",
    "\n",
    "    \n",
    "df[\"cltdob_fix\"] = pd.Series(age_list)\n",
    "median_value = df[\"cltdob_fix\"].median()\n",
    "df[\"cltdob_fix\"] = df[\"cltdob_fix\"].replace({None: np.nan})\n",
    "df[\"cltdob_fix\"] = df[\"cltdob_fix\"].fillna(median_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using hh_20 column and pop_20 columns as the X_train data for KNN imputation of hh_size_est since there are links between these 3 columns. This is because we checked that there are the same number of missing values for the three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n"
     ]
    }
   ],
   "source": [
    "print(sum(df[\"hh_20\"].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n"
     ]
    }
   ],
   "source": [
    "print(sum(df[\"pop_20\"].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15179\n",
      "2610\n"
     ]
    }
   ],
   "source": [
    "print(df[\"hh_20\"].value_counts().sum())\n",
    "print(df.shape[0] - df[\"hh_20\"].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_20\"] = df[\"hh_20\"].fillna(-1)\n",
    "df[\"pop_20\"] = df[\"pop_20\"].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_20\"] = df[\"hh_20\"].astype(int)\n",
    "df[\"pop_20\"] = df[\"pop_20\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "#calculating median value for hh_20 column\n",
    "hh_20_lst = list()\n",
    "for i in df[\"hh_20\"]:\n",
    "    if i != -1:\n",
    "        hh_20_lst.append(i)\n",
    "hh_20_median = statistics.median(hh_20_lst)\n",
    "print(hh_20_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n"
     ]
    }
   ],
   "source": [
    "#calculating median value for pop_20 column\n",
    "pop_20_lst = list()\n",
    "for i in df[\"pop_20\"]:\n",
    "    if i != -1:\n",
    "        pop_20_lst.append(i)\n",
    "pop_20_median = statistics.median(pop_20_lst)\n",
    "print(pop_20_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_20\"] = df[\"hh_20\"].replace(-1, hh_20_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[\"hh_20\"] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pop_20\"] = df[\"pop_20\"].replace(-1, hh_20_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[\"pop_20\"] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n"
     ]
    }
   ],
   "source": [
    "print(sum(df[\"hh_size_est\"].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_size_est\"] = df[\"hh_size_est\"].replace(\">4\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    6095\n",
       "4    3221\n",
       "5    2674\n",
       "2    2276\n",
       "1     837\n",
       "0      76\n",
       "Name: hh_size_est, dtype: int64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"hh_size_est\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[202., 144.,   1.],\n",
       "       [480., 153.,   3.],\n",
       "       [179.,  62.,   3.],\n",
       "       ...,\n",
       "       [145.,  43.,   3.],\n",
       "       [323.,  72.,   5.],\n",
       "       [ 88.,  88.,   1.]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=15)\n",
    "imputed_data = imputer.fit_transform(df[[\"pop_20\",\"hh_20\",\"hh_size_est\"]]).round()\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hh_size_est\"] = pd.DataFrame(imputed_data[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since there are still NA values after performing imputation using KNN, we decided to use DecisionTreeClassifier to impute the remaining missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5. 1. ... 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc_x_train = df.dropna(subset = [\"hh_size_est\"])[[\"hh_20\", \"pop_20\"]]\n",
    "dtc_y_train = df.dropna(subset=[\"hh_size_est\"])[\"hh_size_est\"]\n",
    "\n",
    "dtc_x_test = df[df[\"hh_size_est\"].isna()][[\"hh_20\", \"pop_20\"]]\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(dtc_x_train, dtc_y_train)\n",
    "\n",
    "dtc_y_predicted = clf.predict(dtc_x_test)\n",
    "print(dtc_y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0, 2.0, 3.0, 4.0, 5.0}\n"
     ]
    }
   ],
   "source": [
    "print(set(dtc_y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"hh_size_est\"].isna(), \"hh_size_est\"] = dtc_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(df[\"hh_size_est\"].isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, all the missing values of the hh_size_est column are fully imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "hh_size_est_lst_numpy_array = np.array(df[\"hh_size_est\"]).reshape(-1,1)\n",
    "label_encoding = OneHotEncoder()\n",
    "encoded = label_encoding.fit(hh_size_est_lst_numpy_array)\n",
    "print(encoded.transform(hh_size_est_lst_numpy_array).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding_hh_size_est = encoded.transform(hh_size_est_lst_numpy_array).toarray()\n",
    "type(one_hot_encoding_hh_size_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoding_hh_size_est_T = one_hot_encoding_hh_size_est.T\n",
    "print(one_hot_encoding_hh_size_est_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_id = 0\n",
    "for i in [\"0\", \"1\", \"2\", \"3\", \"4\", \">4\"]:\n",
    "    name = \"hh_size_est_\" + i\n",
    "    df[name] = one_hot_encoding_hh_size_est_T[some_id]\n",
    "    some_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.transform(np.array([0,1,2,3,4,5]).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 17789)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding_hh_size_est_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17789, 61)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cltdob_fix</th>\n",
       "      <th>flg_substandard</th>\n",
       "      <th>flg_is_borderline_standard</th>\n",
       "      <th>flg_is_revised_term</th>\n",
       "      <th>flg_is_rental_flat</th>\n",
       "      <th>flg_has_health_claim</th>\n",
       "      <th>flg_has_life_claim</th>\n",
       "      <th>flg_gi_claim</th>\n",
       "      <th>flg_is_proposal</th>\n",
       "      <th>flg_with_preauthorisation</th>\n",
       "      <th>...</th>\n",
       "      <th>recency_hlthclaim_14cb37</th>\n",
       "      <th>giclaim_amt</th>\n",
       "      <th>recency_giclaim</th>\n",
       "      <th>f_purchase_lh</th>\n",
       "      <th>hh_size_est_0</th>\n",
       "      <th>hh_size_est_1</th>\n",
       "      <th>hh_size_est_2</th>\n",
       "      <th>hh_size_est_3</th>\n",
       "      <th>hh_size_est_4</th>\n",
       "      <th>hh_size_est_&gt;4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19550</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13337</th>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15074</th>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19724</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17789 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cltdob_fix  flg_substandard  flg_is_borderline_standard  \\\n",
       "19550        45.0              0.0                         0.0   \n",
       "4600         48.0              0.0                         0.0   \n",
       "13337        47.0              0.0                         0.0   \n",
       "15074        49.0              0.0                         0.0   \n",
       "19724        45.0              0.0                         0.0   \n",
       "...           ...              ...                         ...   \n",
       "11284        57.0              0.0                         0.0   \n",
       "11964        39.0              0.0                         0.0   \n",
       "5390         34.0              0.0                         0.0   \n",
       "860          32.0              0.0                         0.0   \n",
       "15795        45.0              0.0                         0.0   \n",
       "\n",
       "       flg_is_revised_term  flg_is_rental_flat  flg_has_health_claim  \\\n",
       "19550                  0.0                 0.0                   0.0   \n",
       "4600                   0.0                 0.0                   0.0   \n",
       "13337                  0.0                 0.0                   0.0   \n",
       "15074                  0.0                 0.0                   0.0   \n",
       "19724                  0.0                 0.0                   0.0   \n",
       "...                    ...                 ...                   ...   \n",
       "11284                  0.0                 0.0                   0.0   \n",
       "11964                  0.0                 0.0                   0.0   \n",
       "5390                   0.0                 0.0                   0.0   \n",
       "860                    0.0                 0.0                   0.0   \n",
       "15795                  0.0                 0.0                   0.0   \n",
       "\n",
       "       flg_has_life_claim  flg_gi_claim  flg_is_proposal  \\\n",
       "19550                 0.0           0.0              0.0   \n",
       "4600                  0.0           0.0              0.0   \n",
       "13337                 0.0           0.0              0.0   \n",
       "15074                 0.0           0.0              0.0   \n",
       "19724                 0.0           0.0              0.0   \n",
       "...                   ...           ...              ...   \n",
       "11284                 0.0           0.0              0.0   \n",
       "11964                 0.0           0.0              0.0   \n",
       "5390                  0.0           0.0              0.0   \n",
       "860                   0.0           0.0              0.0   \n",
       "15795                 0.0           0.0              0.0   \n",
       "\n",
       "       flg_with_preauthorisation  ...  recency_hlthclaim_14cb37  giclaim_amt  \\\n",
       "19550                        0.0  ...                       NaN        400.0   \n",
       "4600                         0.0  ...                       NaN        400.0   \n",
       "13337                        0.0  ...                       NaN        400.0   \n",
       "15074                        0.0  ...                       NaN        400.0   \n",
       "19724                        0.0  ...                       NaN        400.0   \n",
       "...                          ...  ...                       ...          ...   \n",
       "11284                        0.0  ...                       NaN        400.0   \n",
       "11964                        0.0  ...                       NaN        400.0   \n",
       "5390                         0.0  ...                       NaN        400.0   \n",
       "860                          0.0  ...                       NaN        400.0   \n",
       "15795                        0.0  ...                       NaN        400.0   \n",
       "\n",
       "       recency_giclaim  f_purchase_lh  hh_size_est_0  hh_size_est_1  \\\n",
       "19550                0            0.0            0.0            0.0   \n",
       "4600                 0            0.0            0.0            0.0   \n",
       "13337                0            0.0            0.0            0.0   \n",
       "15074                0            0.0            0.0            0.0   \n",
       "19724                0            0.0            0.0            0.0   \n",
       "...                ...            ...            ...            ...   \n",
       "11284                0            0.0            0.0            0.0   \n",
       "11964                0            0.0            0.0            0.0   \n",
       "5390                 0            0.0            0.0            0.0   \n",
       "860                  0            0.0            0.0            0.0   \n",
       "15795                0            0.0            0.0            1.0   \n",
       "\n",
       "       hh_size_est_2  hh_size_est_3  hh_size_est_4  hh_size_est_>4  \n",
       "19550            0.0            1.0            0.0             0.0  \n",
       "4600             0.0            1.0            0.0             0.0  \n",
       "13337            0.0            0.0            0.0             1.0  \n",
       "15074            0.0            1.0            0.0             0.0  \n",
       "19724            0.0            0.0            0.0             1.0  \n",
       "...              ...            ...            ...             ...  \n",
       "11284            0.0            0.0            0.0             1.0  \n",
       "11964            1.0            0.0            0.0             0.0  \n",
       "5390             0.0            0.0            0.0             1.0  \n",
       "860              0.0            1.0            0.0             0.0  \n",
       "15795            0.0            0.0            0.0             0.0  \n",
       "\n",
       "[17789 rows x 61 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on annual_income_est column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, None, 5, None, 5, 1, 5, 4, 3, 1, 4, 4, 4, 5, 3, 5, 5, 5, 5, 5, None, 3, 5, 5, 5, None, 5, 4, 1, None, 3, 5, 5, 5, 5, 5, 4, 5, 5, 1, 3, 1, 3, 5, 3, 2, 4, 5, 3, 5, 3, 1, 5, 3, 5, 3, 3, 3, 1, 5, 3, 3, None, 5, 3, 5, 5, 2, None, 5, 3, 5, 2, None, 3, 5, 3, 5, 5, 3, None, 5, None, 3, 5, 3, 5, 5, 1, 5, 5, 1, 5, 5, None, 5, 5, 5, 4, 1, 5, 1, 5, None, 5, 5, 1, 5, None, 5, 5, 3, 3, 5, 3, 1, 5, 1, 1, 5, 5, None, 5, 1, 5, None, None, None, None, 2, 5, None, 3, 5, 5, 5, 5, 5, 1, 1, 5, 5, None, 4, 1, None, 1, 5, 5, None, None, 5, 5, 3, 5, 5, 5, 1, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 1, 3, 5, 3, 1, 3, 3, 5, 5, 3, 5, 3, 3, 5, 5, 5, None, 5, 5, 4, 5, 1, 5, 3, 5, 1, 1, 3, 5, 5, 5, 4, 1, None, 5, 5, 5, 5, 5, 3, 4, 5, 3, 1, 5, None, None, 4, 5, 4, 5, 5, 5, 5, 1, None, 3, 5, None, None, 5, 5, 5, 5, 5, None, 1, 3, 1, None, 5, 5, 2, 3, 5, None, 5, 1, 5, 1, 5, 5, 5, 3, 3, 5, 5, 5, 4, 5, 1, 3, 5, 4, 1, 5, 4, 3, 4, 5, None, 5, 5, 5, 4, 3, 1, None, 3, 5, 5, 3, 3, 5, 5, 5, 5, 5, 5, None, None, None, 4, 5, 4, 3, 3, 3, 3, 1, 5, 3, None, 1, 5, 5, None, 5, None, 3, 5, 5, 3, 5, 5, 2, None, 1, 5, None, 1, 5, 5, None, 5, 5, 5, 5, 3, 5, 5, 1, 3, 5, 5, 3, 2, 2, 1, 3, 5, None, 1, 5, 5, 3, 1, 3, None, 4, 5, 5, 1, 3, 5, 5, 4, 5, 3, 3, 5, 5, 5, 5, None, 5, None, 4, 4, 5, 5, 3, 5, 5, 5, 5, None, 1, None, 4, 4, 1, 5, 5, 5, 5, 1, 1, 4, 3, None, 3, None, 5, 2, 3, 5, 1, 3, 5, None, 5, 1, 5, None, None, 3, 5, None, 5, None, 4, 1, None, 3, 4, 5, None, 5, 1, 4, 1, 2, 3, 5, 3, 5, 5, None, 1, 3, 3, 5, 5, 5, 2, 1, 5, 2, 5, 1, 5, 2, 1, 5, None, 4, 5, 5, 4, None, 4, None, 1, 5, 5, 5, 4, None, 3, 5, 2, 3, 2, 5, 4, 4, 1, 5, 1, 5, 3, 3, 5, None, 5, None, 3, 5, 5, 5, 5, 2, 5, 5, None, 5, 5, 3, None, None, 2, 3, 5, 5, 5, None, 5, 1, 5, None, 3, 1, 5, 3, 5, 2, 3, 5, 3, 5, 1, 5, 3, 4, 5, 4, None, 5, 5, 2, 5, 3, None, 5, 5, 5, 5, 5, None, 1, None, 5, 3, 3, 2, 3, None, 4, 4, 1, 5, None, 5, 1, None, 3, None, 4, 5, 4, 3, 5, 1, 4, 5, None, 5, 5, None, 5, 4, 5, None, 1, 3, 5, 1, 5, 5, 5, 5, 5, 5, None, 4, 5, 5, None, 4, 3, 5, 5, 4, None, 2, 5, 4, 5, None, 5, 3, 4, 3, 3, 3, 5, None, 5, 4, 5, 1, 5, None, None, None, 4, 5, 5, 2, 4, 4, 3, 3, 5, 5, 3, 3, 5, 4, None, None, 5, 3, 4, None, 5, 4, 5, 1, 5, 5, 3, 5, None, None, 5, 2, 5, None, 3, 1, None, 5, 5, 1, 3, 3, 5, None, None, None, None, 4, 5, 5, 5, 5, None, 1, 5, 3, None, 5, None, 4, 5, 1, 3, 1, 5, 5, 3, 2, 2, 5, 5, None, 5, 3, 4, 5, 5, 3, 1, 5, 3, None, 5, 5, 5, 2, 5, 5, None, 5, 1, 1, 1, 2, None, 5, 5, 4, 5, 1, 2, 3, 3, 5, 5, 1, 3, 5, 5, 4, 3, 5, 5, 2, 5, None, None, 5, 5, 3, 5, 3, 5, 5, 4, 5, 5, 1, 4, None, 4, 5, 4, 5, 4, 5, 3, 5, 5, 5, 5, None, 5, None, 5, 3, 5, None, 1, 1, 1, 3, 4, 5, 5, 2, 5, 5, 1, 4, 5, 5, None, 5, 5, 5, 5, 5, 5, 5, 1, 4, 5, None, None, 5, None, 5, 5, 5, None, 4, 3, None, 5, 2, 3, 5, 5, 4, 5, 3, 1, 5, 2, 3, None, 5, 5, 4, 4, 1, 5, 5, 5, 3, 5, 4, 5, 3, 3, 5, 4, 3, None, 4, None, 1, 3, 5, 5, 4, 5, None, 3, 3, 3, 3, 5, 5, 5, 5, 5, 3, 3, 5, 1, 5, 3, 2, 3, 4, 4, 1, 5, 3, None, 2, 5, 2, 3, 5, 3, 1, 5, 3, 5, 3, 5, 5, 5, 4, 5, 5, 3, 5, 1, 5, 2, None, 1, 5, 4, 5, 1, None, 5, None, 5, 2, 3, 5, 1, 4, 5, None, 5, None, 1, 5, 4, 2, 4, 2, 4, 5, 5, None, 3, 5, 3, None, 5, 4, 4, 4, 3, 3, 4, 1, 5, 4, 2, 5, 2, None, 5, 5, 5, 4, 5, 4, 5, None, 5, 5, 5, 1, 1, 5, 1, 1, None, 4, None, 5, None, 5, 5, 1, None, None, 3, 5, 3, 5, 3, None, 5, None, 4, 3, None, 4, 3, 4, 5, 5, 4, None, 1, 5, 3, 1, 5, None, 2, 3, 1, 1, 4, 1, 3, 1, 5, 5, 5, 1, 5, 5, 4, 1, 3, 5, 4, 4, 5, 3, None, 1, 3, 3, 2, None, 1, 2, 5, 4, 1, None, 5, 3, 5, 4, 2, 5, 5, 3, 3, 5, 5, None, 5, 1, None, 5, None, 5, 4, 4, 1, 5, 5, 1, 4, 4, 1, 1, 5, 3, 2, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 3, None, 3, 5, 3, 5, 3, 4, 5, 5, 5, 4, 5, None, 4, None, 4, 5, 5, 1, None, 3, 1, 5, 5, 5, 5, 4, 5, 5, None, 5, 5, 5, 5, 3, 5, 3, None, 3, None, 3, 1, 5, 3, 3, 5, 1, 5, 3, 5, None, 3, 5, None, 3, None, 4, 5, 5, 5, 5, 5, None, 2, 3, 3, 5, 5, 1, None, None, 4, 4, 4, 2, 5, None, 5, None, 3, 4, 5, 5, 2, 5, 5, 5, None, None, 4, 1, None, 5, 4, 5, 3, None, 5, 5, 5, 3, 1, 5, 1, 5, 5, 5, 4, 5, None, 5, None, None, None, 1, 5, 3, 4, 5, 5, None, 3, 5, 5, 5, 4, 5, 5, 5, 5, None, 5, 5, 5, None, 4, 3, 5, 5, 5, 2, 1, None, 5, None, 1, 5, 5, 5, 5, 4, 5, 1, 2, 5, 5, 5, 5, 2, None, 5, 5, 5, 5, 5, 3, 5, 5, 5, None, None, 3, 4, 5, 3, 3, 3, 1, 3, None, 2, 5, None, 4, 4, 5, 5, 5, 4, 5, 5, None, 4, 5, 1, 5, 5, 1, 3, 3, 5, 5, None, 3, 5, 5, 3, 5, 3, 5, 4, 2, 3, 3, 3, 5, 3, 5, 5, None, None, 2, 5, 5, 1, 3, 3, 5, 5, 5, 1, 5, 4, 5, 3, 5, 5, 1, None, 5, 5, 3, 5, 4, 3, 5, 1, None, 3, 4, 5, 4, 5, 5, 5, 1, 3, 5, None, 1, 5, 5, None, 1, 5, 5, 3, 5, None, 4, 3, 5, 3, 4, 4, 5, 5, None, None, None, None, 5, 1, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 4, 3, 5, None, 5, 1, None, 5, 5, None, 5, None, 3, 1, 5, 4, 5, None, 5, 3, 5, 1, None, None, 1, 1, None, None, 5, 1, 3, 1, 5, 4, 5, None, 1, 5, None, 5, 5, 1, 3, 5, None, None, 3, 5, 4, 5, 5, 3, 4, 3, 5, 1, 4, 3, 2, 5, 3, 5, None, 5, None, 3, 1, 4, 5, 3, None, None, 5, 5, 5, 5, 4, 5, 1, 5, None, 5, 5, 5, 5, 5, 4, 4, 3, 5, 3, 5, 3, None, 3, 5, None, 5, 4, 4, 4, 5, 5, 3, 5, 3, 5, 3, 5, 1, 3, 5, 4, 4, 1, None, 1, 5, None, None, 5, 5, 5, 5, 5, None, 5, 5, 2, 5, None, 1, 5, 2, None, None, 5, 5, 5, 1, None, 5, 5, 5, 4, 3, 5, 5, None, 5, 5, 1, 5, 5, 4, 5, 5, 5, None, 5, 1, 3, 1, 4, 5, 3, 1, 5, 5, 3, 5, 5, 1, 4, None, None, 5, 3, 4, 5, 3, 4, 5, 5, 5, 5, 4, 5, 3, 5, None, 5, None, 2, 1, 5, 5, 5, 1, 2, 5, 5, 1, 2, 2, 3, None, 5, None, 3, 3, None, 1, 2, 4, None, None, 4, 4, 4, None, 4, 3, None, 3, 5, 1, 5, 5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 1, 5, 3, None, 1, 5, 3, 5, 3, 3, 1, 4, 5, 4, 4, 5, 4, None, None, 5, 3, 5, 3, 4, 5, 4, 1, 3, 2, 4, 5, 3, 5, 5, None, None, 5, 3, 3, 5, 4, 5, 5, 4, None, 5, 5, 3, 5, 5, 5, 5, None, 5, 5, 1, 5, 3, 4, 2, 5, 5, 3, 1, 4, None, 4, 5, 1, 3, None, 5, 3, 5, 1, 5, 5, 5, 1, 5, 5, 3, 3, None, 1, 5, None, 5, 5, 5, 3, None, 5, 5, 5, None, 5, 5, 3, 5, 4, 5, 3, None, 1, 5, 5, None, 4, 4, 1, 5, 4, 3, 5, 5, 5, 5, 3, 5, None, 2, 3, None, 5, 5, None, 1, None, None, 3, None, 5, None, None, 3, 1, 1, 4, None, 3, 5, 3, 4, 2, 5, 5, 5, 3, None, 5, 3, 1, 5, 4, 4, None, 5, 5, 4, 5, 5, 3, 5, 5, 5, 1, 1, 5, 1, 5, 1, None, 3, 2, None, 1, None, 5, 3, 5, 5, 4, 5, 4, 4, None, 3, 5, 5, 5, 5, 5, 2, 1, 5, 1, 5, 3, 3, 5, 3, None, None, 4, None, 4, 5, 5, 2, 3, 5, 5, 5, 5, None, 5, None, 5, 5, 5, 1, None, 5, None, 5, 3, 1, None, 4, 5, None, None, 5, 5, 5, 3, None, 4, 5, 5, 4, 5, 1, 1, 3, None, 1, 2, 5, None, 3, 5, 1, 5, 3, None, None, 3, 3, 4, 5, 5, 5, 5, 5, 4, 4, 2, 3, 3, 5, 4, 4, 1, 1, 3, 1, None, 5, 4, None, 3, 4, 5, 5, 5, 5, 5, 5, 3, 1, 4, None, 5, 4, 5, 3, 5, 5, 3, None, 5, None, 5, 3, 4, 5, 3, 5, None, 5, 5, 3, None, 5, 3, 3, 5, 3, 3, 1, 1, 4, 5, None, 3, 1, 1, 5, 1, 3, 5, None, 5, None, 5, None, 5, 5, 5, 5, 4, 3, 4, 2, None, 1, 3, 3, 4, None, 3, 3, 3, 5, 3, None, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 3, 3, 4, 5, 5, 5, 5, 1, None, None, 5, 1, 2, 5, 5, 5, 4, 5, None, 5, 5, 5, None, 5, None, 5, 5, 5, 5, 3, 5, 4, 4, 5, 5, None, 2, 4, 4, 3, None, 3, 5, 4, 3, 4, 4, 5, 1, 1, 4, 5, 5, 3, 2, 4, 3, 4, 5, 1, None, None, 1, 4, 5, None, 5, 5, 5, 5, 4, 4, 1, 5, 5, None, 4, 1, 3, 1, 5, 5, None, None, 5, 1, 4, 4, 5, 5, 5, 5, 2, 5, 1, 5, 1, 5, 5, 3, 5, None, None, 5, 4, 5, 5, 5, 5, None, 5, 5, 5, 5, 5, None, 3, 5, 5, 5, 1, None, 4, 5, 4, None, 3, 3, 4, 5, 3, 1, 3, 3, 5, 5, 3, None, 5, 3, 1, 5, 5, 4, 5, None, 5, 5, 3, None, 1, 2, 2, 5, 5, 5, 5, 2, 5, 5, None, None, None, 5, 5, 3, 5, 5, 4, 4, 5, 1, None, 5, 3, 5, 5, 5, 1, 3, 5, 1, None, 5, 4, 5, 5, 1, 1, 4, 5, 1, 1, 4, None, 1, 4, 5, None, 1, 3, 3, None, 1, 5, 3, None, 5, 1, 3, None, 1, 5, 1, 5, 5, 5, 2, 5, 4, 5, 5, None, 5, 3, 1, None, 3, 3, 3, 4, 4, 1, 3, 5, 1, 3, 3, 2, 5, None, 5, 4, 1, 4, 4, 5, None, None, 5, 5, 1, 4, 5, 5, 3, 5, None, 5, 4, 2, 5, 3, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 3, 4, 4, 4, 5, 2, 3, 5, 3, 5, 5, 2, 5, None, 3, 5, 5, 1, 3, 5, 5, 5, 5, 4, 5, 4, None, 3, 1, 1, None, 3, 1, 3, 4, 5, 3, 2, 5, None, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 3, 3, 4, 5, 1, None, 5, 5, 3, 2, None, None, 5, 3, 1, 3, 5, 5, 4, 3, 3, 1, 5, 4, None, 5, 5, 5, None, None, 1, 5, 5, 3, None, 5, None, 5, 4, 5, 4, 5, 3, None, 5, 3, 5, 4, 5, 5, 4, 5, None, 5, 5, 1, 4, 4, 3, None, 5, 1, 5, 5, 2, 5, 5, 5, 3, 1, 1, 3, 2, 3, None, 5, 2, 3, 5, 5, 3, 3, 5, 5, 5, 3, 1, 4, 5, 1, None, 5, 4, 5, 3, 1, 5, 4, 5, None, 3, 5, None, 5, 4, 3, 1, 5, None, None, 5, 2, 1, 3, 5, None, 2, 3, 5, 3, 4, 5, 4, 5, 5, 4, 4, 5, None, 5, 3, 3, 5, 3, 3, None, 5, None, None, 5, 3, 4, 3, 5, 5, 5, 5, 5, None, 5, 1, 1, 3, None, 1, 5, 2, 3, 3, 5, 3, 1, 1, 5, 4, None, None, 5, 5, 3, 5, 5, 3, 5, 2, 1, 5, None, 2, None, 5, 1, 3, 3, 3, 4, None, 5, 5, 5, None, 2, 5, 2, 5, 5, 2, 2, 5, 5, 5, 3, 5, 5, 3, 3, 5, 5, 3, 5, 3, None, 5, 5, 5, 4, 5, 3, 4, 5, 5, 3, 3, 5, 3, 5, 4, 4, 5, 5, 1, None, 5, None, 3, 4, 5, 5, 5, 5, None, 5, 1, 5, 3, 5, None, 4, 5, 4, 1, 5, 5, 5, 5, 5, 3, 4, 4, 5, None, 5, 5, 1, None, 5, 5, None, 1, 5, 5, 1, None, 5, 4, 5, 1, None, 4, None, 5, 5, 3, None, 2, None, 5, 4, 1, 1, 3, 5, None, 5, 4, 1, 3, 4, None, 5, 4, 1, 5, 5, 5, 3, 3, 3, 5, 1, 3, 4, 3, 1, 3, 5, 5, 3, 4, 5, 4, None, None, 5, 3, 3, 5, 3, 5, 5, None, 5, 5, 3, 1, None, 1, None, 5, 3, 5, 3, 5, 5, 4, 1, 5, 4, 5, 5, 5, 4, 1, 3, None, None, 5, None, 4, 3, 3, 1, 5, 5, None, 5, 2, 5, 5, 4, 1, 3, 3, None, 5, 5, 5, None, 4, 5, 1, 1, 4, 5, 5, 4, 3, 5, 5, 1, 5, 2, None, 5, 5, 3, 4, 5, 5, 4, 5, 5, None, 5, 5, 5, None, 5, 3, 3, 3, 5, 5, 5, None, 1, None, 3, 5, 5, 5, 1, 3, 5, 3, 4, 4, None, None, 3, 5, 3, 1, 5, 1, 2, 3, 1, 3, 4, 4, 2, 5, None, 4, 5, None, 4, 5, 5, 3, 5, 1, 5, 5, 5, 3, None, 1, 5, 5, 5, 5, 3, 5, 3, 5, 5, 5, 4, 5, 4, 5, 1, 5, 5, None, 1, None, 5, 5, 4, 5, None, 5, 5, 5, 4, 3, 5, None, 5, None, 1, 5, 1, 5, None, 4, 5, 1, 1, None, 5, 1, 5, 5, 1, 5, 4, 4, 2, 5, 1, 3, 5, 3, 4, None, 5, 3, 4, 5, 5, 1, 1, 5, None, 5, 5, None, 3, 3, 5, 5, 1, None, 5, 5, 4, 1, 1, 5, 3, 5, 1, 3, None, 4, 5, 5, None, 5, 4, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 5, 3, 5, None, 1, 3, 5, 5, 3, 5, 4, 5, 5, 1, 3, 5, 3, None, 5, 5, 5, 3, 5, 5, 5, 4, 5, 3, 3, 4, 4, None, 3, 4, 5, 5, 3, 5, 3, None, 4, 3, 5, 1, 4, 5, 5, 1, None, 3, 1, 4, 1, 5, 2, 3, 1, None, None, 1, 5, 1, 1, 3, 5, 5, 3, 5, None, 3, None, 3, 5, 4, 3, None, 1, 1, 4, 5, None, 5, 1, None, None, 1, None, 4, 5, None, 5, 3, 1, 1, 5, None, 5, 5, 5, 5, 5, 5, 5, 1, 1, 2, 5, None, 5, None, 5, 5, 5, 5, 3, 3, None, 5, 5, 5, 5, None, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, None, 5, 5, 5, 3, None, 5, 5, None, 4, 4, 4, 3, 1, None, 5, 3, 1, 3, 4, 5, 5, 5, 5, None, 5, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 2, 4, 5, 5, 3, None, 1, 2, None, None, 3, 1, 5, 5, 5, 3, 4, 5, 5, 3, None, None, 5, 1, None, 2, 2, 5, 5, 4, 4, None, 4, None, 5, 4, 5, None, 5, None, None, 3, 4, 4, 5, 4, 3, 5, 5, None, 4, 5, 3, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 2, 3, 5, 5, 5, 1, 5, 3, None, 1, 5, 5, 3, 5, 5, 4, 1, 3, 4, 5, None, 5, 5, 5, 3, 5, 5, 5, 5, 3, None, 1, 5, 4, 3, 3, None, 4, None, None, 4, 5, None, 5, 5, 5, 4, 5, 4, 3, 5, 5, 5, 4, 3, None, None, 5, 1, 2, 3, 5, None, 5, 5, None, 5, 5, 5, 5, 5, None, 4, 5, 5, 2, 5, 1, 5, 5, 1, 5, 5, 3, 3, 5, 5, 2, 5, 1, 4, 5, 5, 5, 4, 4, 1, 4, None, 2, 5, 1, None, 1, 5, 5, 5, 3, 5, 5, 5, 5, 5, 4, 5, 5, 1, 1, 4, None, 4, 5, 5, 1, 1, 3, 5, 5, 4, 4, 5, 1, 5, 5, 5, 5, 5, 5, 4, 5, 5, 2, 5, 5, 2, 4, 4, 5, 5, 5, 5, 5, 1, 3, 5, 5, None, 4, 5, None, 5, 1, 5, 1, 4, 5, 1, 5, 5, 4, 5, None, 5, 1, 5, 5, None, 5, 1, 3, 5, 2, 5, 3, 4, 2, 5, None, 1, 3, 5, 5, 5, 3, None, 3, 3, 2, 2, None, None, 4, 1, 5, 3, None, 5, 5, None, 5, 3, None, 3, 5, 5, None, 5, 5, 1, None, None, 5, 4, 5, 3, 5, 1, 5, 5, 3, 4, 4, 4, None, 3, 1, None, 1, 4, 5, 1, None, 5, 5, 5, 4, 5, 1, 1, 5, None, 5, 5, 5, None, None, None, 5, 5, 5, 5, 5, 4, 4, 5, 3, 5, 5, 1, 5, 5, 1, 1, 5, 4, 2, 3, 4, 3, 1, 5, 5, 1, 5, 5, None, 5, 5, 5, 1, 5, 5, 5, 5, None, 4, None, None, 5, 4, 4, None, 3, None, None, 1, 5, 5, 3, 5, 3, 5, 5, 1, 5, 5, 4, None, 5, 1, None, 5, 3, None, 5, 4, None, 5, 5, 3, 4, 5, 4, 5, 5, 3, 1, 4, 3, 1, 4, 5, 5, 5, 5, 4, 5, None, 3, 5, 4, 5, 3, 5, 5, 5, 5, 5, None, 3, 1, 4, 5, 4, None, 5, 5, 1, 5, 3, 5, 1, 5, 3, 1, None, 3, None, None, 1, 3, 5, 5, 4, 5, None, 3, 5, None, 3, 5, 5, 3, 5, 5, 5, 1, 5, 2, 5, 2, 5, 3, 5, 5, 5, 1, 3, 5, 3, None, 5, 5, 5, 5, None, 2, None, 4, None, 2, 3, 3, 5, None, 5, 4, 1, None, None, 5, 5, 5, 5, 3, 5, 3, 5, 5, 4, 3, 5, 5, 1, 5, 3, 5, 5, None, 3, 5, 5, 3, 1, None, 3, 5, None, 5, 2, None, None, 3, 5, 1, 5, 5, 5, 1, 5, None, None, 3, None, 3, 5, None, 5, 5, 3, 5, 5, None, 5, 1, 4, 5, 5, None, 1, 5, 5, 5, 2, None, 5, 5, 1, None, 1, 5, 5, 1, 5, 5, 2, 1, 5, 4, None, 5, None, 3, 5, 1, 4, 5, 3, None, 1, 5, 1, 1, 5, 5, 4, 2, None, 5, None, 2, None, 3, None, None, 5, 5, 5, 1, None, 1, 5, 5, 3, None, 5, None, 3, None, None, 5, 5, 5, 4, 3, 5, 1, 1, 5, None, 5, 3, 4, 5, 5, 5, 5, None, 1, 1, 5, 3, 5, 4, 5, 5, 2, None, 3, 4, 1, 5, None, 4, None, 5, None, 3, None, None, 4, 5, 3, 5, 5, 5, 3, None, 3, 3, 4, 5, 3, None, 1, 4, None, 4, 5, 4, 5, 3, 5, 5, 5, 3, 4, None, 3, 5, 5, 5, 3, 4, 5, 3, None, 5, 5, 5, 5, 5, 3, 5, 4, 5, 4, 1, 1, 3, 5, 4, 3, 5, 5, 5, None, None, None, 4, 5, 4, 4, 1, 5, None, 5, 1, 4, 5, 4, 5, 2, 5, None, None, 5, 3, 4, 5, 3, None, None, 5, 1, 5, None, 1, 5, None, 3, 5, 5, 5, 4, 3, 3, 5, 3, 5, 1, 5, 4, 5, 4, 5, 5, 5, None, 3, 5, 1, 2, 5, 5, 4, None, None, 4, 3, 5, 3, 5, 4, 5, 5, 5, 1, 1, 1, 4, 2, 5, None, None, 4, 5, 3, 4, None, 3, 5, 1, 4, 5, 5, 1, 5, 5, 1, 3, 5, 4, 5, 1, 5, None, 5, None, None, 5, 1, 1, 2, 5, 5, 5, 5, 1, None, 5, 1, 1, None, 4, 5, 3, 3, None, 4, None, 3, 5, 5, 2, 5, 2, 5, 5, 5, 5, 2, None, 4, 5, 1, None, 5, 5, 5, 5, 5, None, 5, 5, 4, 1, 2, 4, 5, 1, 5, None, None, 4, None, 5, None, 5, 5, 4, 5, 5, 1, 5, 5, 5, 5, None, 5, 5, 2, 5, None, 5, 5, 5, None, 5, 4, None, 5, 5, None, 5, 3, None, 5, 5, 4, 1, None, 4, 5, 4, 3, 5, 3, 3, 5, 4, None, 5, 5, 1, 5, 1, 5, 3, 4, 5, 4, 5, 5, 3, 5, 1, 5, 5, 3, 5, 5, 4, 5, 5, 5, 5, 1, 5, 3, 1, 3, None, 1, 5, 3, None, 5, 5, 5, 3, 1, 1, 5, 5, 5, 1, 1, 5, 5, 3, 5, 5, None, 3, 3, 3, None, 5, 5, 5, 1, 5, 5, 5, 5, 4, 1, 1, 1, 5, 5, 5, 5, 5, None, 1, 1, None, 5, 5, 5, None, 5, 1, 5, 5, 4, 3, 5, None, 1, None, 2, 3, 4, 5, 3, 5, 5, 5, 5, 5, None, 3, 2, 4, 1, 1, None, 5, 5, 3, 5, None, None, 1, 5, 5, 3, 5, 1, 5, 5, None, 3, 5, 5, 5, 5, None, 5, 4, None, 1, 1, 4, 1, 1, 3, 5, 5, 1, 5, 3, 1, 3, 3, 1, 5, 4, 3, 1, 1, None, 5, 3, 2, 4, 3, 2, None, 5, 5, 4, 3, 3, 5, 5, 2, 5, 5, 5, 2, None, None, 1, 1, None, 5, 1, 3, 1, 5, 1, 5, None, None, 3, 5, 5, 1, 5, 5, None, 3, 1, 1, None, 5, 3, None, 1, 3, 3, 5, 5, 3, 5, None, 2, 5, 1, 5, 2, 2, 1, None, 1, 4, 2, 4, None, None, None, 1, 4, 5, 1, 3, 4, 2, 5, 4, 5, 5, 5, 5, 4, 3, None, 5, 3, 5, 5, 5, 1, 5, 4, 3, 4, 5, None, 5, 5, 1, 5, None, 5, None, 5, 5, 4, 5, 1, 5, None, 5, 5, 5, 5, 3, 4, 1, 3, 4, 4, 5, None, None, 5, 5, None, 3, 5, None, 5, 5, 1, None, 4, 5, 5, 5, 5, None, 5, 5, 4, 5, None, 5, 5, None, 5, 3, 5, 5, 5, 1, 3, None, 3, None, 5, 4, 5, 4, 5, 5, 3, 5, 5, 4, 5, 5, 3, 1, 1, 3, 5, 1, 4, 3, 5, 3, 3, 2, 3, 5, 5, 5, 3, None, 5, 5, 5, 4, 5, 5, 4, 5, 3, 2, 5, 2, 1, 4, None, 4, None, 1, 3, 1, 5, 3, 4, 5, 5, 3, 3, 1, 5, 5, 1, 5, 5, 5, 3, 5, 1, None, 5, 5, 5, 2, 3, 5, 5, None, 2, None, 5, 5, 3, None, 4, None, 4, 1, 4, 5, 5, 3, 5, 4, 5, 5, None, None, 1, None, None, 1, 4, None, None, 3, 5, 1, 5, 5, 3, 5, 4, None, 1, None, 5, 3, 5, None, 5, 5, 4, 5, 5, 3, 5, None, 5, 5, 5, 5, 5, 4, 4, None, 5, 3, 5, 5, 5, 5, 5, 2, 5, None, 5, 5, 3, 5, 4, 5, 5, 5, 3, 3, 3, 5, 4, 1, 5, 2, 1, 5, None, 5, 3, None, 3, 5, 4, 5, None, 4, 1, 5, None, 4, 1, 4, 3, None, 5, None, None, None, 5, 5, 5, None, 5, None, 1, None, 4, 1, 5, None, 5, 5, 5, 1, 4, 5, 5, 5, 3, 3, 5, 4, None, None, 1, 3, 5, 3, 5, 3, 5, 4, 5, 5, 5, None, 3, 5, None, 5, 1, 5, 4, None, 5, 5, 3, 5, 1, 5, 5, 5, 5, 2, 3, 1, 5, 3, 5, 5, 5, 5, 5, 1, 5, 5, None, 5, 5, 1, 4, None, 5, None, 5, 4, 5, 3, 5, 4, 3, 5, 1, 1, 5, 1, 5, 5, 2, 5, 5, 1, 4, 5, 5, 5, 1, 1, 5, 5, 3, 5, 2, 3, 5, 5, 4, None, 5, 5, 5, 5, None, 5, 5, 5, None, 5, None, 3, 3, 4, 2, 3, 3, 3, 5, None, 3, 5, 3, 3, 5, 3, 3, 5, 5, 5, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, None, 5, 3, 5, 5, 3, 5, 5, 1, 5, 4, 4, 5, 5, 5, 1, 2, 1, None, 5, 5, 1, 4, 1, 5, 3, 2, 4, 5, 5, 3, 3, 5, 5, 4, None, 5, 5, 5, 1, 5, 5, None, None, None, 3, 3, 1, 5, 5, 5, None, None, None, 5, 5, 3, None, 5, 3, None, 3, 5, 3, 2, 3, 3, 3, 2, 3, None, 1, 1, 5, 5, 5, 5, 3, 4, 1, 4, 5, 5, 2, 5, 1, 3, 1, 4, 1, 5, 1, 3, None, 5, 3, 5, 5, None, 5, 5, 2, 5, 5, 5, None, None, 5, 5, 1, 5, 1, 5, 5, 4, 3, 4, 5, 5, 5, 5, 1, None, 5, 5, 5, 5, 2, 5, 1, 5, 3, 4, 3, 3, 5, 2, None, 1, 2, 3, 4, 4, 1, 5, 3, 3, 5, 1, 3, 5, 4, 3, None, 3, 5, 1, 5, None, 5, 4, 5, 5, 1, 1, 2, 3, 5, 4, 5, 4, 3, 3, 5, 4, 5, 1, None, 5, 4, None, 1, 1, 5, 4, None, 5, 3, 5, 5, None, 5, 5, 1, 5, None, 3, 5, 3, 5, None, None, 3, None, 4, 5, 5, 3, 1, 5, 5, 5, 5, 4, 2, 4, 3, 5, 5, 1, 3, 4, 1, 3, None, 5, 1, None, None, 4, 3, 5, 1, 3, 3, 5, 5, 1, 5, 5, 5, 5, 4, 5, 2, None, 1, 5, 5, 5, 4, 5, 5, 5, 5, 3, 5, 5, 5, 2, 5, None, 5, None, 1, 5, 1, 3, 3, 4, 4, None, 4, 3, 4, 5, 2, 5, 5, None, 5, None, 5, 4, 1, 1, 1, 1, 5, 1, 4, 4, 5, 5, 3, 3, 5, 3, None, 2, 5, None, 5, 1, 5, 5, None, 5, 1, 5, 5, 5, None, 5, 4, 1, 5, 5, 5, 4, 4, 5, 3, 5, 5, 5, None, 2, 4, None, 4, 5, 4, 5, 1, 5, None, 5, 1, 5, 4, 5, 2, 5, 5, 3, 4, None, 1, 4, 4, 5, 5, None, 4, 2, 5, 5, None, None, 5, None, 1, None, 1, 5, 5, 5, 3, 4, 3, 5, 4, 4, 5, 5, 4, 4, 4, 5, None, 1, 3, 3, 3, None, 5, 1, 5, 3, None, None, 5, 3, 5, 5, 1, 3, 3, 5, None, 5, 5, 5, 5, 4, 5, 5, 2, 5, 3, 5, 5, 5, 5, 4, 1, 5, None, 5, 4, 5, 5, 3, 5, 5, 5, 5, 5, 5, 1, 1, 4, 5, 1, 2, 3, 5, 5, None, 5, None, 1, 3, None, None, 5, 5, 3, 2, 5, 5, 5, 5, 3, 4, 1, 5, 5, 1, 5, 5, 3, 4, None, 2, 5, 5, 5, 5, None, 5, None, None, 5, 5, 4, 1, 5, 5, None, 5, 2, None, 2, 1, 1, None, 1, 5, 2, None, 5, 1, 5, 5, 3, 4, None, 2, 1, 5, None, None, 1, 3, 4, 5, 5, 4, 5, 5, 4, 5, 5, 3, 2, 5, 5, 5, 3, 5, 5, 1, 5, None, 4, 5, 5, None, 4, 5, 5, 1, 5, 5, 5, 5, 5, 3, 1, 3, 5, 1, 5, 5, 3, 5, 3, 5, 1, 3, 5, 5, None, 3, 5, 3, 3, 5, 3, None, 1, 5, 5, 5, 5, None, 5, 1, 5, 3, 2, 4, 1, 3, None, 3, 1, 1, 5, None, None, 1, 5, 4, 4, 3, 1, 5, 5, 4, 3, None, 5, 3, None, 5, 5, 5, 4, None, None, 2, 3, 3, 5, 4, 4, 3, 1, 3, None, None, 5, 5, 5, 4, 5, 5, None, 3, 5, 5, 3, 5, 5, 5, None, None, 1, 5, 5, 1, 3, 1, 4, 3, 5, 5, 3, 1, 2, 3, 5, 2, 5, None, 5, 5, 2, 4, 4, 5, 5, 3, 3, 5, None, 5, None, 5, 3, 5, 3, 5, 5, 1, None, 3, None, 1, 4, 5, 4, 5, 2, None, 1, 5, 3, 5, 5, 5, 1, 1, 4, 5, 3, 5, 4, 2, None, 4, 5, 5, 3, 5, 5, 5, 5, 5, 5, None, 5, 5, 1, 5, 2, 3, 2, None, 5, 1, 3, 3, 3, 5, 4, 4, 5, 1, 5, 5, 5, 3, 5, None, 5, None, 5, 5, 3, 5, 3, 5, 4, 5, 1, 3, 5, 5, 5, None, 1, None, 5, 5, 5, 3, 5, 1, 3, 3, 5, 1, 5, 2, None, 5, 5, 5, 5, 2, 5, 5, 4, 3, 5, 5, 4, 5, 3, 3, 5, 4, None, None, 4, 5, 4, None, 5, 3, 5, 4, 1, None, 1, 5, 1, 5, 1, 5, 4, 5, 2, 5, 5, 3, 5, None, 5, 1, 5, 5, 1, 2, 1, 5, None, 3, 3, 1, 5, 3, 4, 3, 3, 5, 5, 5, 5, 5, 5, 2, 3, 5, 5, None, None, 5, None, 5, 5, 5, 5, 3, None, 3, 4, 1, 1, 5, 1, 5, 5, 5, 5, 3, 1, 5, 4, 5, 5, 5, 5, 5, 3, 4, 5, 3, 5, 3, 5, 1, None, 5, 3, 5, 3, 5, 5, None, 5, 5, 5, 4, 4, 4, 5, 1, 5, 4, 1, 4, 5, 3, 2, 5, 2, 1, 1, 1, 5, 5, 3, 4, 5, 2, 3, 3, 5, 5, 5, None, 5, 3, 1, 5, 1, 5, 5, None, 5, 5, 3, 5, 5, 3, None, 5, 3, 5, 5, 2, 1, 5, 2, 5, 5, 5, 5, 5, 5, 4, 5, None, 5, None, 4, 4, 3, 2, 1, 1, 1, 2, 5, 5, 5, None, 3, 1, 4, 4, 5, 5, 1, 5, None, 1, 3, 1, 5, 5, 1, 5, 5, None, 5, 1, 5, 5, 5, 1, 4, 2, 3, 1, 4, 3, 5, 2, 1, 1, 5, 3, 5, 5, 3, 5, 5, 5, 5, 1, 5, 3, 5, None, 3, 5, 5, None, 4, 4, 5, 3, None, 4, 2, None, 1, 5, 5, 5, 1, 5, 5, 3, 5, 5, 5, 5, 5, 4, None, 5, None, 5, 5, None, 3, None, 3, 3, 1, 1, 5, None, None, 5, 3, 2, 2, 1, 4, 1, 5, 1, 5, 4, 4, 4, 5, 5, 5, 1, 5, None, 1, 1, 5, 5, 2, 5, 2, 5, 3, 5, None, 5, 5, None, 5, 5, 5, 4, 5, 1, 3, 1, 3, 5, 5, 5, 2, 5, 5, 1, 3, 5, 5, 4, 5, 4, None, 1, 5, 4, 5, 1, 1, 5, 4, 5, 5, None, 5, 1, 4, 5, None, None, 5, 1, 3, None, 1, 5, 5, None, 5, 5, 1, 3, None, None, 5, 5, 4, 2, 5, 4, 3, 5, 5, 3, 5, 1, 5, 5, 4, 5, None, None, 3, 5, 2, 1, 5, 4, 5, 4, 2, 1, 5, None, 5, 5, 4, 4, 4, 5, 1, 5, None, 5, None, None, 5, 1, 1, 4, 5, None, 1, 1, 3, 5, None, 2, 3, 5, 5, None, 1, 5, 1, 5, 5, 4, 3, 3, 2, None, 5, 4, 3, 1, 3, 4, None, 2, 4, None, 3, None, 3, 1, 5, 5, 3, 4, 3, 3, 1, 3, 2, 5, 5, None, 5, 5, 5, 4, None, 1, 5, 3, 4, 3, 5, 5, None, 1, 5, 4, 5, None, 5, 5, 5, 3, 4, 4, 4, 5, 5, 5, 5, None, 1, None, 5, 5, 5, 5, None, 4, None, 5, 3, None, 5, 3, 5, 1, 3, 5, 3, 2, 5, 3, 5, 3, None, 2, 3, 5, 1, 4, 1, 1, None, 5, 3, 5, 1, None, 1, 5, 5, 3, 3, 5, 1, 4, 5, None, 1, None, 5, None, 5, 3, 5, 5, 3, 3, 1, None, 5, 5, 5, 3, 5, 5, 5, 5, 3, None, 3, None, 5, 5, 1, 4, 3, 5, 5, 5, 3, 5, 4, 3, 1, 2, None, 2, 5, 5, 5, 5, 5, 5, 3, 5, 4, None, 4, 5, 3, 1, 5, 5, 5, 5, 4, 5, 5, 5, 3, 5, None, 5, 5, 1, 1, 1, None, 5, 5, 4, 5, 2, 3, 1, 5, 5, 5, 5, 5, 5, None, None, 5, None, 5, 1, 5, 5, 4, 5, 3, 1, None, None, 5, 5, 2, 5, 4, None, 1, None, 5, 5, 1, 5, 3, 1, 5, None, None, 5, 5, None, 5, 5, 5, 5, 4, 5, 5, 3, 5, 5, 1, 5, 5, 3, None, 3, 5, 3, 5, 2, None, 5, 5, 5, 5, 1, 5, None, 5, 1, 2, 5, 5, None, 5, 5, None, 4, 1, 5, 5, 5, 5, None, 1, 3, 3, 1, 5, 5, 5, 5, 1, 1, 5, 4, 5, 5, 1, 5, 2, None, 5, 5, 5, 5, 4, None, 5, 5, None, 3, 3, 1, 4, None, 2, 5, 3, 3, 5, 5, 5, 3, None, 5, 5, 5, 5, None, 1, 4, 4, 3, 3, None, 5, 2, None, 1, 5, 2, 5, 5, 5, 3, 1, 5, 5, 5, 3, 5, None, 5, 5, 5, 1, 5, 3, 4, 5, None, None, 5, 1, None, 5, 5, 5, 5, 5, 5, None, 5, None, 4, 1, None, None, 5, None, 3, 3, None, 5, 5, None, None, 3, 4, 1, 1, None, 1, 5, 3, None, 1, 5, 5, 4, 1, None, 5, None, 5, 5, 1, 5, 5, 5, 5, 1, 5, None, 5, 5, 5, 2, None, 5, 5, 4, 5, None, 5, 1, 5, 3, 5, 1, 5, 1, 5, 1, 5, 1, 3, None, 5, 3, 1, 5, 1, None, 5, 5, None, 2, None, 5, 3, 5, None, 5, 5, None, 5, None, 3, 5, 5, 5, 3, 5, 3, 5, 5, 5, 5, 5, 5, 5, 1, 3, 3, None, 5, 3, 4, 3, 4, None, 4, 1, None, 5, 5, 3, 1, None, 4, 5, 5, 3, 3, 5, 5, 5, 4, 5, 1, None, 5, None, 5, 1, 5, 5, 5, 5, 3, 5, 3, 4, 1, 1, None, 5, 3, 5, 5, 3, 1, 5, 4, None, 1, 5, 5, None, 1, 4, 1, 1, 3, 1, 1, 3, 3, 3, 5, 1, 5, 5, 5, None, 5, 5, 1, 4, None, None, 5, 3, 5, None, 5, 1, None, 5, 1, 3, 5, 3, 3, 5, 5, 3, 5, 2, 5, 4, None, 5, 5, 2, 5, 5, None, 5, 1, 1, 5, 1, 1, None, 4, 5, 5, 5, 3, 5, None, 3, 3, 5, 5, 5, None, 1, None, 5, None, 5, 5, 2, 5, 1, 2, 5, None, None, 5, 5, 5, 2, 4, 5, 4, 3, 1, None, 3, 1, 5, None, 5, 3, 1, 3, 5, 5, 5, 4, 5, 3, 2, 1, 1, 5, 3, 5, 3, 5, 3, 1, 5, 5, 3, None, 5, 5, 4, None, 5, 5, 4, 5, 4, 1, 5, 3, 5, 5, 1, 3, 5, 5, 2, None, 5, 3, 5, 5, 5, 5, 1, None, None, None, 5, 2, 3, 3, 5, 1, None, None, None, None, 5, 5, 3, 2, 3, 5, None, 5, 5, None, 4, 5, 5, 1, 3, None, 5, 3, None, 3, 5, 5, 5, 5, 3, 5, 4, 5, None, 4, None, 2, 3, 5, 5, 5, 5, 2, None, 5, 5, 3, 5, 4, 4, 3, 5, 5, 3, None, 5, 2, 1, 3, 5, 5, 5, 3, 3, 5, None, 5, 5, 4, 3, 1, 4, 5, 3, None, 5, 3, 5, 5, 5, 5, 5, None, 1, 5, 3, None, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 3, 5, None, None, 5, 1, 5, 1, 5, 4, 3, 3, 1, 5, 2, 1, None, 4, 4, 5, 5, 5, 1, 5, None, 3, None, 4, 5, 5, 5, 1, 4, 5, 5, 1, 3, 4, 4, 2, 2, 5, 3, 4, None, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 3, 3, 4, 5, 1, 5, 5, 4, None, 5, 5, 4, 3, 1, 2, 4, 4, 3, 5, 3, 5, 1, 3, 5, 5, 5, 5, None, 3, None, 5, 4, 5, 3, 3, 5, 3, 5, 4, None, 3, None, 3, 1, 3, 1, 1, 3, 1, 1, None, 5, 4, 5, None, 4, 5, 5, None, 1, 5, None, 5, 3, 1, 4, None, 1, 5, 5, 3, None, None, 5, None, 5, 4, 4, 3, 5, 1, 5, 5, 4, 3, 5, 1, 5, 5, 4, 5, 5, 3, 5, 5, None, 1, None, 5, 3, 5, None, 5, 3, 1, 4, 5, 5, 1, None, 5, 5, 3, 1, 5, 5, 5, 5, 5, 1, None, 5, 4, 4, 3, 1, None, 1, 3, 3, 5, 5, 5, 5, None, 1, 5, 4, 5, 5, 5, 4, None, None, 5, 5, 3, 5, 5, 5, 3, None, 4, 5, 3, 5, 4, 5, 5, 1, None, None, 4, 3, 5, None, 4, 3, 1, 4, None, 1, 3, None, None, 5, 3, 3, 3, None, 2, 5, None, None, 5, 1, 5, 5, 4, 5, 4, 5, 5, 1, 5, 5, None, 1, None, None, 5, 1, 5, None, 3, 5, 5, 5, 3, 4, 3, 2, 5, 4, 3, 2, None, 5, None, 5, 3, 5, None, 5, 5, 5, 3, 5, 3, 3, 2, 5, 1, 1, None, 5, 5, 4, 5, 5, 5, 5, 3, None, 4, 5, None, 4, 2, None, None, 5, 5, 1, 5, None, 5, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, None, 3, 1, 3, 5, 5, None, 3, 5, 2, 4, 4, None, 3, 5, 5, 4, 5, 5, 5, 5, None, 5, 1, 5, 5, None, None, 5, 2, 3, 5, 3, 5, 5, 1, 3, 5, 5, 1, 3, 3, 5, 1, None, 5, 4, 2, 1, 5, None, 3, 5, 5, None, 3, None, None, 3, 3, 5, None, 4, 5, 1, 5, 5, 5, 1, 4, None, 5, 5, 5, 4, 5, 1, 3, None, None, None, 3, 5, 3, 3, 4, 4, 3, 4, None, 5, 5, 2, None, 3, 1, 2, 5, 5, 5, 5, 1, None, 5, 5, 5, None, None, 1, 3, 2, 3, 5, 5, 5, 5, 5, 3, None, 3, 5, 2, 2, 1, 5, 5, 5, None, 3, 5, 2, 5, 4, 4, 1, None, 4, None, 5, 5, 5, 4, 1, 5, 5, None, 3, 5, None, 5, 3, 1, None, 3, None, 5, None, 5, 3, 4, 5, 3, 5, None, 3, 5, None, 5, 2, 3, 2, 5, 5, 5, 5, 1, 5, 1, 4, 4, 1, 2, 3, None, 5, 1, 5, 5, 5, 5, 3, 5, 5, 5, 5, 4, 5, None, 1, 5, None, 3, 1, 5, 3, 5, 1, 5, 5, 1, 3, None, 5, 3, None, 3, 5, 4, 5, 3, 5, 1, 2, 4, 3, 1, 3, 3, 5, 4, 5, 3, None, 3, 5, 5, 5, 5, 5, 5, 5, 2, None, 4, 4, None, 5, 5, 3, 3, 1, 5, None, None, 5, None, 5, 4, 5, None, 1, 5, None, 3, 5, 3, 5, 1, 5, 5, 1, 1, 1, 2, None, 5, 4, 5, None, 5, 3, 5, 5, 5, 5, None, 4, None, None, None, 4, 5, 3, 5, None, 5, 5, 5, 5, 1, 4, 5, 5, 4, 5, 4, 5, 1, 3, 1, 5, 5, None, 5, 5, 2, 5, 4, 1, 3, 5, 5, 1, 5, 3, 2, 1, None, 5, 5, None, 5, 4, 5, 3, 5, 3, 5, 3, 4, 5, 4, 5, 5, 5, 3, 5, 5, 3, 5, 5, None, None, 5, 2, 5, 1, 1, None, 5, 5, 3, 5, 5, 3, None, 5, None, 5, 5, 1, 3, 3, 5, 3, 3, 5, 5, 5, 3, 5, 3, 1, 3, 1, 5, 5, 3, 5, None, 5, 5, 5, 5, 3, 3, 3, 4, 5, 4, 5, 3, 5, 5, 5, 5, 4, 4, 5, None, 5, 5, 1, 5, 2, 5, None, 4, None, 1, 4, 1, 4, 5, 5, None, 3, None, 1, 4, 2, 1, None, None, 5, 5, None, None, 4, 3, 5, 5, 3, None, 4, 5, None, 5, None, None, 1, 1, 5, 5, 5, None, 5, 5, 5, 5, 3, None, 1, 5, 5, None, 3, 1, 4, 5, 5, 5, None, 3, 1, 3, 4, 5, 1, 2, 5, 5, None, 3, 4, 5, None, 5, 1, 5, 5, 5, 5, 5, 1, 3, 1, None, 4, 1, 3, None, 5, None, 1, 5, 2, None, 5, 5, 1, 4, 5, 1, 3, 5, 5, 3, 3, 1, 5, 5, 3, 1, 1, 3, 5, 5, 5, 1, 2, 2, 3, 4, 1, 5, 5, 5, 5, None, None, 3, 5, None, 5, 4, 5, 1, 5, 5, 3, 2, 2, None, 4, 1, 5, None, None, 5, 5, 3, None, 3, 5, 3, 5, 5, 3, 2, None, 1, 4, None, 1, 4, 5, 5, 3, 5, None, 5, 5, 5, 5, 5, 5, 5, 3, 2, 5, None, 5, 4, 4, 3, 4, 2, 5, 3, 5, 3, 5, 3, 4, 5, 3, 5, 3, 5, 1, 5, 5, 5, None, 5, 5, 5, 5, 4, 3, 3, 5, None, 2, 5, None, 5, 5, 5, 5, None, 2, None, 2, 5, 5, 3, 4, 5, 4, 5, None, 4, 5, 5, None, None, 5, 3, 1, 3, 5, 5, 5, 5, 3, 5, 3, None, 4, 1, None, 5, 1, 5, 5, 1, 5, 5, 5, None, 5, 1, 3, 3, 1, 4, 5, 3, 3, 3, 5, 1, 1, 3, 5, 5, 3, 5, 5, 5, 5, 5, 3, None, 2, None, None, 5, 1, 3, None, 5, 5, 1, 1, 4, 5, 5, 4, 5, 1, None, 2, 3, 1, 3, 3, 5, 1, 1, 1, None, 5, 1, 3, 3, 5, 4, 3, 5, 5, 1, 5, 5, 3, 5, 5, 3, 5, 1, 3, 5, 1, None, 1, 5, 5, 5, 5, 5, 5, 4, 2, 2, 5, None, 3, 3, 3, 1, 2, 5, None, None, None, 5, 5, 5, 4, None, 5, 1, 1, 5, 2, 1, 5, 3, 4, 3, 2, 4, 5, 3, None, 3, 3, 5, 3, 5, None, 4, None, 1, 5, 3, 4, 4, 5, 5, 4, 5, None, 5, 3, 3, 4, 4, 3, None, 5, 5, None, None, 5, 1, 5, 3, 5, 1, 5, 3, 5, 4, 4, 5, 1, 5, None, None, 3, 5, 3, 1, 4, 5, 1, None, 5, 5, 3, None, 4, None, 5, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, None, 2, 5, 3, 3, 1, 2, 5, 5, 5, 5, None, 5, 3, 4, None, 5, None, 5, 4, 4, 4, 3, 5, 1, 4, 5, 5, 1, None, None, 5, 4, 5, 4, 1, 4, 5, 5, 5, None, 5, None, 5, 5, 5, 4, 5, None, 5, 1, 5, 1, 4, 2, 5, 5, 1, 5, 5, 5, 5, None, None, None, 4, 5, 5, 5, None, 5, 5, None, None, 5, 5, 5, 5, 5, 4, 5, None, 3, None, 5, 3, 4, 3, 5, 5, 3, 3, 5, 5, 5, 5, 5, 5, 5, None, 5, 1, 3, 5, None, 5, 5, None, 5, 4, 3, None, 5, None, 1, 5, 2, 3, 4, None, 2, 5, 5, None, 5, 3, 5, None, 2, 4, None, 5, 5, None, 4, 1, 5, None, 5, 5, None, 1, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, None, 5, 5, 3, None, None, 3, 5, 5, 1, 5, 5, 3, 4, 4, 5, 1, 5, 3, 5, None, None, 3, 5, 5, 4, 5, 5, 5, None, None, 5, 5, 5, 5, 5, 4, 5, 3, 1, None, 5, 3, 2, 5, 2, 5, 5, 1, 5, 1, 1, 5, 5, 5, 5, None, 5, 1, 1, 5, 2, None, 4, 5, 2, 1, 1, 5, 4, 5, 4, 5, 5, 3, 3, 5, 3, 1, 5, 5, 3, 5, 5, 1, 5, 5, 3, 5, 5, 4, 5, 5, 3, 5, 5, 1, 5, 5, 5, 1, 5, 3, 5, 1, 5, 5, None, 1, 4, 5, 4, None, 5, 5, 5, 5, 3, 5, 3, 1, 4, 5, 5, 3, 3, 5, 5, 1, 3, 3, 5, 5, 5, 5, None, 5, 4, 4, 5, 5, 3, 5, 1, 3, 1, 5, 5, 1, 5, 5, 3, 5, 5, 2, 5, 3, 3, 5, 5, 4, 5, 3, 5, None, 4, 5, 5, 5, 1, None, 5, None, 1, 4, 4, 5, 5, 1, 4, 3, 5, 5, 1, 3, 5, None, 5, None, 1, 3, 3, 5, 5, 5, 5, 3, 5, None, 5, 1, None, 4, None, 5, 5, 3, 5, 1, None, 1, 3, 3, 3, 5, 1, 4, 5, 5, 1, 4, 5, 4, None, 5, 5, None, 3, 5, None, None, None, 3, 3, None, None, 4, 4, 4, 5, 5, None, 5, 5, 2, 3, 3, 5, 4, 5, 2, 4, 5, 5, 5, 5, 5, 5, 5, 3, 3, None, 2, None, 5, None, 3, None, 4, 2, 5, 3, 4, 3, None, 4, None, 5, 5, 5, 5, 5, 5, 1, None, 5, None, 5, 5, 4, None, 1, 5, 5, 3, 5, 1, 5, None, 5, 1, 5, 3, 3, None, 5, 5, 4, 5, None, None, 4, 5, 1, 5, 5, 5, 4, 2, 5, 5, 5, 5, 5, 3, 5, 5, 1, 1, 3, 5, 4, 4, 5, 3, 5, None, None, 5, 4, 3, 5, 5, 2, 5, 5, None, None, 5, 3, 5, 5, 5, 1, 5, 5, 4, 5, 5, None, 5, 1, 5, 5, 5, 5, 1, 1, None, 5, None, 4, None, 5, 5, 5, None, None, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 3, 5, 4, 1, 3, 5, 3, 5, 5, 5, 5, 3, 5, 5, 5, 5, 4, 1, 4, None, 1, 5, 4, None, None, 3, 1, 5, 4, 5, 5, 3, 1, None, 1, 5, None, 5, 1, 5, None, None, 5, None, 5, 3, 3, 3, None, 3, 1, 3, 5, 5, 5, 5, 4, 5, 5, 5, 5, 3, 3, None, 3, 5, None, 1, 3, 3, 3, 5, None, 5, 5, 1, 2, 3, None, None, None, 5, 2, 5, 5, 5, None, 5, 5, 2, 1, 5, 4, None, 3, None, 5, 1, 5, 5, None, 4, None, 5, None, 2, 1, 3, 2, 3, 3, 3, 5, 1, 1, 5, 5, None, 5, 5, None, None, 4, 4, 4, 5, None, 5, None, 1, 5, 5, None, 5, 5, 1, 5, 2, 2, 3, 4, 5, 4, None, 5, 5, 3, None, 5, 3, 3, 5, None, 5, 3, 1, 3, 5, 5, None, 5, None, 4, 5, 5, None, None, 5, 3, 5, 4, 1, 5, 5, 1, 5, 5, 4, 1, 3, 5, 3, None, 3, 5, 4, 5, 5, 1, 5, 5, 3, 3, 5, 5, None, None, 4, None, None, 5, 5, 5, 3, 5, 3, 3, 2, 1, 3, None, 5, 5, 5, 5, None, 1, 5, None, 1, 5, 3, 5, None, 3, 5, 4, None, 2, 5, 5, 5, 3, 5, None, 5, 5, 2, None, 5, 2, 1, 4, 5, 4, 1, None, 5, None, 4, 1, 3, None, 5, 2, 3, 5, None, 5, 3, 5, 5, 3, 5, 5, None, 5, 4, 5, 3, 5, 4, 5, 3, 5, 5, 3, 5, 1, 1, 5, 3, 5, 1, 4, 5, None, 3, 5, None, 5, 2, 5, None, 1, None, 5, 5, None, None, 5, 3, 1, 1, 3, 5, 5, 3, 5, 3, 5, None, None, 5, 3, None, 4, 3, 3, 4, None, 5, None, 5, None, None, 3, 5, 1, 1, 5, 4, 5, None, 5, None, 3, 5, 3, 5, None, 4, 4, 1, 5, 5, 5, 5, 5, None, 1, 1, 4, 5, None, 5, 1, None, 5, None, 1, 1, None, 2, 3, 5, None, 3, 5, 5, 5, 3, 1, 4, 5, 3, 3, 5, 3, 5, 5, 3, 4, 2, None, None, 5, 5, 3, 4, None, 3, 1, 1, 3, None, 3, 5, 5, 3, 5, 5, 4, 3, 4, 1, 1, None, 3, 5, None, 3, 1, 5, None, 3, 3, 5, 1, 5, None, 5, None, 3, None, 1, 5, 5, 3, None, 5, 5, 5, 5, 1, 4, 5, 5, 5, 5, 5, 3, None, None, 5, 1, 4, None, 4, 5, 4, 5, 5, 4, 2, None, 2, None, 5, 5, 5, 5, 5, 5, 5, 3, 1, 5, 5, None, 5, 3, 5, 3, 5, None, 3, 3, 5, 5, 1, 5, 4, 3, 5, 2, 5, 4, 1, 1, 5, 1, 5, 5, None, 1, 5, 4, 5, 5, None, 4, 5, 5, 2, 5, 3, 5, 5, None, 5, 1, 5, 5, 1, 3, 3, 5, 5, 5, 5, 5, 5, None, 5, 5, 3, 1, 5, 5, 5, 1, None, None, 3, 3, None, 5, 5, 3, 4, 5, None, 5, None, 5, 3, 3, 3, None, 5, 3, 4, 5, 5, 1, 1, 5, 5, None, 4, 4, 5, 3, 2, 5, 3, 5, 5, None, 1, 5, 5, 4, 1, None, None, 5, None, 5, 3, 5, None, 5, 5, None, 1, 5, 5, 1, None, 2, 5, 1, 5, 5, 5, 5, None, 3, 5, 5, 5, 3, None, 3, None, 5, 5, 5, 3, 3, 5, 4, None, 5, 5, None, 5, 5, None, None, 1, 4, 3, 5, None, 3, 5, 4, 3, 3, 5, 3, 4, 5, 5, None, 5, 4, 5, None, 3, None, 5, 2, 5, 5, 5, 2, 5, 5, 5, 5, 3, 3, 5, 2, 3, 2, None, 1, 1, 5, 5, 5, None, 5, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 1, 4, 5, 5, 5, 4, 5, 2, 4, 5, 5, 5, 5, None, 5, None, 5, 5, 5, 5, 3, 5, 3, 1, None, 5, 5, 3, 5, 5, 3, 5, 3, 4, 5, 3, 4, 3, 4, None, 1, 4, 5, 5, 4, None, 5, 3, 5, 5, None, 4, 4, 5, 5, 5, 5, None, 5, 4, 5, 5, 1, 1, None, 5, 1, 1, 5, 5, None, 4, 5, 3, 4, None, 3, 5, None, 5, 5, 5, 5, 5, None, 5, 5, 5, 3, 1, 4, None, 2, 1, 5, 5, 1, 4, None, None, 5, 5, 1, None, 4, 5, 5, None, 3, 4, 4, 1, 3, 4, 3, 4, 2, 5, 5, 3, 4, 5, 2, None, None, 5, 5, 5, 5, 3, 4, 5, 5, None, 3, 4, 4, 4, 4, 5, 3, 5, 4, 5, 3, 5, 4, None, 4, 5, 3, None, 5, 5, 3, 5, 1, 5, 4, 1, 5, 5, 5, 5, 4, 5, 5, 3, None, 2, 4, 4, 4, 5, 3, 5, None, 4, 4, 4, 1, 4, None, 5, 3, 3, None, 5, 5, None, 3, 5, 3, 1, 3, 5, 5, 1, 5, 3, 4, 1, None, 1, 5, 3, 5, 3, None, 2, 5, 5, None, None, None, 1, 1, 5, None, 2, 4, None, 5, 3, 4, 5, None, 4, 4, 5, 5, 5, 5, 4, None, 5, 1, None, 5, None, 3, 3, 3, 3, 3, None, 5, 3, 4, 3, None, 1, 3, None, 5, None, 5, None, 5, 1, 5, 5, 5, 3, 4, 3, 1, 4, 3, 5, 1, 5, 3, 1, 5, 5, 4, None, 3, 3, 2, None, 3, 5, 3, 5, 5, 5, 5, None, 2, 3, 1, 5, 5, None, 5, 5, 4, 1, 4, 1, 4, 5, 3, 5, 5, 4, 5, None, 1, 5, None, 4, 5, 3, 1, 3, 5, 5, 1, None, 5, None, 5, 1, 4, 1, None, 4, 1, 3, 4, 3, 5, 5, 1, 1, 5, 5, 5, 3, 1, 5, None, 5, 5, 5, 5, 5, 2, 3, None, 5, 5, 3, None, 4, 5, 4, 1, 5, 5, 5, 5, 2, 5, 5, 3, 5, 4, 3, 5, 5, 5, 5, 2, 5, 4, None, None, 5, 4, 5, 3, 2, 5, 5, 5, 5, 5, 5, 1, None, 5, None, 5, 5, 3, None, None, 1, 3, 5, 5, 3, 5, 2, 5, 5, None, 5, 1, 1, 5, 5, 3, 1, None, 4, None, 4, 4, 1, 5, 5, 5, 1, None, 5, 5, 5, 5, 5, None, 1, 1, 4, 5, 5, 3, 1, 5, None, None, None, None, 4, 5, 5, 1, 1, 5, 5, 3, 5, 3, 5, 5, 1, 3, 3, 1, 5, None, 3, 5, 3, 3, 5, 5, 3, None, 1, 4, 5, 1, 5, 3, 5, 5, 3, None, 5, 4, 1, 3, None, 5, None, 5, 5, 4, 5, 5, 5, 5, 3, 3, 5, 3, 4, 3, 4, 3, 5, 5, 5, 5, 5, 3, 5, 4, 1, 5, 5, 5, 5, 3, 5, 5, None, 5, 5, 5, 3, 5, 5, 1, 5, 5, 3, 1, 5, 3, 4, 2, 5, 5, 5, 1, None, 1, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, None, None, 5, 3, None, 5, 5, None, None, 5, None, 3, None, 3, 5, 5, 4, 3, 3, 3, 2, 5, 3, 5, 5, None, 5, 5, None, 5, 5, 1, None, 3, 4, None, 5, 5, 5, 5, 4, 5, 5, 5, 2, 5, 5, 5, 1, 1, 5, None, None, 5, 5, 5, None, 2, 5, 3, 5, 4, 5, 1, None, 5, 5, 4, 4, None, 5, 4, 3, 3, 3, 1, 1, 3, 2, 3, None, 5, 4, 5, 5, 5, 5, 5, 5, 1, 2, 4, 5, 3, 1, 5, None, 3, 5, 3, 5, 3, None, 2, 5, 2, 4, 5, None, None, 5, 3, 3, None, 5, 5, 4, 5, 3, 3, None, 3, None, 1, 5, None, 5, 1, 3, 5, 3, 5, None, 5, 5, 4, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 1, 4, 5, 2, 1, 1, 5, 1, None, 5, 4, 1, 5, 5, 1, 1, 3, 5, 5, 1, 3, 5, 5, 1, 5, 3, 5, None, None, 3, 1, 5, 1, 5, 4, 4, None, 5, None, 5, 3, 3, 5, None, 3, 3, 4, 5, 2, 5, 1, 5, 3, 5, 3, 4, 1, 5, 4, None, 5, None, 5, 5, 1, 5, None, 1, 5, 5, 3, 4, 5, 5, 3, None, 5, 4, None, 5, 1, 3, 2, 5, 5, 1, None, 5, 3, 5, 5, None, 2, 5, 5, 5, 5, 5, 3, 1, 1, 5, 5, 5, 3, None, 5, 4, 5, 4, 1, 5, 5, 5, None, 5, 5, 5, 5, 4, 4, None, 3, 4, 3, 1, 4, 5, 5, 4, 5, 1, 5, 1, 5, None, 5, 1, 3, 1, 5, 2, 1, None, 5, 3, 5, 1, 5, 3, None, 5, 5, 3, None, 2, 5, 4, 1, 5, 3, None, 5, 5, 5, 5, 3, 3, 4, 3, 5, 2, 3, 3, 5, 1, None, 5, 5, 5, 5, 4, 3, 2, 3, 1, 4, 1, 5, None, 1, 5, 5, 4, 3, None, 3, 3, None, None, 5, 5, 3, 4, 1, 4, 5, 5, 4, None, 5, None, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 1, None, 5, 5, None, 5, 5, 4, None, None, 3, 1, 1, 1, 5, 5, 5, 4, 4, 5, 5, 2, 5, None, 4, None, 3, 2, 5, 5, 5, 3, 5, 5, 5, 5, 3, 3, 3, 5, 5, 5, None, 3, 2, 4, 5, 5, 3, 1, 3, None, 5, 2, 5, 5, 1, 3, 1, None, 1, 1, 5, 4, 5, 5, 5, None, 5, 5, 4, 1, 5, None, 3, 3, 5, 1, 5, 4, None, 4, 4, 5, 1, 5, 3, 3, 5, 1, 4, 5, 2, 2, None, 5, 5, 1, 3, 2, 5, 5, 5, 4, 5, 4, 5, 3, 5, None, 5, 5, 5, 1, 3, 5, 5, None, None, 5, 2, 3, 5, 3, 2, 4, 4, None, 5, 5, 4, 5, 5, 3, 1, None, None, 5, 5, 4, 5, 1, 5, None, 5, 1, 3, None, None, 3, 3, 3, 1, 5, 3, 5, 5, None, 4, 5, None, 5, 1, None, 5, 2, 3, 5, 4, 5, 5, 5, 1, 5, 4, None, 5, 5, 1, 5, 5, 1, 5, 4, 5, 2, 4, 3, 3, None, None, 1, 5, 2, 5, None, 4, 3, 5, 3, 4, 3, 5, 2, 5, 5, 5, 1, 2, 5, 5, 5, 1, 5, 2, 4, 5, 5, 5, 3, 2, 5, 5, None, 5, 5, 5, 5, 5, 5, None, 5, None, 5, None, 4, 1, None, 5, 1, None, 2, 3, 1, None, 5, 5, 3, 1, 5, 5, 1, 1, 5, 4, None, 3, 5, 5, 5, 5, 5, 4, None, 5, 2, 5, 3, 5, 3, 5, None, 3, 1, 5, None, 4, 3, 5, 1, 4, 2, 4, 1, 3, 5, 4, 5, 3, 1, 5, 5, 1, 1, 5, 1, 5, None, None, None, 5, 3, 3, 2, 3, 5, 5, 1, None, 5, 4, 5, 5, None, 5, 5, 5, 1, 5, 5, None, 5, 1, 3, 5, None, 5, None, 5, 4, 1, 1, 5, None, 3, 4, 1, 4, 5, 3, 2, 3, 1, 3, 1, 3, 5, None, 5, 5, 1, 5, 5, 5, 5, 5, 5, 4, 1, 5, 5, 2, None, None, 1, None, 1, 5, 4, 1, None, 5, 5, 5, 5, 5, 5, 5, None, 1, 3, 1, 1, 2, 5, 5, 5, 5, 5, 5, 4, 5, 5, None, None, 2, 5, 1, 5, 5, 3, 5, 1, 1, 3, 5, 5, None, 5, 4, 5, 5, 4, 4, 3, 3, 5, 5, 5, None, 1, 4, 5, 1, 5, 3, None, 2, 1, None, 3, 4, None, 5, None, None, 3, 3, 5, 5, 3, 5, 4, None, 5, 1, 5, 1, 4, 4, 5, 5, 5, 4, 1, 5, 5, 5, 4, 2, 2, 4, 5, 3, None, 5, None, 1, 5, 5, 5, 5, None, 5, 5, 5, 2, 5, 5, 5, None, 5, None, 4, 5, 5, 5, 5, 4, None, 5, 5, 2, 5, None, 5, 3, 5, 5, None, 5, 5, None, 5, None, 4, 5, 5, 2, 5, 3, 5, 5, 3, 5, 1, 5, 5, None, 1, 5, 5, 3, None, 5, 4, None, 5, None, 4, 5, None, 5, 2, 5, 5, None, 5, 5, 1, 5, 1, 3, 5, None, 2, 5, 3, 1, 5, 3, 5, 5, 5, 1, 4, None, 5, 5, None, 4, 5, 5, None, 1, 5, 5, 3, 5, None, 5, 3, None, None, 1, 5, 3, 2, 1, 3, 1, 5, 3, 3, 5, 1, 5, None, 3, None, 5, None, 5, 5, 3, 1, 4, 3, None, None, 4, 5, 3, 5, 5, 1, 5, 1, 5, 5, 1, None, 3, None, 2, 5, 5, 5, 5, 5, 5, 3, 5, 5, 2, 5, 5, 1, 5, 3, 1, 3, 4, None, 4, 3, 5, 4, 5, 5, None, 5, 5, 4, 5, 3, 5, None, 5, 5, 5, None, 5, 5, 5, 5, 1, 5, None, 2, None, None, 4, 3, 5, 5, 5, None, 3, 3, 1, 5, 4, 1, 5, 4, None, 4, None, 5, 5, 3, None, 5, 2, None, None, 2, 2, 4, 5, 3, 3, 2, 1, None, 3, 5, 5, 5, 5, 4, 5, 2, 5, 1, 4, 3, 1, 5, None, 5, 5, None, 5, 5, 5, 5, 5, 5, 5, 4, 1, 5, 3, 5, 1, 4, 3, 1, 5, 4, None, 5, 5, 5, 5, 5, 4, 5, 1, 5, 5, 4, 5, 3, 1, 5, 5, 1, 5, 1, 4, 3, 5, 1, 3, 4, None, None, 5, 5, 5, 5, 1, 1, 5, 5, 5, 1, 5, 5, 4, 5, None, 5, 5, 5, 1, 1, 5, 3, 5, 3, 4, 5, None, 5, 5, 5, 4, None, 5, 5, 5, 4, 5, 3, 3, 5, 5, 1, None, 3, 5, 2, 5, 5, 3, 5, 2, None, 5, 3, None, 5, 5, 5, 5, 3, None, 1, 1, 5, 5, 3, 1, 5, 5, 3, None, 5, 1, 5, 1, 5, 4, 4, 5, 5, 4, 4, 5, None, 1, 5, 5, None, 4, 3, None, 5, None, 5, 2, 3, 1, 3, None, 4, 5, 5, 5, 5, 3, 5, 3, 5, 3, 4, 4, None, None, 5, 5, 5, 5, 5, 4, 5, 4, 5, 1, 1, 1, 5, 4, 5, 2, 5, None, 5, 3, 1, 5, 5, 3, 4, 5, 3, 5, 1, 1, 5, 5, 3, 5, 1, 5, 4, 1, 3, 3, 5, None, 5, None, 5, 5, 5, None, 4, 5, None, 1, 5, 5, None, 5, 3, 5, 5, 1, 4, 5, 5, 3, 5, None, 1, 1, 3, 5, 5, None, 3, 3, 5, 5, 3, 5, 4, None, 5, 1, 5, 5, None, 5, 1, 5, 5, 4, 5, None, 5, 4, 5, None, 5, 1, None, 5, None, 5, 5, 4, None, 4, 3, 1, None, 5, 5, 5, 5, 5, 5, 3, 5, 4, 4, 5, 5, 5, 3, 3, 3, None, 4, 5, 5, 5, 5, None, None, 5, 1, 5, 1, 4, 5, None, 5, 5, 5, 2, 1, 3, 5, 5, 3, 5, 4, 4, 2, None, 1, 5, 3, 3, 5, None, 1, 2, 3, 4, 3, 4, 5, 2, 5, 5, 5, None, None, 1, 5, 5, 5, 5, 5, 5, 4, 1, 5, 2, 5, 5, 2, 3, 3, 5, 5, 5, 5, 1, 5, None, 5, 4, 5, 3, None, 1, 5, 5, None, 5, 5, 5, 4, 5, 5, 3, 5, 5, 4, None, 5, 5, 5, 5, 5, None, 3, 5, 5, 5, 5, 3, 1, 1, 1, 5, 1, 4, 3, 4, 5, 2, 4, 1, 1, 5, 5, 5, 5, None, 3, 5, 2, 5, 4, 5, 5, 2, 3, 5, 5, 3, None, 5, 4, 1, 5, 5, 5, 5, 1, 3, 5, 4, None, 3, None, 5, None, 1, 1, None, 1, 5, 5, None, 5, 3, 5, 5, 3, 5, None, 3, None, None, 5, 5, 3, 5, 1, None, 5, 1, 5, 5, 5, 5, 5, 4, 5, 5, 5, None, 5, 5, 4, 5, 5, 3, 5, 5, 5, 1, 3, 1, 5, 1, 5, 1, 5, 3, 3, 1, 5, 4, None, 1, 5, 5, 5, 5, 4, 5, 4, 5, 4, None, 5, 5, 5, 3, 5, 2, 5, 5, 4, 4, 5, 3, None, 1, 5, 5, None, 3, 1, 3, 5, 3, 5, 5, 1, 5, 4, 5, 1, 4, 2, 1, None, 4, None, 5, None, 5, 5, 3, 5, 2, 3, 5, 3, 3, 3, None, 5, 5, 4, 5, 4, None, 3, 3, 5, 3, 5, 5, 5, 2, 3, 4, 1, 1, 5, 5, None, 5, 5, 4, None, 5, 5, 5, 5, None, None, 5, 5, 4, 2, None, None, 3, 5, 5, 1, 5, 3, 1, 1, 5, 1, None, 5, 5, 5, 5, 3, 3, 2, 3, 5, 1, None, 5, 4, 5, 5, 3, None, 5, 5, None, 5, 5, 5, 3, 1, 5, None, 3, 5, 4, 5, None, None, 1, None, 3, 3, 5, 5, 1, 5, 5, 1, 1, 3, 5, 5, 4, 4, 5, 5, None, 4, 5, 4, 3, 4, 1, 3, 5, 1, 5, 5, None, 1, 5, 5, 1, 5, 3, 1, None, 5, 5, None, None, 1, 3, 5, 4, 5, 4, 5, 5, 1, 5, 4, 2, 3, None, 1, None, 5, 3, 2, 5, 3, 4, None, 3, 1, 1, 5, None, 1, 5, 4, 3, 3, 5, None, 5, 3, 3, 5, None, 1, 4, 3, 3, 5, 4, 5, 4, 2, 5, 5, 4, None, 5, 5, 4, 3, None, None, 5, 4, 4, 4, 2, 4, 5, 4, 5, 5, None, 3, 5, 5, None, 5, 5, 5, 5, 5, None, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, None, 5, None, 1, None, None, 3, 3, 1, 5, 5, 5, 5, None, 5, 1, 5, 5, 4, None, 3, None, 2, 5, 5, 5, 4, 5, 1, 3, 3, 5, 5, None, 3, 5, 5, 4, 5, None, None, 2, None, 4, None, 5, None, None, 3, 3, 5, 4, 4, 4, 3, None, 5, 5, 5, 5, None, 4, 1, 1, None, 5, 3, 4, 3, 5, 4, 1, 5, 3, None, 1, 4, 3, 5, 5, 3, 3, 3, 3, 5, None, 4, 3, 5, None, 5, 5, None, 5, 5, 5, 5, 3, 1, 3, None, None, 4, None, 5, 2, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 4, 1, 5, 5, 5, 5, 1, 1, 3, 3, 4, 3, 5, 3, 5, 4, 1, 5, 5, 5, 5, 5, 1, 5, 5, 5, 3, 3, 5, 1, 3, 4, 5, 5, 5, None, 1, 1, 3, 2, 5, 5, 1, 1, 1, 5, 5, 5, 2, None, 4, None, 1, 5, 5, 5, 1, 5, 4, 5, None, 5, 5, 5, None, 5, 5, None, 2, 3, 5, 3, 1, 5, 5, None, 3, 5, 5, 5, 5, 5, 3, 4, 1, None, 3, 1, 5, 4, 5, None, 4, 5, 5, None, 4, 5, 3, 3, 5, 5, 5, 5, 3, None, 1, 2, 5, 5, 5, 5, 5, 3, 5, 3, 5, 5, None, 5, None, 5, 1, 5, 5, 5, 5, None, 5, 5, 5, 5, 1, None, 5, 1, 5, 3, 3, 1, 3, 4, 5, 3, 5, 5, 4, 5, 4, 3, 5, 4, 3, 4, None, 5, 5, 2, 5, None, 3, 5, 5, 5, None, 1, 2, 3, 5, 3, None, 4, 2, 5, 5, None, 3, 5, 3, 1, 4, 4, 5, 5, 3, 5, 1, None, 1, 5, 1, 5, 5, None, 1, 3, 5, 5, 5, 4, 3, 5, 1, 2, 5, 1, None, 1, 5, 5, 1, 5, 4, 3, 5, 4, 1, 4, 5, 5, None, 5, 5, 5, 2, None, 5, 3, 5, 4, 1, 5, 5, None, 4, 5, 5, None, 4, 1, 3, 5, 5, 5, 5, 1, 4, 1, 5, 3, 3, None, 5, 5, 3, 1, 5, None, 5, 5, 1, None, 1, 5, 1, 5, None, None, 4, 2, 5, 5, None, 5, 3, 4, 4, 5, 5, 5, 5, 5, 2, 3, 5, None, 2, 1, 3, 5, 5, 1, 3, 1, None, 5, 5, 5, 5, None, 5, 5, 4, 5, 2, None, 5, 3, 5, 3, 5, 1, 5, 5, 5, 5, 3, 1, None, None, 5, 3, 4, 1, 3, 3, 5, 5, 4, 4, 5, None, 1, 5, 3, 5, 5, 5, 1, 5, 5, 3, 5, 5, None, 3, 5, 5, 1, 1, None, 5, 1, 1, 3, 2, 5, None, 5, 4, 5, 3, 3, None, None, 5, None, 3, 2, 5, 1, 4, None, 5, 5, 5, 5, None, 4, 5, 5, 5, 5, 1, 3, 5, 5, 3, None, 5, 4, 5, 1, 5, 1, 3, 1, None, None, 1, 3, 3, 5, 3, 5, 3, 3, 3, 5, 5, 4, 5, 5, 3, None, 3, 5, 5, 5, 3, 2, 1, 5, 3, 3, 2, None, 3, 5, 5, 5, 1, 1, 5, 3, 4, 2, 5, 5, 5, 1, 5, 2, 5, 5, 1, 3, 3, 3, 5, 3, 5, None, 5, 3, 5, 5, 5, 3, 5, 1, None, 5, 5, 1, 4, 5, 1, 4, 5, 5, 4, 3, 5, 5, 3, 2, 4, 5, 5, 3, 5, None, 4, 2, None, 1, 5, 3, 3, None, 5, 2, 1, 5, 5, 1, 3, 3, 5, 5, 5, 1, 4, 2, 5, 5, None, None, 3, 2, 5, None, 1, 1, 5, 5, None, 5, None, 3, 5, 1, 4, 3, 5, 3, 3, 5, 3, 5, 5, 1, None, None, 1, None, None, None, 5, 5, 3, 5, 3, 3, 3, 5, 2, 5, 5, 4, 4, 5, None, 5, 4, 5, 1, 3, 5, 3, 5, 2, 5, 1, 1, 5, 5, 3, 5, None, None, 1, 5, 5, None, 1, 5, 5, None, 5, 3, 3, 4, 5, 3, 5, None, 2, 5, 4, None, 4, 4, 5, 5, None, 1, None, 1, 3, 4, 5, 5, 4, 1, None, 5, None, None, None, 5, None, None, None, None, 5, 3, 2, 5, 5, 5, None, 5, None, 5, 4, 5, 5, None, None, 5, 5, 3, 1, 5, 1, 5, None, None, None, 4, None, 5, 4, 1, 5, None, 5, 1, 5, None, 1, 5, 3, 3, 3, 5, None, 5, 1, 5, 3, 4, 5, 5, None, 1, 2, 5, 3, 5, 5, 1, 5, 5, 5, None, 4, 5, 5, 5, 5, None, None, 5, 5, 3, 5, 5, None, 5, 5, None, 2, None, None, 3, 1, 3, None, 5, 5, 3, 4, 4, None, None, 5, 5, 5, 3, 5, 4, 4, 2, 1, 5, 3, None, 3, 1, 3, None, 5, 3, 5, 5, 5, None, 5, None, 5, None, 2, None, 4, None, 4, 5, 5, 5, 1, 5, 5, 5, 5, 4, 5, 5, None, 3, None, None, 1, None, 4, 1, 3, 1, None, 5, 5, 3, 4, 2, 3, 4, 5, 5, 3, 5, 4, 5, 5, 3, None, 5, 1, None, 5, 1, 5, 5, 5, 5, 2, 5, 4, None, None, 5, 3, 5, None, 5, 5, 1, 3, 5, 5, 3, 5, None, 3, 5, 5, 5, 1, 4, 1, 1, 1, 4, 1, 5, 4, 5, 3, 5, None, 3, None, 5, 5, 1, 5, 5, 4, 5, 5, 1, 3, 5, 5, 5, 3, 5, 5, 1, 5, 1, None, 5, 3, 1, 1, 5, 5, 5, 3, 5, 1, 5, None, 3, 5, None, 5, 1, 3, None, 4, 4, 5, 5, 5, 1, 5, 5, 1, 5, 5, 3, 5, 5, 1, 5, 5, 5, 5, 4, None, None, 1, 5, 4, 5, 5, None, 5, 1, None, 3, 3, 1, None, 5, 5, 2, 4, 4, 5, 5, 2, None, 5, 5, 1, 5, 4, 5, 5, 2, 5, 5, 5, 5, 5, None, None, 4, None, 5, 5, None, None, 3, 1, 5, 5, 5, 1, None, None, 5, 3, 5, 5, 5, 4, 5, 4, 5, 3, None, 1, 3, 5, 2, 3, 1, 1, 3, None, 5, None, 5, 5, 5, 1, 3, 5, 5, 5, None, 5, None, None, 5, 5, 3, None, 4, 1, 5, None, None, 3, 2, 3, 3, 5, 1, 3, 5, 5, None, 5, None, 4, None, 5, 5, None, 5, 5, 2, 5, 3, 5, 4, 1, 1, 5, 5, 4, None, 3, None, 2, 4, 1, None, 1, 5, 1, 5, 5, 1, 2, 3, 1, None, 1, 5, 4, 5, 5, None, 5, 3, 4, 5, 1, 5, 5, 5, None, 5, 4, 5, 3, 5, 4, None, 5, 4, 5, None, 5, 5, 5, None, 3, 5, 5, 3, 3, 2, None, 4, None, 4, 5, 5, None, 5, 1, 3, 5, 5, None, 5, None, 5, 5, None, None, None, 5, None, 3, 1, 5, 5, 5, 1, None, 1, 4, 1, None, None, 5, 5, 3, 5, 2, 1, 4, 3, 5, 4, 5, 5, 5, 5, 5, 5, 3, 4, None, 5, 2, 3, None, 5, 2, 1, 3, 4, 4, None, None, 2, 5, None, None, 5, 1, 5, 1, 5, 3, 5, None, None, 5, None, 5, 4, 4, 5, 5, 5, None, 4, 4, 1, 5, 1, 1, None, 5, None, None, 2, 5, 1, 1, 4, 4, None, 3, 5, 5, 3, 5, 3, 3, None, 5, 5, 5, 4, 5, 3, 5, 5, 3, None, 3, 3, 4, 1, 3, 5, 3, 5, 5, 5, 5, 5, 5, 5, 4, None, 3, None, 5, 5, 5, 5, 2, 3, 5, 2, 5, 5, 4, 1, None, 5, 1, 4, 5, 5, None, 1, 3, 5, 5, 4, 4, 5, 5, None, 3, 3, 1, 5, 3, 4, 4, 5, 5, 3, 5, 5, 3, None, 5, 5, None, 5, 5, 1, 2, None, 3, 3, 1, 5, 5, 4, 2, 5, None, 5, 5, 3, 5, 4, 5, 5, 5, 5, 3, 2, 1, 1, 2, 4, 5, 3, 5, 3, 5, 1, 5, 4, 1, 3, 5, 4, None, 5, 3, 5, 1, 5, 5, 5, 5, 3, 5, 5, 1, None, None, 4, 5, 4, None, 5, 2, 5, 5, 1, 3, 3, 4, 5, 5, 5, 5, None, 5, None, 3, 4, 5, 3, 5, 5, 1, 5, 3, 4, 1, 5, 3, 5, 5, 5, 5, 3, 5, None, 5, 5, 5, None, 5, 1, None, 2, 1, 5, 5, None, 3, 4, 2, 5, 4, 3, 2, 1, 1, 3, 5, 3, 5, 5, 5, 5, 4, 1, 3, 2, 5, 5, 1, 5, 4, 1, None, 1, 4, None, None, 5, 5, 1, 3, 4, 5, 5, 4, 3, 5, None, 4, 4, None, None, 1, 5, 3, 3, 5, 3, 5, None, 5, 2, 5, 1, 5, 5, 5, None, 5, 5, 5, None, None, 5, 5, 5, 4, 5, None, 5, 5, None, None, 3, None, 5, 5, 3, 5, 3, None, 5, 5, 5, 5, None, 1, 2, 5, 5, 5, 5, None, 5, 5, 1, None, 1, 5, 5, None, 5, 2, 5, 5, 4, 3, 5, 1, None, 5, 5, None, 4, 3, 5, 4, 1, 5, 5, 3, 3, 5, 5, 3, 5, 4, 4, 2, 5, 3, 3, None, 5, 5, 5, 4, 3, 3, 5, 3, 3, 5, 5, 5, 5, None, 1, 1, 1, 3, 3, 3, None, None, 1, None, 3, 5, 5, 5, None, 3, 5, 1, 4, 5, 4, 3, 5, None, 5, 5, None, 1, 1, 3, 1, 5, 5, 5, 5, 5, 1, 5, 3, 5, 5, 3, 2, 5, 4, 3, 2, 5, 1, None, 5, 1, 1, 1, 5, 3, 4, 4, 5, 5, 5, 4, 4, 2, 5, 5, 2, 1, 5, 4, 5, 1, None, 4, 4, 4, 5, 5, 5, 5, None, 1, 5, 5, None, 5, 4, 5, 5, 5, 5, 3, 1, 5, 5, 5, 5, 5, 5, 5, None, 4, 4, 2, 3, 3, 3, None, 5, 5, 5, 5, 5, 5, 4, 3, 3, 5, 5, None, 1, 5, 4, None, 5, 3, 5, 2, 5, 5, None, 5, None, 5, 2, None, 5, 5, 4, 5, None, 3, None, 5, None, None, 3, 1, None, 5, 5, 1, 5, None, 5, None, None, 1, 5, 5, None, 4, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, None, 5, 5, 5, 5, 5, 5, 1, None, 5, 3, 4, 5, 4, 5, 5, 1, 1, 2, None, 5, 5, 5, 5, 5, None, 1, 5, 5, 3, 5, 1, None, 2, 3, None, 5, 1, 5, 4, 1, None, 5, 4, 5, 5, 5, 5, 1, 5, 5, 5, 1, 2, 5, 5, 5, 1, 5, 5, 5, 3, None, 3, 5, 4, 1, 2, 5, 4, 5, 5, 5, 5, None, 5, 2, 5, 1, None, 5, 2, 5, 1, None, 3, 1, 5, 5, 5, 3, 5, 5, None, 5, 4, 5, 5, 5, 1, 5, 1, 4, 5, None, 3, 5, 5, 1, 5, 5, 5, None, 4, 5, 5, None, 5, 5, 1, None, None, 5, 5, 5, 3, 3, 4, 3, 5, 4, 5, 5, 5, 2, 5, 4, 5, 5, 4, 4, None, 4, None, 5, 5, 5, 5, 5, 3, 3, 3, 5, None, 5, 5, None, 5, 1, 5, 2, None, 1, 5, 4, 5, None, 5, 5, 2, 5, 4, 5, 5, 1, 1, 5, 1, 5, 5, 4, 5, 2, None, 4, 5, None, None, None, 5, 5, 1, 1, None, 3, 3, 1, 1, 5, 4, 5, None, 5, 5, 3, 1, 5, 4, 1, 5, 3, 5, 5, 5, 5, 4, 2, 3, 5, 3, 4, 5, 3, 3, 5, 5, 4, 5, 3, 5, 4, 5, 4, 3, 5, 1, None, None, 5, 5, 4, 1, None, 5, None, 5, 5, 4, None, 5, 5, 3, 5, 4, None, 3, 4, 5, 4, 5, 5, 5, 5, 5, 3, 1, 5, 4, 3, 5, 5, 5, None, 3, 5, 5, 5, 5, 1, 5, 5, None, 3, 4, 3, 1, 4, None, 4, 5, 5, 5, 2, 5, 5, 3, 3, None, 5, 3, 1, 4, 5, 3, 2, None, 5, 5, 5, 5, None, 4, None, 1, None, 4, 3, None, None, None, 2, 4, 3, 5, 3, 1, 5, 5, None, 5, 2, 5, 5, 5, 5, 5, 4, 5, 5, None, 5, 5, 1, 1, 1, 4, 1, None, None, 5, 5, 3, 5, 1, 5, 5, 1, 2, 3, 4, 5, 1, 5, 3, 5, 1, 4, 5, 5, 2, 5, 2, 5, None, 5, None, 3, 5, None, 5, None, 1, 5, 4, 5, 5, None, 5, None, 5, 5, 5, 4, None, None, 5, None, 1, None, 3, 5, 3, 3, 4, 5, 5, 3, None, None, 1, 5, 3, 3, 5, 5, 5, 4, 5, 2, 5, None, 1, 1, 1, 5, 5, 4, 5, 5, 1, None, 5, 5, 2, 5, None, 4, 2, None, 5, None, 5, 5, 5, 5, 5, 1, 5, 5, 3, 5, 5, None, 1, 3, None, 5, 5, 5, None, 4, 1, 4, 5, 4, None, 1, 5, None, 4, 2, 3, 5, None, 5, 4, None, 5, 2, 5, 5, 3, 4, 3, 4, 3, None, 5, 5, 1, 4, 5, 4, 1, 1, 5, 4, 5, 5, 5, 5, 5, 5, 3, None, 5, 3, 5, 5, 5, 3, 5, 3, 5, 4, None, 5, None, 1, 1, 5, None, 3, 5, 5, 5, 3, 5, 5, 4, 5, 5, 4, 5, 5, 3, 4, 3, 4, None, 3, 5, 1, 1, 2, 4, 5, 3, 5, 1, 4, 3, None, 5, 2, None, 4, 5, 2, 2, 1, 5, 1, 2, 3, 2, 2, 5, None, None, 3, 3, 4, 5, 5, None, None, 5, 2, 5, None, 5, 5, 4, None, 5, 5, 1, 3, 5, 5, 5, 1, 5, 3, 5, 5, 5, 5, 5, 3, None, 5, 5, 4, 4, None, 3, None, 5, 3, 4, 5, 3, 5, 5, 3, 1, 4, 5, 5, 5, 5, 3, 3, 5, None, 5, None, 5, 1, 2, 5, 4, 1, 5, 5, 5, 1, None, 5, 3, 3, 4, 5, 5, 5, 3, 4, 5, 5, 1, None, 5, 5, 5, 5, None, 5, 5, None, 5, None, 5, None, None, None, None, None, 1, 3, 3, 4, 1, 5, 4, 3, 5, 3, 5, 5, None, 5, 4, 5, 3, 5, None, 5, 5, 5, 5, 5, 3, 3, 5, 5, 5, 5, None, 3, 5, 4, 5, 3, 4, 4, 5, 5, 2, 5, 5, 5, None, 1, 5, None, 3, 1, 4, 4, 5, 5, 5, 5, 5, 5, 1, 3, None, None, 5, 4, 3, 3, None, 3, 5, 5, 3, 2, None, 5, 5, None, 4, 5, 5, 5, 4, 5, 5, 4, 1, 5, 4, 5, 2, 1, None, 3, 5, 2, 3, 3, 5, 5, 3, 5, 5, 5, 5, 5, 3, 5, 1, 3, 3, None, 5, 4, 3, 5, 5, 4, 5, 5, 3, None, 5, 2, 1, None, 5, None, None, 5, None, 5, 3, 4, 3, 3, 5, 4, 3, 4, 5, 3, 5, 5, 5, None, 4, 3, 1, 4, None, None, 3, 1, 5, None, 3, 1, 4, 3, 5, 5, 4, 5, None, 1, 5, 1, 5, 2, 3, 5, None, 3, 2, 3, 3, 5, 4, 3, None, None, None, 5, 3, 2, None, 5, 5, 4, 1, 3, 5, 5, 4, 1, 5, 2, 4, 4, 5, 5, 5, 1, 5, None, 3, 1, 5, None, 4, 1, 3, None, 1, 4, 5, 5, 5, 1, 3, 5, 5, 3, 5, 1, 1, 5, None, 3, 3, 5, 4, 5, 5, 5, 5, 3, 5, 4, None, 3, 5, 4, 4, 5, 3, 5, None, 1, None, 5, 1, None, 5, 5, 5, 3, 5, None, 5, 5, 1, 5, 3, None, None, 3, 5, None, 5, 3, 5, None, 5, 4, 1, 5, 3, 1, 1, 4, 4, 1, None, 5, 3, 5, 4, 3, 3, 5, 5, 3, 5, 5, 5, 5, 3, 4, 5, 5, 5, 5, None, 4, 2, 4, None, 5, 5, 5, None, 1, 1, 2, 5, 5, 4, 3, None, None, 5, None, 5, 3, 3, 5, 5, None, 4, 5, 5, None, 2, 1, 4, 5, 2, None, 1, 4, None, 5, 4, 3, 2, 5, 1, 3, 1, None, 4, 5, 3, 5, 1, 5, None, 1, 1, 3, 4, 5, 5, 3, 3, 5, None, None, 5, 3, 5, 2, 2, 5, 5, 3, 1, 3, 4, 5, 5, 3, 2, None, 4, 5, 3, 4, 5, 4, 3, 1, 3, 5, None, None, 5, 5, 4, 3, 5, None, 4, 5, 5, 2, None, 5, 5, 4, 1, 5, 5, 1, 2, 5, 4, 2, 3, 5, 4, 5, 1, 5, 1, 3, 3, 1, 3, 5, 5, 5, 5, None, 1, None, 1, 1, 4, 3, 3, 5, 3, 5, 5, 5, 5, None, None, 5, 3, 5, 4, 5, None, 5, 5, None, 5, 5, 3, 5, 5, None, 5, 5, 3, 4, 4, 5, 3, 3, 3, 5, 5, 1, 4, 5, None, 5, 5, 5, 5, 1, 5, 5, 5, 5, 2, 5, 3, 3, 3, 5, 5, 4, 5, 5, None, None, 5, 1, 4, None, 1, 4, 5, 5, 1, 3, 5, 3, 5, 5, 2, 5, 5, 5, 3, None, 1, 5, 5, 5, 2, 3, 3, 4, 1, 3, None, None, 3, None, 4, 3, 5, 4, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 1, 5, 5, None, 5, 1, 5, 3, 3, 1, 3, 3, 5, 1, 5, 4, 1, 1, 1, 5, None, 5, 5, 5, None, 1, 3, 5, None, 5, 5, 5, 4, 1, 5, 5, 5, 3, 3, 5, 5, 1, 5, 1, 5, 5, 5, 5, None, 1, 5, 5, 5, 1, None, None, 5, 5, 2, 5, 5, 5, 3, 5, None, None, 5, 5, 1, 4, 5, 4, 1, 3, 2, 2, 5, 3, 3, 3, 5, 5, 5, 1, 5, 5, None, 3, None, None, 4, 5, 3, 5, 1, 1, 1, 1, 5, 5, 5, 5, None, 2, 5, 5, 5, 5, 5, 1, 2, 2, 5, 3, 3, 5, 5, 5, 1, 2, 4, 1, 3, 3, 5, 1, 3, 3, None, 5, 4, 5, 5, 5, None, 3, 5, 5, None, 5, 5, None, 5, 1, 1, 4, None, 5, 1, 3, 4, 4, 5, 5, 3, 3, 1, 5, 4, 1, None, 5, 3, 1, None, 5, None, 3, 5, 1, 5, 5, 5, 5, 2, 5, 4, 4, 3, 3, 4, 5, 5, 5, 4, 4, 3, 5, None, 5, 3, 2, None, 5, 1, 5, 3, 5, 3, 2, 1, 5, 5, None, 5, 5, 4, 5, 1, 3, 1, 5, 5, None, None, 3, 4, 5, 5, 5, 5, None, 5, 4, 4, 5, 4, 5, 2, 5, 5, 5, 1, None, 5, 5, 1, 3, 5, 5, 5, 1, 1, 5, 5, None, 4, 5, 3, 2, 3, 4, 5, 5, None, 3, 3, 4, 5, 1, 3, 4, 5, 5, 2, 5, 5, None, 5, 1, 4, 1, 4, 5, 5, None, 5, 4, 5, 1, 5, 5, 4, 3, 5, 2, 5, 3, 5, 5, 3, 1, 4, 3, 4, 2, 5, 1, 5, 3, 5, 5, 5, 5, 3, None, 5, 5, 5, 5, 3, 5, 5, 5, 5, None, 5, 5, None, 5, 1, 3, 5, 5, 3, None, None, None, 5, None, 5, 1, 3, 5, 3, 5, 1, None, None, 5, 3, None, 3, None, 1, 3, None, None, 5, 5, 5, 5, 2, 3, 3, 1, 4, 4, 4, 5, 3, 5, 5, 5, 4, 5, 5, 1, 3, 5, 3, 3, 5, 5, 5, 4, 5, 4, 5, 1, None, 5, 5, 5, 4, 5, 3, 5, 5, 5, None, 5, 5, None, 5, 3, 4, None, None, 3, 5, 4, 5, 3, 3, 3, 4, 5, 5, 5, 1, 5, 2, 4, 4, 1, 4, 1, 5, None, 1, 5, 3, 5, 5, 5, 2, 1, None, 1, 5, 5, 5, None, 5, None, None, 4, None, 3, 1, 4, 5, 5, None, None, 3, 2, 5, 1, 5, None, 5, None, 3, 5, 5, 5, 3, 3, 1, 1, None, 4, None, 1, 1, 4, 4, None, None, 2, 5, None, 4, 3, None, None, 5, 5, None, 5, 5, 3, 5, 5, 5, 4, 5, 5, 1, 3, 2, 5, 3, 2, 2, 5, 1, 5, 3, 1, 5, None, 5, None, 5, 5, 3, 5, 1, 1, 5, 5, 5, None, 5, 5, 4, 5, 5, 1, 5, 4, 4, 5, 5, 4, None, 5, None, 5, 2, 3, 3, 5, 2, 3, 1, None, 5, 1, None, 5, 4, 5, 5, 4, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 3, 5, 1, 5, 2, 3, 4, 5, 5, 4, 3, 1, 3, 1, None, None, 5, 5, 4, 5, None, 3, 5, None, 5, 3, 4, 5, 5, 5, 4, 1, 5, 3, 5, 3, None, 4, 3, None, 3, None, 4, 5, None, 5, 3, 5, None, 5, 3, None, 1, 5, 4, 1, 4, 5, 5, 1, 5, None, 3, 1, None, 3, 2, 5, 5, 5, 4, 5, 1, 1, 5, None, 3, None, None, 5, 5, 5, 3, 4, 5, 5, 4, 5, 5, 4, 5, 2, 5, 5, None, 5, 4, 4, 4, 5, None, 5, None, 4, None, 1, None, 5, 5, 5, 1, 5, 3, None, 5, 2, 5, None, 3, 5, 3, 5, 3, 5, 3, 4, 1, None, 5, 1, 5, 4, 5, 5, None, 3, 5, 4, 3, 5, None, 5, 3, 1, 5, 5, 5, 2, 3, 5, 3, 2, 4, 5, 5, 5, 5, 5, 5, 5, 3, 5, None, 2, 5, 2, 5, 3, None, 4, 2, 4, 1, 4, 5, 4, None, None, None, 5, 5, 3, 3, 5, 5, 5, None, 1, 5, 4, 1, 1, 3, 4, None, 3, 5, 3, 5, 1, 1, None, 3, 5, 5, 2, 5, None, 5, 5, 5, 1, 5, None, 5, 1, 2, 3, 1, 1, 3, 1, 5, 3, 3, 3, 5, None, None, 4, 5, 5, 4, 3, 5, 5, 4, 5, 4, 3, 5, None, None, 1, 5, 5, 3, 3, 5, 5, 5, None, None, 5, 5, None, 5, 5, 1, 3, None, None, 2, 3, 4, 1, 5, 1, 5, 4, 4, 3, 2, 4, 5, 5, 3, None, 5, 5, 5, 4, None, 5, 3, 5, 5, 5, None, 5, 5, 4, 5, 5, 5, None, None, 5, None, 5, 1, None, 5, 5, 5, 2, 1, 5, 5, 3, 5, 1, 1, None, None, 4, 5, 4, 1, None, 5, 3, None, 5, 5, 4, 4, None, 3, None, 5, 5, 2, 5, 5, 5, 5, 5, 1, None, 5, 5, 5, 1, 1, 5, 4, 5, 5, 5, None, None, 5, None, 3, 4, 2, 4, 5, 5, 5, 1, 5, 5, 2, 5, 4, 3, 5, 5, None, None, 5, 3, None, 5, 5, 1, None, 1, None, 5, 5, 5, 3, 1, 3, 5, 3, 5, None, 5, 5, None, 4, 5, None, 5, 5, 3, 2, 4, 3, 1, 3, 5, 4, 5, 1, 3, 4, 5, None, 1, 1, 5, 1, 4, 2, 5, 5, 5, 2, 4, 5, 5, 1, 1, 5, 2, None, 4, 3, None, None, 5, None, 5, 1, 4, None, 1, 5, 5, 5, None, 5, 1, 5, 5, 5, 5, 1, 4, 5, 3, 5, 4, None, 1, 5, 3, 5, None, 5, 3, 1, 5, 5, 3, 5, 3, 5, 5, 3, None, None, 5, 5, None, 5, 5, 5, 5, None, 3, None, 5, None, 4, 5, None, 5, None, 3, 3, 5, 4, 5, 4, 3, 5, 5, 3, 4, 3, 5, 1, 5, None, 3, None, 3, 5, 1, 3, 5, None, 5, 4, None, 1, 5, 5, 5, 5, 5, 5, 2, 5, None, 5, 3, None, 3, 5, 5, None, 1, 5, 3, 5, 5, 1, 4, None, 3, 5, 5, 5, 5, 4, 5, 2, 5, 5, 3, 1, None, 5, 5, 5, 4, 3, 1, 5, None, 3, 3, 3, 1, 5, 5, None, 5, 3, 5, 3, 4, 5, 5, 3, 3, 1, 5, 5, 5, None, 5, 4, 5, 4, 3, 5, 4, 5, 1, 4, 3, 1, 5, 4, None, None, 5, 2, 5, 5, 1, 5, 5, 5, 4, 5, 3, None, 3, None, 1, 5, 5, 5, 5, None, 1, 5, 2, 1, 3, 5, 4, 1, 5, 5, 5, 3, None, 5, 5, 5, 5, None, 2, 5, 5, 1, None, 5, 5, 5, 5, 5, 1, None, 5, 2, 4, 4, 2, 3, 5, 1, None, 5, 5, 5, 5, 5, 1, 5, 2, 3, 4, 5, 5, 2, 1, 5, 1, 4, 4, None, 5, None, 5, 5, 3, 5, 5, 1, 5, 5, 4, 3, 5, 5, 5, 3, 4, 1, 5, 1, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, None, 4, 5, 5, 3, 1, 4, 1, 3, None, 5, 5, 2, 1, 2, 5, 5, None, 5, 1, 5, 4, 5, 5, 1, None, 5, None, 5, 3, 5, 3, 5, 5, 5, 3, 5, 5, 5, 1, 2, 5, 2, None, 3, 5, 5, 1, 5, 5, 5, 5, None, 5, 5, None, None, 3, 5, 5, 2, 5, None, 5, 3, 5, 5, 3, 5, 1, 4, 5, 5, 4, None, 5, 3, 4, 5, 5, None, 5, 5, 5, 5, 5, None, 5, 4, 1, None, 3, 4, 5, 5, 5, 1, 5, 1, 3, 5, 3, 5, None, None, 3, 1, 5, 5, None, 5, 5, 5, 5, 1, 5, 3, 2, 5, 3, 1, 4, None, 5, 5, 2, 5, 5, None, 5, 4, 2, 4, None, 5, 4, 2, 3, 5, 1, 4, 1, 5, 5, 5, None, 4, 4, 5, None, 3, 5, 3, 4, None, 5, 5, 5, 1, 4, None, 5, 4, 1, 1, 5, 5, None, 2, 3, 1, 3, 5, None, 5, 3, 5, 5, 5, 1, None, None, 4, 1, None, 5, 5, 5, 4, 3, 5, 4, 1, 5, 5, 3, 5, None, 5, 2, 5, 3, 3, 4, 5, 3, 3, 1, 4, 1, 5, None, None, None, 4, 5, 5, 5, 1, 5, 5, 4, 5, 5, 5, 4, 3, 1, 1, 5, 5, 5, 2, None, 5, 4, 5, 3, 5, None, 5, 5, 1, 1, 5, 2, 5, 5, 5, 5, 5, 5, 5, None, 3, 4, None, 4, 3, 5, 3, 4, 4, 1, 4, 5, 5, 5, 4, 1, 3, None, 1, None, 5, 5, 5, 3, 1, 1, 5, 1, 3, 5, 3, 5, 3, 5, 4, None, 1, 5, None, 5, None, 4, 1, 3, 1, 3, 5, 5, 5, 3, 3, 5, 3, 3, 4, 5, 5, 5, None, 3, 5, 5, None, 3, 5, 4, 5, 5, 4, 1, 5, 3, 1, 5, None, None, None, 3, 5, 5, 5, 4, 5, None, 5, 5, 1, None, None, 1, 1, None, 5, 1, 5, 5, 1, None, None, 1, 1, 5, 5, None, 5, 3, 1, 1, 5, 1, 5, 5, 3, 5, 1, 3, 1, 3, 2, None, 3, None, 5, 1, 1, 4, 5, 3, 5, 1, 5, 1, None, 4, None, 5, 5, 1, 3, None, None, 3, 5, 1, 5, 5, 5, 3, 5, 3, 4, 5, 5, 3, 5, 5, 3, 4, 5, 5, 5, 3, 5, 1, 5, 1, 5, 5, 5, None, 3, None, 3, 4, 1, 1, 5, 5, 5, 5, 3, 2, 5, 4, 4, None, 1, 1, None, 3, 5, 5, 1, 4, 5, None, 5, 2, 5, 3, None, 4, 3, None, 5, 1, 3, 5, 5, 4, 5, 5, 4, 5, 2, 2, 1, 3, 2, 5, 3, None, 5, 1, 5, 4, 5, 5, 1, 3, 1, 5, 5, 4, 5, 5, 5, 3, 5, None, 1, 5, 5, 5, 4, 3, 2, 3, None, 1, 5, 5, 5, 4, 1, 4, 4, 2, 5, None, 3, 3, None, 5, 3, 5, 5, 5, 5, 2, None, 5, 5, 4, 5, None, None, 4, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 3, 5, 4, 4, None, 4, 5, 2, 5, 5, 5, 5, 5, 3, 4, 5, None, 1, None, None, 4, 5, None, 5, 5, 5, 3, 5, 3, None, 5, 4, 3, 4, 5, 5, 5, 3, None, 5, 5, 1, None, 5, 5, 1, 5, 3, 5, 5, 5, 3, 5, 1, 1, 5, 5, 5, 3, 5, 5, 1, 4, None, 3, 5, 5, 4, 5, None, 4, 1, None, 2, 5, 1, 5, 5, 3, 3, None, 4, 2, 1, 3, None, 5, 3, 5, 5, 1, 5, 5, 2, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 2, 5, 5, 5, 4, 5, 5, 1, 3, 3, 3, 3, 4, 4, 3, 2, 5, 4, 5, 4, 5, 4, 3, None, 5, 1, 5, None, 5, 4, 1, 4, 5, 1, 3, 3, 5, 1, 5, 4, 5, 3, 5, 3, 3, None, 5, None, 5, None, 2, None, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 1, 3, 5, 3, 5, 3, 3, 5, 3, 1, 5, None, 2, 5, 5, 3, 4, 5, 5, 5, 5, 5, 5, 3, 5, 3, 4, None, 3, 4, 4, 4, 3, 2, 5, None, 5, 3, 4, 5, 5, None, 3, 5, 3, 5, 3, 5, 5, 5, 1, 5, 4, 3, 5, 3, 5, 5, 5, 5, 3, 4, 1, 5, 3, 1, 5, 4, 3, None, 5, 5, 1, 5, 3, 5, None, 5, 3, 5, 5, 5, 3, 3, 5, 5, 5, 5, None, 1, 5, 3, 3, 1, 5, 5, 4, 4, None, 1, 5, 4, 5, None, 5, 5, None, 3, 3, 3, 1, 5, 3, 4, 4, 5, 1, 3, 5, None, 5, 4, 4, None, 3, 1, 2, 5, None, 3, 4, 3, 4, None, 1, None, 5, 5, 4, 2, 5, 5, None, 5, 5, 1, 5, 5, None, 3, 5, 5, 5, 3, 4, 5, 5, 3, 5, 5, 5, 5, 1, 5, 4, 3, 4, 3, 1, None, 1, None, 5, 3, None, None, 1, 3, 5, 1, 5, 3, 4, 3, 5, 1, 5, 5, None, 5, None, 5, 4, 5, 3, 5, 5, 4, 5, None, 5, 4, 3, 1, 5, None, 3, 2, 5, 5, 5, 5, 1, None, 5, 5, 4, 3, 5, 1, 5, 3, 5, 5, 1, 4, 5, 3, 3, 2, 5, None, 5, 3, None, 2, None, 5, 5, 5, 5, 5, 5, 1, 5, 5, 4, None, 5, 1, 1, 4, 5, 2, 1, 5, 1, 1, 5, 5, 5, 4, 3, 5, 5, 4, 3, 1, 4, 5, 5, 5, 1, 5, 5, 5, 1, 2, 5, 4, 5, 4, None, 5, 3, 5, None, 5, 5, 1, None, 3, 5, None, None, 1, 3, 3, 1, 1, 3, 5, 5, 3, 3, 5, 4, 5, 5, 5, 5, 4, 5, 5, 1, 5, 5, 5, None, 5, 5, None, 5, None, 1, 5, None, 1, 1, 3, None, 5, 5, 5, 5, 5, 5, 5, 2, 3, 3, 1, 1, 1, 5, 4, 5, 5, 3, 5, 3, 1, 3, 4, 3, 5, 2, 5, 3, None, 5, 1, 1, 3, None, 2, 5, 5, 1, 3, 5, 2, 3, 3, 3, 1, 3, 2, 2, 3, 4, 5, 1, 2, 4, 1, 5, 1, 1, 5, None, 5, None, 1, 5, 4, None, 5, 1, 3, 3, 5, None, 5, 5, 1, 1, 5, 4, 2, 5, 5, 5, 3, 5, 5, 5, 5, 5, 1, 5, None, 1, 5, 5, 5, 4, 5, 5, 5, None, 5, 4, 1, 3, 5, 3, 2, 1, 1, None, 3, 3, None, 5, 5, 3, None, None, 5, None, 3, 1, 5, 3, 1, 3, 5, 5, 5, 2, None, 3, 5, 5, 4, 5, 5, 2, 3, 3, 4, None, 5, 1, None, 3, 5, 1, 4, 1, 5, 5, 3, 1, 5, 1, 5, 5, 2, None, 5, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, None, 5, None, 5, 5, 5, 5, 5, 5, 2, None, 1, 5, None, 5, 3, 1, 3, 5, 5, 3, 5, 3, 5, 5, 5, 5, 5, 5, 1, 4, 1, 5, 5, 3, 5, None, 5, 5, 1, 4, 5, 1, 3, 4, 3, 4, 1, 5, None, 4, 5, 5, 3, 3, None, 5, 5, 2, 5, 3, 5, 1, 5, 4, 4, None, 1, 5, 1, 5, 3, 3, None, 1, 5, 1, 5, 5, 1, 5, 5, 3, 3, 4, None, 3, 1, 5, None, 5, 3, 5, None, 4, 2, 4, 5, None, 5, 3, 4, 2, None, 1, 5, 4, 3, 5, 3, 2, 5, 4, None, 2, 5, 4, 3, None, None, 5, 3, 4, None, 5, 5, None, 5, 4, 3, 5, 3, 5, 4, 5, 1, None, 5, 5, 4, 1, 1, 3, 5, 1, 5, 2, 5, 5, 5, 5, 4, 1, 3, 5, 1, 1, 3, 5, 4, 1, 5, 5, 5, None, 3, None, None, 5, 4, 5, 1, 5, 3, 5, 3, 5, 5, 5, 1, 5, 5, None, None, 4, 5, 5, 5, None, 5, 5, 5, 1, 5, 2, 5, 5, 1, 3, 5, 3, 4, 2, 5, 5, 5, 5, 4, 5, 5, 1, 4, 5, None, None, 3, 5, 3, 3, 1, None, None, 5, 5, 4, 5, 4, None, 5, 5, 5, 5, None, 1, 5, 5, 5, 5, 5, 4, 4, 1, None, 1, None, 5, 3, 1, 5, None, 5, 5, 5, 3, 5, 5, None, 1, 3, 5, 4, 5, 5, 3, 5, 5, 5, 4, 3, 5, 5, None, 5, 5, 4, 5, 5, None, 5, 5, 1, 4, 5, 5, 2, 3, 4, 5, 5, 5, 5, 3, 5, 2, 3, 5, 3, 5, 1, 4, 5, 5, 5, 1, 5, 5, 5, 5, 3, 5, 3, 1, 3, 5, 3, 3, 5, 5, 4, 1, 3, 5, 3, 3, None, 5, 4, 5, 5, 3, 5, 5, 4, 5, 5, 1, 5, 2, 5, 4, 5, 2, 5, 5, 3, 4, 3, 3, 5, 5, 3, None, 3, 5, 3, 5, None, 4, 1, 5, 3, 5, 5, 5, 1, None, 1, 2, None, 5, 5, 4, 3, 3, None, 5, 5, 5, 4, 3, 5, 5, None, 5, 5, 5, 5, 1, 1, 3, 3, 5, 5, 4, 5, 5, 4, 5, 3, 5, 5, 3, 1, None, 3, None, 5, 5, 1, 3, 4, 3, 5, 5, 3, 5, 3, 1, 5, 1, 3, 5, None, 3, 2, None, None, 5, 4, 5, None, 5, None, 5, None, 3, 5, None, 5, 5, 4, 2, 5, None, 1, 5, None, 1, 5, 5, 5, 4, 3, 5, 3, 5, 4, None, None, 1, 5, 2, None, 5, 5, 5, 4, 4, 5, 5, 5, 3, None, 4, 5, 4, 5, 4, 3, 3, None, 3, None, 4, 1, 4, 3, 2, 3, 5, 4, 5, 5, 5, None, 5, 4, 3, 5, 3, 2, 5, 2, None, 5, 4, 5, None, 5, 5, 5, 3, 3, 4, 3, 3, 5, 5, None, 1, 2, 5, 5, 5, 5, 5, 5, None, 4, 2, 5, 1, 5, 5, 4, 5, 2, 3, 5, 1, 5, 1, None, 2, None, 4, 3, 4, 5, 5, 5, 5, 5, 5, 4, 3, 4, 3, 3, 4, 5, 5, None, 3, 3, 5, 4, 5, 5, 5, 1, 3, 5, 3, 1, None, 4, 5, 5, 5, 5, 2, 4, 5, 4, 5, 1, 3, 5, 5, None, 5, 5, 1, 4, 5, 5, 3, 5, None, 5, 5, 3, 3, 3, None, 1, 3, None, None, 2, 5, None, 5, None, None, 5, None, 1, 5, None, 5, 3, None, 5, 5, 1, None, 5, 2, None, 5, 3, 3, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 1, 3, 1, 4, 3, 1, 2, 5, 3, None, 3, 1, 5, 4, 5, None, 5, 1, 4, 5, 2, None, None, 4, 2, 5, None, 4, None, 1, 1, None, 2, None, 5, 5, 3, 5, 4, 3, 1, 5, 2, None, None, None, 5, 3, None, 5, 1, 5, 4, 3, 5, 5, None, 1, 5, 1, 5, 5, 5, 5, 3, 1, 5, 1, 5, 5, 5, 5, 1, 1, 3, 4, 4, 4, 2, None, 4, 5, 1, 3, None, 3, 2, 5, 5, None, 4, None, 1, 4, 2, 4, 2, 3, 5, 2, 5, 4, 4, 5, None, 1, 5, 5, 1, 5, 5, None, None, 3, 4, 4, 5, 4, 4, 3, 5, 3, 3, 1, 5, 3, 3, 1, 5, 5, 5, 3, None, 5, 3, 5, 4, 5, 1, 5, 2, 5, 3, 2, 5, 5, 5, 5, 5, 3, 5, None, 5, 3, 5, 5, 5, 5, None, 4, None, 3, 3, 5, 5, 5, None, 5, 4, 3, 2, 4, 5, 3, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 3, None, None, 2, 1, 5, 3, 3, 1, None, 5, 4, None, 5, 1, 5, 5, 1, 3, 5, None, 4, None, 3, 1, 2, 5, 1, 3, None, 5, 4, 5, 5, 3, 2, 1, 5, 4, 1, 2, None, None, 3, 5, 4, 2, 3, 5, 3, 3, 3, 4, None, 5, 3, 5, None, 5, 2, 5, 1, 5, 4, 5, 5, 5, 4, 5, 4, 1, 5, None, None, 4, 4, 1, None, 1, 5, 5, 5, 5, 5, 5, None, 5, 4, 5, 5, 5, 4, 5, 4, 5, 2, 4, 5, None]\n"
     ]
    }
   ],
   "source": [
    "dic_annual_income_est = {'C.60K-100K':3, 'D.30K-60K':4, 'A.ABOVE200K':1, 'B.100K-200K':2, 'E.BELOW30K':5}\n",
    "annual_income_est_lst = list()\n",
    "for i in df[\"annual_income_est\"]:\n",
    "    if dic_annual_income_est.get(i):\n",
    "        annual_income_est_lst.append(dic_annual_income_est.get(i))\n",
    "    else:\n",
    "        annual_income_est_lst.append(i)\n",
    "print(annual_income_est_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annual_income_est_label_encode\"] = annual_income_est_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2610"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"annual_income_est_label_encode\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### annual_income_est_label_encode also have the same number of missing data as the 3 other columns (before these 3 columns are imputed): hh_20, pp_20 and hh_size_est. Hence, this is a good basis for the assumption that there is correlation between these 4 columns. hh_20, pp_20 and hh_size_est will be used as X_train for imputation using DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A.ABOVE200K' 'A.ABOVE200K' 'A.ABOVE200K' ... 'A.ABOVE200K' 'A.ABOVE200K'\n",
      " 'A.ABOVE200K']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc_x_train_aie = df.dropna(subset = [\"annual_income_est\"])[[\"hh_20\", \"pop_20\",\"hh_size_est\"]]\n",
    "dtc_y_train_aie = df.dropna(subset=[\"annual_income_est\"])[\"annual_income_est\"]\n",
    "\n",
    "dtc_x_test_aie = df[df[\"annual_income_est\"].isna()][[\"hh_20\", \"pop_20\", \"hh_size_est\"]]\n",
    "\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf2.fit(dtc_x_train_aie, dtc_y_train_aie)\n",
    "\n",
    "dtc_y_predicted_aie = clf2.predict(dtc_x_test_aie)\n",
    "print(dtc_y_predicted_aie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"annual_income_est\"].isna(), \"annual_income_est\"] = dtc_y_predicted_aie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one hot encoding for annual_income_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "annual_income_est_np_array = np.array(df[\"annual_income_est\"]).reshape(-1,1)\n",
    "label_encoding2 = OneHotEncoder()\n",
    "encoded2 = label_encoding2.fit(annual_income_est_np_array)\n",
    "print(encoded2.transform(annual_income_est_np_array).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded2.transform(np.array(['C.60K-100K','D.30K-60K','A.ABOVE200K','B.100K-200K','E.BELOW30K']).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E.BELOW30K     7770\n",
       "A.ABOVE200K    4705\n",
       "C.60K-100K     2679\n",
       "D.30K-60K      1910\n",
       "B.100K-200K     725\n",
       "Name: annual_income_est, dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"annual_income_est\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annual_income_est_T = encoded2.transform(annual_income_est_np_array).toarray().T\n",
    "annual_income_est_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 0\n",
    "for i in ['A.ABOVE200K','B.100K-200K','C.60K-100K','D.30K-60K','E.BELOW30K']:\n",
    "    name = \"annual_income_est_\" + i\n",
    "    df[name] = annual_income_est_T[some_index]\n",
    "    some_index += 1\n",
    "df = df.drop(\"annual_income_est\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_list = list()\n",
    "# for i in df.columns[-12:]:\n",
    "#     random_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_random = pd.concat([df.iloc[:,-12:-6], df.iloc[:,-5:]], axis = 1)\n",
    "# df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = pd.concat([df_numeric, df_random], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_last = pd.concat([df.iloc[:,-12:-6], df.iloc[:, -5:]], axis = 1)\n",
    "# df_final = pd.concat([df[list(numeric_col)], df_last], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values of the numeric columns with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Counter({0.0: 13661, 1.0: 570})\n",
      "After: Counter({0.0: 13661, 1.0: 13661})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.96      3418\n",
      "         1.0       0.18      0.22      0.20       140\n",
      "\n",
      "    accuracy                           0.93      3558\n",
      "   macro avg       0.57      0.59      0.58      3558\n",
      "weighted avg       0.94      0.93      0.93      3558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=[\"f_purchase_lh\"])\n",
    "y = df[\"f_purchase_lh\"]\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print('Before:', Counter(y_train))\n",
    "X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "print('After:', Counter(y_train))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    " \n",
    "model = RandomForestClassifier() \n",
    "model.fit(X_train, y_train) \n",
    " \n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_train.columns) \n",
    "top_features = feature_importances.nlargest(20).index \n",
    "X_train_selected = X_train[top_features] \n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# # instantiate the model (using the default parameters)\n",
    "# logreg3 = LogisticRegression(random_state=42)\n",
    "\n",
    "# # fit the model with data\n",
    "# logreg3.fit(X_train_selected, y_train)\n",
    "\n",
    "# y_pred = logreg3.predict(X_test[top_features])\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    " \n",
    "dt_clf = DecisionTreeClassifier() \n",
    "dt_clf.fit(X_train, y_train) \n",
    " \n",
    "y_test_pred = dt_clf.predict(X_test) \n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(df: pd.DataFrame) -> list:\n",
    "\n",
    "\n",
    "    df = df.drop(columns=pd.Index([\"flg_affconnect_lapse_ever\", \"hlthclaim_cnt_success\",\"giclaim_cnt_success\",\"recency_cancel\", \"recency_lapse\"]))\n",
    "    spike_cols = [col for col in df.columns if 'ape_' in col[:4]]\n",
    "    df = df.drop(columns = pd.Index(spike_cols))\n",
    "\n",
    "    sumins_cols = [col for col in df.columns if 'sumins_' in col[:len(\"sumins_\")]]\n",
    "    df = df.drop(columns = pd.Index(sumins_cols))\n",
    "\n",
    "    prempaid_cols = [col for col in df.columns if 'prempaid_' in col[:len(\"prempaid_\")]]\n",
    "    df = df.drop(columns = pd.Index(prempaid_cols))\n",
    "\n",
    "    for names in [\"clmcon_visit_days\", \"recency_clmcon\", \"recency_clm_regis\", \"flg_hlthclaim_\", \"flg_gi_claim_\" , \"f_ever_bought_\", \"n_months_last_bought\" , \"lapse_ape_\", \"n_months_since_lapse_\", \"cltsex_fix\"]:\n",
    "        spike_cols = [col for col in df.columns if names in col[:len(names)]]\n",
    "        df = df.drop(columns = pd.Index(spike_cols))\n",
    "\n",
    "    df = df.drop(columns = pd.Index([\"clttype\", \"stat_flag\", \"min_occ_date\", \"recency_giclaim_success\", \"giclaim_cnt_unsuccess\", \"recency_giclaim_unsuccess\"]), axis = 1)\n",
    "    df[\"recency_giclaim\"] = df[\"recency_giclaim\"].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "    df[\"recency_hlthclaim\"] = df[\"recency_hlthclaim\"].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "    df[\"giclaim_amt\"] = df[\"giclaim_amt\"].astype(\"float64\")\n",
    "    gi_claim_median = df[\"giclaim_amt\"].median()\n",
    "    df[\"giclaim_amt\"] = df[\"giclaim_amt\"].fillna(gi_claim_median)\n",
    "    df[\"hlthclaim_amt\"] = df[\"hlthclaim_amt\"].astype(\"float64\")\n",
    "    hlthclaim_median = df[\"hlthclaim_amt\"].median()\n",
    "    df[\"hlthclaim_amt\"] = df[\"hlthclaim_amt\"].fillna(hlthclaim_median)\n",
    "    df[\"tot_cancel_pols\"]=df[\"tot_cancel_pols\"].fillna(0)\n",
    "    non_numeric_cols = df.select_dtypes(include=[\"string\", \"object\"]).columns\n",
    "    df_numeric = df.drop(columns=non_numeric_cols)\n",
    "    id = 1\n",
    "    \n",
    "    df[\"flg_affconnect_ready_to_buy_ever\"] = df[\"flg_affconnect_ready_to_buy_ever\"].fillna(0)\n",
    "    df[\"flg_affconnect_show_interest_ever\"] = df[\"flg_affconnect_show_interest_ever\"].fillna(0)\n",
    "    df[\"f_ever_declined_la\"] = df[\"f_ever_declined_la\"].fillna(0)\n",
    "    df[\"is_dependent_in_at_least_1_policy\"] = df[\"is_dependent_in_at_least_1_policy\"].fillna(1)\n",
    "\n",
    "    df= df.drop([\"clntnum\", \"race_desc\"], axis=1)\n",
    "    ## Majority of the clients are Singaporean as shown below. Hence, we are going to focus on Singaporean clients\n",
    "    df[\"ctrycode_desc\"]\n",
    "    df[\"ctrycode_desc\"].value_counts()\n",
    "    df = df[df[\"ctrycode_desc\"] == \"Singapore\"]\n",
    "    df = df.drop(\"ctrycode_desc\", axis = 1)\n",
    "    id = 1\n",
    "    \n",
    "    #### Editing column feature: DOB -> Age\n",
    "  \n",
    "    # converting DOB to age\n",
    "    age_list = list()\n",
    "    for x in df[\"cltdob_fix\"]:\n",
    "        if x.lower() != \"none\":\n",
    "            year = int(x[:4])\n",
    "            age = 2024 - year\n",
    "            age_list.append(age)\n",
    "\n",
    "        \n",
    "    df[\"cltdob_fix\"] = pd.Series(age_list)\n",
    "    median_value = df[\"cltdob_fix\"].median()\n",
    "    df[\"cltdob_fix\"] = df[\"cltdob_fix\"].replace({None: np.nan})\n",
    "    df[\"cltdob_fix\"] = df[\"cltdob_fix\"].fillna(median_value)\n",
    "    ### Using hh_20 column and pop_20 columns as the X_train data for KNN imputation of hh_size_est since there are links between these 3 columns. This is because we checked that there are the same number of missing values for the three columns\n",
    "    \n",
    "    df[\"hh_20\"] = df[\"hh_20\"].fillna(-1)\n",
    "    df[\"pop_20\"] = df[\"pop_20\"].fillna(-1)\n",
    "    df[\"hh_20\"] = df[\"hh_20\"].astype(int)\n",
    "    df[\"pop_20\"] = df[\"pop_20\"].astype(int)\n",
    "    hh_20_lst = list()\n",
    "    for i in df[\"hh_20\"]:\n",
    "        if i != -1:\n",
    "            hh_20_lst.append(i)\n",
    "    hh_20_median = statistics.median(hh_20_lst)\n",
    "  \n",
    "    pop_20_lst = list()\n",
    "    for i in df[\"pop_20\"]:\n",
    "        if i != -1:\n",
    "            pop_20_lst.append(i)\n",
    "    pop_20_median = statistics.median(pop_20_lst)\n",
    "\n",
    "    df[\"hh_20\"] = df[\"hh_20\"].replace(-1, hh_20_median)\n",
    "    sum(df[\"hh_20\"] == -1)\n",
    "    df[\"pop_20\"] = df[\"pop_20\"].replace(-1, hh_20_median)\n",
    "    sum(df[\"pop_20\"] == -1)\n",
    "    \n",
    "    df[\"hh_size_est\"] = df[\"hh_size_est\"].replace(\">4\", \"5\")\n",
    "    df[\"hh_size_est\"].value_counts()\n",
    "    from sklearn.impute import KNNImputer\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=15)\n",
    "    imputed_data = imputer.fit_transform(df[[\"pop_20\",\"hh_20\",\"hh_size_est\"]]).round()\n",
    "    imputed_data\n",
    "    df[\"hh_size_est\"] = pd.DataFrame(imputed_data[:,2])\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    dtc_x_train = df.dropna(subset = [\"hh_size_est\"])[[\"hh_20\", \"pop_20\"]]\n",
    "    dtc_y_train = df.dropna(subset=[\"hh_size_est\"])[\"hh_size_est\"]\n",
    "\n",
    "    dtc_x_test = df[df[\"hh_size_est\"].isna()][[\"hh_20\", \"pop_20\"]]\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(dtc_x_train, dtc_y_train)\n",
    "\n",
    "    dtc_y_predicted = clf.predict(dtc_x_test)\n",
    "    \n",
    "    df.loc[df[\"hh_size_est\"].isna(), \"hh_size_est\"] = dtc_y_predicted\n",
    "\n",
    "    #### Now, all the missing values of the hh_size_est column are fully imputed\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    hh_size_est_lst_numpy_array = np.array(df[\"hh_size_est\"]).reshape(-1,1)\n",
    "    label_encoding = OneHotEncoder()\n",
    "    encoded = label_encoding.fit(hh_size_est_lst_numpy_array)\n",
    "    \n",
    "    one_hot_encoding_hh_size_est = encoded.transform(hh_size_est_lst_numpy_array).toarray()\n",
    "    type(one_hot_encoding_hh_size_est)\n",
    "    one_hot_encoding_hh_size_est_T = one_hot_encoding_hh_size_est.T\n",
    "    some_id = 0\n",
    "    for i in [\"0\", \"1\", \"2\", \"3\", \"4\", \">4\"]:\n",
    "        name = \"hh_size_est_\" + i\n",
    "        df[name] = one_hot_encoding_hh_size_est_T[some_id]\n",
    "        some_id += 1\n",
    "    encoded.transform(np.array([0,1,2,3,4,5]).reshape(-1,1)).toarray()\n",
    "    ## Working on annual_income_est column\n",
    "    dic_annual_income_est = {'C.60K-100K':3, 'D.30K-60K':4, 'A.ABOVE200K':1, 'B.100K-200K':2, 'E.BELOW30K':5}\n",
    "    annual_income_est_lst = list()\n",
    "    for i in df[\"annual_income_est\"]:\n",
    "        if dic_annual_income_est.get(i):\n",
    "            annual_income_est_lst.append(dic_annual_income_est.get(i))\n",
    "        else:\n",
    "            annual_income_est_lst.append(i)\n",
    "    \n",
    "    df[\"annual_income_est_label_encode\"] = annual_income_est_lst\n",
    "    df[\"annual_income_est_label_encode\"].isna().sum()\n",
    "    ##### annual_income_est_label_encode also have the same number of missing data as the 3 other columns (before these 3 columns are imputed): hh_20, pp_20 and hh_size_est. Hence, this is a good basis for the assumption that there is correlation between these 4 columns. hh_20, pp_20 and hh_size_est will be used as X_train for imputation using DecisionTreeClassifier \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    dtc_x_train_aie = df.dropna(subset = [\"annual_income_est\"])[[\"hh_20\", \"pop_20\",\"hh_size_est\"]]\n",
    "    dtc_y_train_aie = df.dropna(subset=[\"annual_income_est\"])[\"annual_income_est\"]\n",
    "\n",
    "    dtc_x_test_aie = df[df[\"annual_income_est\"].isna()][[\"hh_20\", \"pop_20\", \"hh_size_est\"]]\n",
    "\n",
    "    clf2 = DecisionTreeClassifier()\n",
    "    clf2.fit(dtc_x_train_aie, dtc_y_train_aie)\n",
    "\n",
    "    dtc_y_predicted_aie = clf2.predict(dtc_x_test_aie)\n",
    "    \n",
    "    df.loc[df[\"annual_income_est\"].isna(), \"annual_income_est\"] = dtc_y_predicted_aie\n",
    "    ##### one hot encoding for annual_income_est\n",
    "    annual_income_est_np_array = np.array(df[\"annual_income_est\"]).reshape(-1,1)\n",
    "    label_encoding2 = OneHotEncoder()\n",
    "    encoded2 = label_encoding2.fit(annual_income_est_np_array)\n",
    "    \n",
    "    encoded2.transform(np.array(['C.60K-100K','D.30K-60K','A.ABOVE200K','B.100K-200K','E.BELOW30K']).reshape(-1,1)).toarray()\n",
    "    df[\"annual_income_est\"].value_counts()\n",
    "    annual_income_est_T = encoded2.transform(annual_income_est_np_array).toarray().T\n",
    "    annual_income_est_T\n",
    "    some_index = 0\n",
    "    for i in ['A.ABOVE200K','B.100K-200K','C.60K-100K','D.30K-60K','E.BELOW30K']:\n",
    "        name = \"annual_income_est_\" + i\n",
    "        df[name] = annual_income_est_T[some_index]\n",
    "        some_index += 1\n",
    "    df = df.drop(\"annual_income_est\", axis = 1)\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # from collections import Counter\n",
    "    # from imblearn.over_sampling import SMOTE\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # print('Before:', Counter(y_train))\n",
    "    # X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "    # print('After:', Counter(y_train))\n",
    "\n",
    "    \n",
    "    # model = RandomForestClassifier() \n",
    "    # model.fit(X_train, y_train) \n",
    "    \n",
    "    # feature_importances = pd.Series(model.feature_importances_, index=df.columns) \n",
    "    # top_features = feature_importances.nlargest(20).index \n",
    "    df = df[top_features] \n",
    "\n",
    "\n",
    "\n",
    "    # from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay \n",
    "    # from sklearn.tree import DecisionTreeClassifier \n",
    "    \n",
    "    # dt_clf = DecisionTreeClassifier() \n",
    "    # dt_clf.fit(X_train, y_train) \n",
    "    \n",
    "    y_test_pred = dt_clf.predict(df) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    result = list(y_test_pred) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- affcon_visit_days\n- annual_income_est_A.ABOVE200K\n- annual_income_est_B.100K-200K\n- annual_income_est_D.30K-60K\n- f_elx\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[268], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(filepath)\n\u001b[0;32m      3\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf_purchase_lh\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtesting_hidden_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[267], line 188\u001b[0m, in \u001b[0;36mtesting_hidden_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    178\u001b[0m df \u001b[38;5;241m=\u001b[39m df[top_features] \n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay \u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# from sklearn.tree import DecisionTreeClassifier \u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# dt_clf = DecisionTreeClassifier() \u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# dt_clf.fit(X_train, y_train) \u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mdt_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    196\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(y_test_pred) \n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:426\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    425\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 426\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    428\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:392\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[1;32m--> 392\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    394\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[0;32m    395\u001b[0m     ):\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\timot\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- affcon_visit_days\n- annual_income_est_A.ABOVE200K\n- annual_income_est_B.100K-200K\n- annual_income_est_D.30K-60K\n- f_elx\n- ...\n"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
